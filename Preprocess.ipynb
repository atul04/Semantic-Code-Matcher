{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 : Data loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import ast\n",
    "import glob\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import astor\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from general_utils import apply_parallel, flattenlist\n",
    "\n",
    "EN = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 50.3 s, sys: 20.8 s, total: 1min 11s\n",
      "Wall time: 4min 34s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nwo</th>\n",
       "      <th>path</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fnl/libfnl</td>\n",
       "      <td>src/fnl/nlp/dictionary.py</td>\n",
       "      <td>\"\"\"\\n.. py:module:: fnl.text.dictionary\\n   :s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KivApple/mcu-info-util</td>\n",
       "      <td>mcu_info_util/linker_script.py</td>\n",
       "      <td>from six import iteritems\\n\\n\\ndef generate(op...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yelp/pyleus</td>\n",
       "      <td>examples/bandwith_monitoring/bandwith_monitori...</td>\n",
       "      <td>from __future__ import absolute_import, divisi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jhuapl-boss/boss-manage</td>\n",
       "      <td>bin/bearer_token.py</td>\n",
       "      <td>#!/usr/bin/env python3\\n\\n# Copyright 2016 The...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>djfroofy/beatlounge</td>\n",
       "      <td>bl/orchestra/base.py</td>\n",
       "      <td>from itertools import cycle\\n\\nfrom twisted.py...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       nwo                                               path  \\\n",
       "0               fnl/libfnl                          src/fnl/nlp/dictionary.py   \n",
       "1   KivApple/mcu-info-util                     mcu_info_util/linker_script.py   \n",
       "2              Yelp/pyleus  examples/bandwith_monitoring/bandwith_monitori...   \n",
       "3  jhuapl-boss/boss-manage                                bin/bearer_token.py   \n",
       "4      djfroofy/beatlounge                               bl/orchestra/base.py   \n",
       "\n",
       "                                             content  \n",
       "0  \"\"\"\\n.. py:module:: fnl.text.dictionary\\n   :s...  \n",
       "1  from six import iteritems\\n\\n\\ndef generate(op...  \n",
       "2  from __future__ import absolute_import, divisi...  \n",
       "3  #!/usr/bin/env python3\\n\\n# Copyright 2016 The...  \n",
       "4  from itertools import cycle\\n\\nfrom twisted.py...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Read the data into a pandas dataframe, and parse out some meta-data\n",
    "\n",
    "df = pd.concat([pd.read_csv(f'https://storage.googleapis.com/kubeflow-examples/code_search/raw_data/00000000000{i}.csv') \\\n",
    "                for i in range(10)])\n",
    "\n",
    "df['nwo'] = df['repo_path'].apply(lambda r: r.split()[0])\n",
    "df['path'] = df['repo_path'].apply(lambda r: r.split()[1])\n",
    "df.drop(columns=['repo_path'], inplace=True)\n",
    "df = df[['nwo', 'path', 'content']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1241664, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect shape of the raw data\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to parse data and tokenize¶ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our goal is to parse the python files into (code, docstring) pairs. Fortunately, the standard library in python comes with the wonderful ast module which helps us extract code from files as well as extract docstrings.We also use the astor library to strip the code of comments by doing a round trip of converting the code to an AST and then from AST back to code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_docstring(text):\n",
    "    \"Apply tokenization using spacy to docstrings.\"\n",
    "    tokens = EN.tokenizer(text)\n",
    "    return [token.text.lower() for token in tokens if not token.is_space]\n",
    "\n",
    "\n",
    "def tokenize_code(text):\n",
    "    \"A very basic procedure for tokenizing code strings.\"\n",
    "    return RegexpTokenizer(r'\\w+').tokenize(text)\n",
    "\n",
    "\n",
    "def get_function_docstring_pairs(blob):\n",
    "    \"Extract (function/method, docstring) pairs from a given code blob.\"\n",
    "    pairs = []\n",
    "    try:\n",
    "        module = ast.parse(blob)\n",
    "        classes = [node for node in module.body if isinstance(node, ast.ClassDef)]\n",
    "        functions = [node for node in module.body if isinstance(node, ast.FunctionDef)]\n",
    "        for _class in classes:\n",
    "            functions.extend([node for node in _class.body if isinstance(node, ast.FunctionDef)])\n",
    "\n",
    "        for f in functions:\n",
    "            source = astor.to_source(f)\n",
    "            docstring = ast.get_docstring(f) if ast.get_docstring(f) else ''\n",
    "            function = source.replace(ast.get_docstring(f, clean=False), '') if docstring else source\n",
    "\n",
    "            pairs.append((f.name,\n",
    "                          f.lineno,\n",
    "                          source,\n",
    "                          ' '.join(tokenize_code(function)),\n",
    "                          ' '.join(tokenize_docstring(docstring.split('\\n\\n')[0]))\n",
    "                         ))\n",
    "    except (AssertionError, MemoryError, SyntaxError, UnicodeEncodeError):\n",
    "        pass\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def get_function_docstring_pairs_list(blob_list):\n",
    "    \"\"\"apply the function `get_function_docstring_pairs` on a list of blobs\"\"\"\n",
    "    return [get_function_docstring_pairs(b) for b in blob_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  The below convience function apply_parallel parses the code in parallel using process based threading. Adjust the cpu_cores parameter accordingly to your system resources!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 37.2 s, sys: 31.2 s, total: 1min 8s\n",
      "Wall time: 5min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pairs = flattenlist(apply_parallel(get_function_docstring_pairs_list, df.content.tolist(), cpu_cores=32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nwo</th>\n",
       "      <th>path</th>\n",
       "      <th>content</th>\n",
       "      <th>pairs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fnl/libfnl</td>\n",
       "      <td>src/fnl/nlp/dictionary.py</td>\n",
       "      <td>\"\"\"\\n.. py:module:: fnl.text.dictionary\\n   :s...</td>\n",
       "      <td>[(__init__, 19, def __init__(self, *leafs, **e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KivApple/mcu-info-util</td>\n",
       "      <td>mcu_info_util/linker_script.py</td>\n",
       "      <td>from six import iteritems\\n\\n\\ndef generate(op...</td>\n",
       "      <td>[(generate, 4, def generate(options, filename=...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yelp/pyleus</td>\n",
       "      <td>examples/bandwith_monitoring/bandwith_monitori...</td>\n",
       "      <td>from __future__ import absolute_import, divisi...</td>\n",
       "      <td>[(__init__, 18, def __init__(self, size):\\n   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jhuapl-boss/boss-manage</td>\n",
       "      <td>bin/bearer_token.py</td>\n",
       "      <td>#!/usr/bin/env python3\\n\\n# Copyright 2016 The...</td>\n",
       "      <td>[(request, 46, def request(url, params=None, h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>djfroofy/beatlounge</td>\n",
       "      <td>bl/orchestra/base.py</td>\n",
       "      <td>from itertools import cycle\\n\\nfrom twisted.py...</td>\n",
       "      <td>[(schedule, 149, def schedule(time, func, args...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       nwo                                               path  \\\n",
       "0               fnl/libfnl                          src/fnl/nlp/dictionary.py   \n",
       "1   KivApple/mcu-info-util                     mcu_info_util/linker_script.py   \n",
       "2              Yelp/pyleus  examples/bandwith_monitoring/bandwith_monitori...   \n",
       "3  jhuapl-boss/boss-manage                                bin/bearer_token.py   \n",
       "4      djfroofy/beatlounge                               bl/orchestra/base.py   \n",
       "\n",
       "                                             content  \\\n",
       "0  \"\"\"\\n.. py:module:: fnl.text.dictionary\\n   :s...   \n",
       "1  from six import iteritems\\n\\n\\ndef generate(op...   \n",
       "2  from __future__ import absolute_import, divisi...   \n",
       "3  #!/usr/bin/env python3\\n\\n# Copyright 2016 The...   \n",
       "4  from itertools import cycle\\n\\nfrom twisted.py...   \n",
       "\n",
       "                                               pairs  \n",
       "0  [(__init__, 19, def __init__(self, *leafs, **e...  \n",
       "1  [(generate, 4, def generate(options, filename=...  \n",
       "2  [(__init__, 18, def __init__(self, size):\\n   ...  \n",
       "3  [(request, 46, def request(url, params=None, h...  \n",
       "4  [(schedule, 149, def schedule(time, func, args...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert len(pairs) == df.shape[0], f'Row count mismatch. `df` has {df.shape[0]:,} rows; `pairs` has {len(pairs):,} rows.'\n",
    "df['pairs'] = pairs\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flatten code, docstring pairs and extract meta-data¶\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flatten (code, docstring) pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 15s, sys: 28.9 s, total: 6min 44s\n",
      "Wall time: 6min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# flatten pairs\n",
    "df = df.set_index(['nwo', 'path'])['pairs'].apply(pd.Series).stack()\n",
    "df = df.reset_index()\n",
    "df.columns = ['nwo', 'path', '_', 'pair']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Extract meta-data and format dataframe.\n",
    "\n",
    "We have not optimized this code. Pull requests are welcome!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 18s, sys: 8.87 s, total: 4min 27s\n",
      "Wall time: 4min 27s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nwo</th>\n",
       "      <th>path</th>\n",
       "      <th>function_name</th>\n",
       "      <th>lineno</th>\n",
       "      <th>original_function</th>\n",
       "      <th>function_tokens</th>\n",
       "      <th>docstring_tokens</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fnl/libfnl</td>\n",
       "      <td>src/fnl/nlp/dictionary.py</td>\n",
       "      <td>__init__</td>\n",
       "      <td>19</td>\n",
       "      <td>def __init__(self, *leafs, **edges):\\n    self...</td>\n",
       "      <td>def __init__ self leafs edges self edges edges...</td>\n",
       "      <td></td>\n",
       "      <td>https://github.com/fnl/libfnl/blob/master/src/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fnl/libfnl</td>\n",
       "      <td>src/fnl/nlp/dictionary.py</td>\n",
       "      <td>__eq__</td>\n",
       "      <td>23</td>\n",
       "      <td>def __eq__(self, other):\\n    if isinstance(ot...</td>\n",
       "      <td>def __eq__ self other if isinstance other Node...</td>\n",
       "      <td></td>\n",
       "      <td>https://github.com/fnl/libfnl/blob/master/src/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fnl/libfnl</td>\n",
       "      <td>src/fnl/nlp/dictionary.py</td>\n",
       "      <td>__repr__</td>\n",
       "      <td>29</td>\n",
       "      <td>def __repr__(self):\\n    return 'Node&lt;leafs={}...</td>\n",
       "      <td>def __repr__ self return Node leafs edges form...</td>\n",
       "      <td></td>\n",
       "      <td>https://github.com/fnl/libfnl/blob/master/src/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fnl/libfnl</td>\n",
       "      <td>src/fnl/nlp/dictionary.py</td>\n",
       "      <td>createOrGet</td>\n",
       "      <td>32</td>\n",
       "      <td>def createOrGet(self, token):\\n    \"\"\"\\n\\t\\tCr...</td>\n",
       "      <td>def createOrGet self token if token in self ed...</td>\n",
       "      <td>create or get the node pointed to by ` token `...</td>\n",
       "      <td>https://github.com/fnl/libfnl/blob/master/src/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fnl/libfnl</td>\n",
       "      <td>src/fnl/nlp/dictionary.py</td>\n",
       "      <td>setLeaf</td>\n",
       "      <td>47</td>\n",
       "      <td>def setLeaf(self, key, order):\\n    \"\"\"\\n\\t\\tS...</td>\n",
       "      <td>def setLeaf self key order self leafs append o...</td>\n",
       "      <td>store the ` key ` as a leaf of this node at po...</td>\n",
       "      <td>https://github.com/fnl/libfnl/blob/master/src/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          nwo                       path function_name  lineno  \\\n",
       "0  fnl/libfnl  src/fnl/nlp/dictionary.py      __init__      19   \n",
       "1  fnl/libfnl  src/fnl/nlp/dictionary.py        __eq__      23   \n",
       "2  fnl/libfnl  src/fnl/nlp/dictionary.py      __repr__      29   \n",
       "3  fnl/libfnl  src/fnl/nlp/dictionary.py   createOrGet      32   \n",
       "4  fnl/libfnl  src/fnl/nlp/dictionary.py       setLeaf      47   \n",
       "\n",
       "                                   original_function  \\\n",
       "0  def __init__(self, *leafs, **edges):\\n    self...   \n",
       "1  def __eq__(self, other):\\n    if isinstance(ot...   \n",
       "2  def __repr__(self):\\n    return 'Node<leafs={}...   \n",
       "3  def createOrGet(self, token):\\n    \"\"\"\\n\\t\\tCr...   \n",
       "4  def setLeaf(self, key, order):\\n    \"\"\"\\n\\t\\tS...   \n",
       "\n",
       "                                     function_tokens  \\\n",
       "0  def __init__ self leafs edges self edges edges...   \n",
       "1  def __eq__ self other if isinstance other Node...   \n",
       "2  def __repr__ self return Node leafs edges form...   \n",
       "3  def createOrGet self token if token in self ed...   \n",
       "4  def setLeaf self key order self leafs append o...   \n",
       "\n",
       "                                    docstring_tokens  \\\n",
       "0                                                      \n",
       "1                                                      \n",
       "2                                                      \n",
       "3  create or get the node pointed to by ` token `...   \n",
       "4  store the ` key ` as a leaf of this node at po...   \n",
       "\n",
       "                                                 url  \n",
       "0  https://github.com/fnl/libfnl/blob/master/src/...  \n",
       "1  https://github.com/fnl/libfnl/blob/master/src/...  \n",
       "2  https://github.com/fnl/libfnl/blob/master/src/...  \n",
       "3  https://github.com/fnl/libfnl/blob/master/src/...  \n",
       "4  https://github.com/fnl/libfnl/blob/master/src/...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "df['function_name'] = df['pair'].apply(lambda p: p[0])\n",
    "df['lineno'] = df['pair'].apply(lambda p: p[1])\n",
    "df['original_function'] = df['pair'].apply(lambda p: p[2])\n",
    "df['function_tokens'] = df['pair'].apply(lambda p: p[3])\n",
    "df['docstring_tokens'] = df['pair'].apply(lambda p: p[4])\n",
    "df = df[['nwo', 'path', 'function_name', 'lineno', 'original_function', 'function_tokens', 'docstring_tokens']]\n",
    "df['url'] = df[['nwo', 'path', 'lineno']].apply(lambda x: 'https://github.com/{}/blob/master/{}#L{}'.format(x[0], x[1], x[2]), axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Duplicates¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 1,196,159 duplicate rows\n",
      "CPU times: user 23.4 s, sys: 2.61 s, total: 26 s\n",
      "Wall time: 26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# remove observations where the same function appears more than once\n",
    "before_dedup = len(df)\n",
    "df = df.drop_duplicates(['original_function', 'function_tokens'])\n",
    "after_dedup = len(df)\n",
    "\n",
    "print(f'Removed {before_dedup - after_dedup:,} duplicate rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5396853, 8)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Separate function w/o docstrings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listlen(x):\n",
    "    if not isinstance(x, list):\n",
    "        return 0\n",
    "    return len(x)\n",
    "\n",
    "# separate functions w/o docstrings\n",
    "# docstrings should be at least 3 words in the docstring to be considered a valid docstring\n",
    "\n",
    "with_docstrings = df[df.docstring_tokens.str.split().apply(listlen) >= 3]\n",
    "without_docstrings = df[df.docstring_tokens.str.split().apply(listlen) < 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partition code by repository to minimize leakage between train, valid & test sets.¶ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rough assumption that each repository has its own style. We want to avoid having code from the same repository in the training set as well as the validation or holdout set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = with_docstrings.groupby('nwo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, valid, test splits\n",
    "train, test = train_test_split(list(grouped), train_size=0.87, shuffle=True, random_state=8081)\n",
    "train, valid = train_test_split(train, train_size=0.82, random_state=8081)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([d for _, d in train]).reset_index(drop=True)\n",
    "valid = pd.concat([d for _, d in valid]).reset_index(drop=True)\n",
    "test = pd.concat([d for _, d in test]).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set num rows 997,815\n",
      "valid set num rows 216,680\n",
      "test set num rows 187,048\n",
      "without docstring rows 3,995,310\n"
     ]
    }
   ],
   "source": [
    "print(f'train set num rows {train.shape[0]:,}')\n",
    "print(f'valid set num rows {valid.shape[0]:,}')\n",
    "print(f'test set num rows {test.shape[0]:,}')\n",
    "print(f'without docstring rows {without_docstrings.shape[0]:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Preview what the training set looks like. You can start to see how the data looks, the function tokens and docstring tokens are what will be fed downstream into the models. The other information is important for diagnostics and bookeeping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nwo</th>\n",
       "      <th>path</th>\n",
       "      <th>function_name</th>\n",
       "      <th>lineno</th>\n",
       "      <th>original_function</th>\n",
       "      <th>function_tokens</th>\n",
       "      <th>docstring_tokens</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pdyba/aioquiz</td>\n",
       "      <td>utils.py</td>\n",
       "      <td>safe_del_key</td>\n",
       "      <td>46</td>\n",
       "      <td>def safe_del_key(data, unwanted_key):\\n    \"\"\"...</td>\n",
       "      <td>def safe_del_key data unwanted_key if isinstan...</td>\n",
       "      <td>safe deleter of keys : param data : dict : par...</td>\n",
       "      <td>https://github.com/pdyba/aioquiz/blob/master/u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pdyba/aioquiz</td>\n",
       "      <td>views/utils.py</td>\n",
       "      <td>user_required</td>\n",
       "      <td>16</td>\n",
       "      <td>def user_required(access_level='any_user', msg...</td>\n",
       "      <td>def user_required access_level any_user msg NO...</td>\n",
       "      <td>no_user - anonymus any_user - loged user mento...</td>\n",
       "      <td>https://github.com/pdyba/aioquiz/blob/master/v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kulbear/endless-2048</td>\n",
       "      <td>tester/minimax_tester.py</td>\n",
       "      <td>test_one_game</td>\n",
       "      <td>33</td>\n",
       "      <td>def test_one_game(self):\\n    \"\"\"Go through on...</td>\n",
       "      <td>def test_one_game self game self create_one_ga...</td>\n",
       "      <td>go through one game , played by a minimaxagent...</td>\n",
       "      <td>https://github.com/Kulbear/endless-2048/blob/m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kulbear/endless-2048</td>\n",
       "      <td>agent/minimax_agent.py</td>\n",
       "      <td>get_move</td>\n",
       "      <td>37</td>\n",
       "      <td>def get_move(self, game):\\n    \"\"\"Search the n...</td>\n",
       "      <td>def get_move self game available game moves_av...</td>\n",
       "      <td>search the next optimal move by the iterative ...</td>\n",
       "      <td>https://github.com/Kulbear/endless-2048/blob/m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kulbear/endless-2048</td>\n",
       "      <td>agent/minimax_agent.py</td>\n",
       "      <td>search</td>\n",
       "      <td>53</td>\n",
       "      <td>def search(self, game, alpha, beta, depth, max...</td>\n",
       "      <td>def search self game alpha beta depth max_dept...</td>\n",
       "      <td>the implementation of the minimax search with ...</td>\n",
       "      <td>https://github.com/Kulbear/endless-2048/blob/m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    nwo                      path  function_name  lineno  \\\n",
       "0         pdyba/aioquiz                  utils.py   safe_del_key      46   \n",
       "1         pdyba/aioquiz            views/utils.py  user_required      16   \n",
       "2  Kulbear/endless-2048  tester/minimax_tester.py  test_one_game      33   \n",
       "3  Kulbear/endless-2048    agent/minimax_agent.py       get_move      37   \n",
       "4  Kulbear/endless-2048    agent/minimax_agent.py         search      53   \n",
       "\n",
       "                                   original_function  \\\n",
       "0  def safe_del_key(data, unwanted_key):\\n    \"\"\"...   \n",
       "1  def user_required(access_level='any_user', msg...   \n",
       "2  def test_one_game(self):\\n    \"\"\"Go through on...   \n",
       "3  def get_move(self, game):\\n    \"\"\"Search the n...   \n",
       "4  def search(self, game, alpha, beta, depth, max...   \n",
       "\n",
       "                                     function_tokens  \\\n",
       "0  def safe_del_key data unwanted_key if isinstan...   \n",
       "1  def user_required access_level any_user msg NO...   \n",
       "2  def test_one_game self game self create_one_ga...   \n",
       "3  def get_move self game available game moves_av...   \n",
       "4  def search self game alpha beta depth max_dept...   \n",
       "\n",
       "                                    docstring_tokens  \\\n",
       "0  safe deleter of keys : param data : dict : par...   \n",
       "1  no_user - anonymus any_user - loged user mento...   \n",
       "2  go through one game , played by a minimaxagent...   \n",
       "3  search the next optimal move by the iterative ...   \n",
       "4  the implementation of the minimax search with ...   \n",
       "\n",
       "                                                 url  \n",
       "0  https://github.com/pdyba/aioquiz/blob/master/u...  \n",
       "1  https://github.com/pdyba/aioquiz/blob/master/v...  \n",
       "2  https://github.com/Kulbear/endless-2048/blob/m...  \n",
       "3  https://github.com/Kulbear/endless-2048/blob/m...  \n",
       "4  https://github.com/Kulbear/endless-2048/blob/m...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output each set to train/valid/test.function/docstrings/lineage files "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original functions are also written to compressed json files. (Raw functions contain ,, \\t, \\n, etc., it is less error-prone using json format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{train,valid,test}.lineage are files that contain a reference to the original location where the code was retrieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir ./data/processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to(df, filename, path='./data/processed_data/'):\n",
    "    \"Helper function to write processed files to disk.\"\n",
    "    out = Path(path)\n",
    "    out.mkdir(exist_ok=True)\n",
    "    df.function_tokens.to_csv(out/'{}.function'.format(filename), index=False)\n",
    "    df.original_function.to_json(out/'{}_original_function.json.gz'.format(filename), orient='values', compression='gzip')\n",
    "    if filename != 'without_docstrings':\n",
    "        df.docstring_tokens.to_csv(out/'{}.docstring'.format(filename), index=False)\n",
    "    df.url.to_csv(out/'{}.lineage'.format(filename), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists('data/'):\n",
    "    os.makedirs('data/')\n",
    "# write to output files\n",
    "write_to(train, 'train')\n",
    "write_to(valid, 'valid')\n",
    "write_to(test, 'test')\n",
    "write_to(without_docstrings, 'without_docstrings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 2.6G\r\n",
      "drwxrwxr-x 2 ritesh ritesh 4.0K Apr  4 05:22 .\r\n",
      "drwxrwxr-x 3 ritesh ritesh 4.0K Apr  4 05:16 ..\r\n",
      "-rw-rw-r-- 1 ritesh ritesh  16M Apr  4 05:19 test.docstring\r\n",
      "-rw-rw-r-- 1 ritesh ritesh  55M Apr  4 05:19 test.function\r\n",
      "-rw-rw-r-- 1 ritesh ritesh  18M Apr  4 05:19 test.lineage\r\n",
      "-rw-rw-r-- 1 ritesh ritesh  25M Apr  4 05:19 test_original_function.json.gz\r\n",
      "-rw-rw-r-- 1 ritesh ritesh  70M Apr  4 05:18 train.docstring\r\n",
      "-rw-rw-r-- 1 ritesh ritesh 308M Apr  4 05:17 train.function\r\n",
      "-rw-rw-r-- 1 ritesh ritesh  86M Apr  4 05:19 train.lineage\r\n",
      "-rw-rw-r-- 1 ritesh ritesh 140M Apr  4 05:18 train_original_function.json.gz\r\n",
      "-rw-rw-r-- 1 ritesh ritesh  16M Apr  4 05:19 valid.docstring\r\n",
      "-rw-rw-r-- 1 ritesh ritesh  69M Apr  4 05:19 valid.function\r\n",
      "-rw-rw-r-- 1 ritesh ritesh  19M Apr  4 05:19 valid.lineage\r\n",
      "-rw-rw-r-- 1 ritesh ritesh  31M Apr  4 05:19 valid_original_function.json.gz\r\n",
      "-rw-rw-r-- 1 ritesh ritesh 1.1G Apr  4 05:19 without_docstrings.function\r\n",
      "-rw-rw-r-- 1 ritesh ritesh 344M Apr  4 05:22 without_docstrings.lineage\r\n",
      "-rw-rw-r-- 1 ritesh ritesh 356M Apr  4 05:22 without_docstrings_original_function.json.gz\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lah ./data/processed_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
