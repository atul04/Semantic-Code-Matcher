{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: The code encoder "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Requisite: Make Sure you have the right files prepared from Step 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have these files in the root of the ./data/processed_data/ directory:\n",
    "\n",
    "1.    {train/valid/test.function} - these are python function definitions tokenized (by space), 1 line per function.\n",
    "2.    {train/valid/test.docstring} - these are docstrings that correspond to each of the python function definitions, and have a 1:1 correspondence with the lines in *.function files.\n",
    "3.    {train/valid/test.lineage} - every line in this file contains a link back to the original location (github repo link) where the code was retrieved. There is a 1:1 correspondence with the lines in this file and the other two files. This is useful for debugging.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the value of use_cache appropriately. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if use_cache = True, data will be downloaded where possible instead of re-computing. However, it is highly recommended that you set use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Optional: you can set what GPU you want to use in a notebook like this.  \n",
    "# # Useful if you want to run concurrent experiments at the same time on different GPUs.\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ./data/seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# This will allow the notebook to run faster\n",
    "from pathlib import Path\n",
    "from general_utils import get_step2_prerequisite_files, read_training_files\n",
    "from keras.utils import get_file\n",
    "OUTPUT_PATH = Path('./data/seq2seq/')\n",
    "OUTPUT_PATH.mkdir(exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Text From File¶ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Num rows for encoder training + validation input: 1,214,497\n",
      "WARNING:root:Num rows for encoder holdout input: 187,049\n",
      "WARNING:root:Num rows for decoder training + validation input: 1,214,497\n",
      "WARNING:root:Num rows for decoder holdout input: 187,049\n"
     ]
    }
   ],
   "source": [
    "if use_cache:\n",
    "    get_step2_prerequisite_files(output_directory = './data/processed_data')\n",
    "\n",
    "# you want to supply the directory where the files are from step 1.\n",
    "train_code, holdout_code, train_comment, holdout_comment = read_training_files('./data/processed_data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code and comment files should be of the same length.\n",
    "\n",
    "assert len(train_code) == len(train_comment)\n",
    "assert len(holdout_code) == len(holdout_comment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize Text¶ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ritesh/work/venv/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "WARNING:root:....tokenizing data\n",
      "WARNING:root:Setting maximum document length to 55 based upon heuristic of 0.7 percentile.\n",
      " See full histogram by insepecting the `document_length_stats` attribute.\n",
      "WARNING:root:(1/2) done. 71 sec\n",
      "WARNING:root:....building corpus\n",
      "WARNING:root:(2/2) done. 29 sec\n",
      "WARNING:root:Finished parsing 1,214,497 documents.\n",
      "WARNING:root:...fit is finished, beginning transform\n",
      "WARNING:root:...padding data\n",
      "WARNING:root:done. 24 sec\n",
      "WARNING:root:....tokenizing data\n",
      "WARNING:root:Setting maximum document length to 15 based upon heuristic of 0.7 percentile.\n",
      " See full histogram by insepecting the `document_length_stats` attribute.\n",
      "WARNING:root:(1/2) done. 28 sec\n",
      "WARNING:root:....building corpus\n",
      "WARNING:root:(2/2) done. 8 sec\n",
      "WARNING:root:Finished parsing 1,214,497 documents.\n",
      "WARNING:root:...fit is finished, beginning transform\n",
      "WARNING:root:...padding data\n",
      "WARNING:root:done. 10 sec\n"
     ]
    }
   ],
   "source": [
    "from ktext.preprocess import processor\n",
    "\n",
    "if not use_cache:    \n",
    "    code_proc = processor(heuristic_pct_padding=.7, keep_n=20000)\n",
    "    t_code = code_proc.fit_transform(train_code)\n",
    "\n",
    "    comment_proc = processor(append_indicators=True, heuristic_pct_padding=.7, keep_n=14000, padding ='post')\n",
    "    t_comment = comment_proc.fit_transform(train_comment)\n",
    "\n",
    "elif use_cache:\n",
    "    logging.warning('Not fitting transform function because use_cache=True')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Save tokenized text (You will reuse this for step 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill as dpickle\n",
    "import numpy as np\n",
    "\n",
    "if not use_cache:\n",
    "    # Save the preprocessor\n",
    "    with open(OUTPUT_PATH/'py_code_proc_v2.dpkl', 'wb') as f:\n",
    "        dpickle.dump(code_proc, f)\n",
    "\n",
    "    with open(OUTPUT_PATH/'py_comment_proc_v2.dpkl', 'wb') as f:\n",
    "        dpickle.dump(comment_proc, f)\n",
    "\n",
    "    # Save the processed data\n",
    "    np.save(OUTPUT_PATH/'py_t_code_vecs_v2.npy', t_code)\n",
    "    np.save(OUTPUT_PATH/'py_t_comment_vecs_v2.npy', t_comment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Arrange data for modeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of encoder input: (1214497, 55)\n",
      "Shape of decoder input: (1214497, 14)\n",
      "Shape of decoder target: (1214497, 14)\n",
      "Size of vocabulary for data/seq2seq/py_code_proc_v2.dpkl: 20,002\n",
      "Size of vocabulary for data/seq2seq/py_comment_proc_v2.dpkl: 14,002\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 3\n",
    "from seq2seq_utils import load_decoder_inputs, load_encoder_inputs, load_text_processor\n",
    "\n",
    "\n",
    "encoder_input_data, encoder_seq_len = load_encoder_inputs(OUTPUT_PATH/'py_t_code_vecs_v2.npy')\n",
    "decoder_input_data, decoder_target_data = load_decoder_inputs(OUTPUT_PATH/'py_t_comment_vecs_v2.npy')\n",
    "num_encoder_tokens, enc_pp = load_text_processor(OUTPUT_PATH/'py_code_proc_v2.dpkl')\n",
    "num_decoder_tokens, dec_pp = load_text_processor(OUTPUT_PATH/'py_comment_proc_v2.dpkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Seq2Seq Model For Summarizing Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build a model to predict the docstring given a function or a method. While this is a very cool task in itself, this is not the end goal of this exercise. The motivation for training this model is to learn a general purpose feature extractor for code that we can use for the task of code search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seq2seq_utils import build_seq2seq_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The convenience function build_seq2seq_model constructs the architecture for a sequence-to-sequence model.\n",
    "\n",
    "The architecture built for this tutorial is a minimal example with only one layer for the encoder and decoder, and does not include things like attention. We encourage you to try and build different architectures to see what works best for you!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq_Model = build_seq2seq_model(word_emb_dim=800,\n",
    "                                    hidden_state_dim=1000,\n",
    "                                    encoder_seq_len=encoder_seq_len,\n",
    "                                    num_encoder_tokens=num_encoder_tokens,\n",
    "                                    num_decoder_tokens=num_decoder_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Decoder-Input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Word-Embedding (Embeddi (None, None, 800)    11201600    Decoder-Input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Input (InputLayer)      (None, 55)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Batchnorm-1 (BatchNorma (None, None, 800)    3200        Decoder-Word-Embedding[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Model (Model)           (None, 1000)         21407800    Encoder-Input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-GRU (GRU)               [(None, None, 1000), 5403000     Decoder-Batchnorm-1[0][0]        \n",
      "                                                                 Encoder-Model[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Batchnorm-2 (BatchNorma (None, None, 1000)   4000        Decoder-GRU[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Final-Output-Dense (Dense)      (None, None, 14002)  14016002    Decoder-Batchnorm-2[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 52,035,602\n",
      "Trainable params: 52,030,402\n",
      "Non-trainable params: 5,200\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "seq2seq_Model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Seq2Seq Model¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1068757 samples, validate on 145740 samples\n",
      "Epoch 1/16\n",
      "  29700/1068757 [..............................] - ETA: 1:50:24 - loss: 8.2796"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-84c05d9a94a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m               \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m               \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m               validation_split=0.12, callbacks=[csv_logger, model_checkpoint])\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/venv/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/work/venv/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/venv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/venv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/venv/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.models import Model, load_model\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "if not use_cache:\n",
    "\n",
    "    from keras.callbacks import CSVLogger, ModelCheckpoint\n",
    "    import numpy as np\n",
    "    from keras import optimizers\n",
    "\n",
    "    seq2seq_Model.compile(optimizer=optimizers.Nadam(lr=0.00005), loss='sparse_categorical_crossentropy')\n",
    "\n",
    "    script_name_base = 'py_func_sum_v9_'\n",
    "    csv_logger = CSVLogger('{:}.log'.format(script_name_base))\n",
    "\n",
    "    model_checkpoint = ModelCheckpoint('{:}.epoch{{epoch:02d}}-val{{val_loss:.5f}}.hdf5'.format(script_name_base),\n",
    "                                       save_best_only=True)\n",
    "\n",
    "    batch_size = 1100\n",
    "    epochs = 16\n",
    "    history = seq2seq_Model.fit([encoder_input_data, decoder_input_data], np.expand_dims(decoder_target_data, -1),\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_split=0.12, callbacks=[csv_logger, model_checkpoint])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-04-04 09:01:19--  https://storage.googleapis.com/kubeflow-examples/code_search/data/seq2seq/py_func_sum_v9_.epoch16-val2.55276.hdf5\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.167.176, 2404:6800:4009:810::2010\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.167.176|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 624441080 (596M) [application/octet-stream]\n",
      "Saving to: ‘py_func_sum_v9_.epoch16-val2.55276.hdf5’\n",
      "\n",
      "py_func_sum_v9_.epo 100%[===================>] 595.51M  21.7MB/s    in 28s     \n",
      "\n",
      "2020-04-04 09:01:47 (21.6 MB/s) - ‘py_func_sum_v9_.epoch16-val2.55276.hdf5’ saved [624441080/624441080]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://storage.googleapis.com/kubeflow-examples/code_search/data/seq2seq/py_func_sum_v9_.epoch16-val2.55276.hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Not re-training function summarizer seq2seq model because use_cache=True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/kubeflow-examples/code_search/data/seq2seq/py_func_sum_v9_.epoch16-val2.55276.hdf5\n",
      "624443392/624441080 [==============================] - 70s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/kubeflow-examples/code_search/data/seq2seq/py_code_proc_v2.dpkl\n",
      "20824064/20815922 [==============================] - 3s 0us/step\n",
      "Size of vocabulary for /home/ritesh/.keras/datasets/py_code_proc_v2.dpkl: 20,002\n",
      "Downloading data from https://storage.googleapis.com/kubeflow-examples/code_search/data/seq2seq/py_comment_proc_v2.dpkl\n",
      "5292032/5288362 [==============================] - 0s 0us/step\n",
      "Size of vocabulary for /home/ritesh/.keras/datasets/py_comment_proc_v2.dpkl: 14,002\n"
     ]
    }
   ],
   "source": [
    "if use_cache:\n",
    "    logging.warning('Not re-training function summarizer seq2seq model because use_cache=True')\n",
    "    # Load model from url\n",
    "    loc = get_file(fname='py_func_sum_v9_.epoch16-val2.55276.hdf5',\n",
    "                   origin='https://storage.googleapis.com/kubeflow-examples/code_search/data/seq2seq/py_func_sum_v9_.epoch16-val2.55276.hdf5')\n",
    "    seq2seq_Model = load_model(loc)\n",
    "    \n",
    "    # Load encoder (code) pre-processor from url\n",
    "    loc = get_file(fname='py_code_proc_v2.dpkl',\n",
    "                   origin='https://storage.googleapis.com/kubeflow-examples/code_search/data/seq2seq/py_code_proc_v2.dpkl')\n",
    "    num_encoder_tokens, enc_pp = load_text_processor(loc)\n",
    "    \n",
    "    # Load decoder (docstrings/comments) pre-processor from url\n",
    "    loc = get_file(fname='py_comment_proc_v2.dpkl',\n",
    "                   origin='https://storage.googleapis.com/kubeflow-examples/code_search/data/seq2seq/py_comment_proc_v2.dpkl')\n",
    "    num_decoder_tokens, dec_pp = load_text_processor(loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Note that the above procedure will automatically download a pre-trained model and associated artifacts from https://storage.googleapis.com/kubeflow-examples/code_search/data/seq2seq/ if use_cache = True.\n",
    "\n",
    "Otherwise, the above code will checkpoint the best model after each epoch into the current directory with prefix py_func_sum_v9_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Seq2Seq Model For Code Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate this model we are going to do two things:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "   1. Manually inspect the results of predicted docstrings for code snippets, to make sure they look sensible.\n",
    "   2. Calculate the BLEU Score so that we can quantitately benchmark different iterations of this algorithm and to guide hyper-parameter tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually Inspect Results (on holdout set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 177285 =================\n",
      "\n",
      "Original Input:\n",
      " def test_image_white for img_format in png jpg gif _load_and_check_img canvas_white img_format 1 1 b x00 b x00\n",
      " \n",
      "\n",
      "Original Output:\n",
      " test rendering solid white image\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " test white image\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 90836 =================\n",
      "\n",
      "Original Input:\n",
      " def __div__ self v return Coordinates self x v self y v self z v self e v\n",
      " \n",
      "\n",
      "Original Output:\n",
      " @rtype : coordinates\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " returns the coordinates of the point in the coordinates of the point\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 48756 =================\n",
      "\n",
      "Original Input:\n",
      " def appendXml self r data json dumps self __nj e etree SubElement r numjobs e text data data json dumps self __iod e etree SubElement r iodepth e text data data json dumps self __runtime e etree SubElement r runtime e text data if self __xargs None data json dumps list self __xargs e etree SubElement r xargs e text data\n",
      " \n",
      "\n",
      "Original Output:\n",
      " append the information about options to a xml node . @param root the xml root tag to append the new elements to\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " dumps an object into a json string\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 109535 =================\n",
      "\n",
      "Original Input:\n",
      " def time_to_expire self expiry_time self reservation_due_date datetime now timezone utc return expiry_time total_seconds 1000\n",
      " \n",
      "\n",
      "Original Output:\n",
      " return expiry time in milliseconds : return : decimal\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " returns the expiry time of the expire time\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 41339 =================\n",
      "\n",
      "Original Input:\n",
      " def get_data_path dataset_name local_root local_repo path if local_root startswith gs return os path join local_root local_repo path return tensorport get_data_path dataset_name dataset_name local_root local_root local_repo local_repo path path\n",
      " \n",
      "\n",
      "Original Output:\n",
      " \"dataset specification , see : get_data_path , https://tensorport.com/documentation/api/#get_data_path\"\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " get the data for the given dataset\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 66851 =================\n",
      "\n",
      "Original Input:\n",
      " def notifyStatus self status if status CCS_CONNECTED self ccsConnectedNotify elif status CCS_CONNECTED2 self ccsConnected2Notify elif status EGSE_ENABLED_ACK1 self egseEnabledAck1Notify elif status EGSE_ENABLED_NAK1 self egseEnabledNak1Notify elif status EGSE_DISABLED_ACK1 self egseDisabledAck1Notify elif status EGSE_ENABLED_ACK2 self egseEnabledAck2Notify elif status EGSE_ENABLED_NAK2 self egseEnabledNak2Notify elif status EGSE_DISABLED_ACK2 self egseDisabledAck2Notify\n",
      " \n",
      "\n",
      "Original Output:\n",
      " generic callback when something changes in the model\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " private method to check if the status of the given button is clicked\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 23126 =================\n",
      "\n",
      "Original Input:\n",
      " def _validate_page self webdriver if google com not in webdriver current_url raise InvalidPageError This is not google\n",
      " \n",
      "\n",
      "Original Output:\n",
      " check to make sure we 're on google.com\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " validate the page\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 84688 =================\n",
      "\n",
      "Original Input:\n",
      " def set_Password self value super PaymentDetailsInputSet self _set_input Password value\n",
      " \n",
      "\n",
      "Original Output:\n",
      " \"set the value of the password input for this choreo . ( ( required , password ) the api password provided by paypal . )\"\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " set the value of the password input for this choreo required password your zendesk api\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 100223 =================\n",
      "\n",
      "Original Input:\n",
      " def get_related_view request return request environ get cone app related_view None\n",
      " \n",
      "\n",
      "Original Output:\n",
      " return related view name from request .\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " return the view for the related views\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 151417 =================\n",
      "\n",
      "Original Input:\n",
      " def _certifi_where_for_ssl_version if not ssl return if ssl OPENSSL_VERSION_INFO 1 0 2 warnings warn You are using an outdated version of OpenSSL that can t use stronger root certificates return certifi old_where return certifi where\n",
      " \n",
      "\n",
      "Original Output:\n",
      " gets the right location for certifi certifications for the current ssl version .\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " returns a list of all the ssl version of the given ssl version\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 97100 =================\n",
      "\n",
      "Original Input:\n",
      " def ServerIsHealthy self if not self _ServerIsRunning return False try self _ExecuteCommand self _gocode_binary_path sock tcp addr self _gocode_host status return True except RuntimeError as error _logger exception error return False\n",
      " \n",
      "\n",
      "Original Output:\n",
      " check if the gocode server is healthy ( up and serving ) .\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " check if the server is running\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 75715 =================\n",
      "\n",
      "Original Input:\n",
      " def set_AccessToken self value super RetrieveProfileInputSet self _set_input AccessToken value\n",
      " \n",
      "\n",
      "Original Output:\n",
      " \"set the value of the accesstoken input for this choreo . ( ( required , string ) the access token retrieved after the final step in the oauth process . )\"\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " set the value of the accesstoken input for this choreo required string the access token\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 37719 =================\n",
      "\n",
      "Original Input:\n",
      " def make_differ implementations return Differ impl DiffPrimitives impl DiffNumbers impl DiffText impl DiffLists impl DiffDicts implementations\n",
      " \n",
      "\n",
      "Original Output:\n",
      " generate a fancy differ with extra implementations\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " make a factory for a c c c c c c c c c c\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 180207 =================\n",
      "\n",
      "Original Input:\n",
      " def test_db metrics None return DatabaseManager router_conf DDBTableConfig tablename router router Mock spec Router message_conf DDBTableConfig tablename message metrics SinkMetrics if metrics is None else metrics resource autopush tests boto_resource\n",
      " \n",
      "\n",
      "Original Output:\n",
      " return a test databasemanager : its storage / router are mocked\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " test if the database is available\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 81561 =================\n",
      "\n",
      "Original Input:\n",
      " def set_Forum self value super ListUsersInputSet self _set_input Forum value\n",
      " \n",
      "\n",
      "Original Output:\n",
      " \"set the value of the forum input for this choreo . ( ( required , string ) forum short name ( i.e. , the subdomain of the disqus site url ) . displays all users contained in that forum . if null , users from all forums moderated by the authenticating user will be retrieved . )\"\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " set the value of the forum input for this choreo required string forum short short\n"
     ]
    }
   ],
   "source": [
    "from seq2seq_utils import Seq2Seq_Inference\n",
    "import pandas as pd\n",
    "\n",
    "seq2seq_inf = Seq2Seq_Inference(encoder_preprocessor=enc_pp,\n",
    "                                 decoder_preprocessor=dec_pp,\n",
    "                                 seq2seq_model=seq2seq_Model)\n",
    "\n",
    "demo_testdf = pd.DataFrame({'code':holdout_code, 'comment':holdout_comment, 'ref':''})\n",
    "seq2seq_inf.demo_model_predictions(n=15, df=demo_testdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>comment</th>\n",
       "      <th>ref</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>function_tokens\\n</td>\n",
       "      <td>docstring_tokens\\n</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>def getall self key default _marker identity s...</td>\n",
       "      <td>return a list of all values matching the key .\\n</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>def getone self key default _marker identity s...</td>\n",
       "      <td>get first value matching the key .\\n</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>def get self key default None return self geto...</td>\n",
       "      <td>get first value matching the key .\\n</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>def keys self return _KeysView self _impl\\n</td>\n",
       "      <td>return a new view of the dictionary 's keys .\\n</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187044</th>\n",
       "      <td>def count_additional_facts_unresolved conversa...</td>\n",
       "      <td>: param conversation : the current conversatio...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187045</th>\n",
       "      <td>def extract_fact_by_type fact_type intent enti...</td>\n",
       "      <td>returns the relevant information for a particu...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187046</th>\n",
       "      <td>def extract_month_from_duration extracted_enti...</td>\n",
       "      <td>\"takes a ner_duckling entity duration classifi...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187047</th>\n",
       "      <td>def is_sufficient self classify_dict if len cl...</td>\n",
       "      <td>\"method which verifies the accuracy of the cla...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187048</th>\n",
       "      <td>def intent_percent_difference self intent_dict...</td>\n",
       "      <td>method used to calculate the math for the perc...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>187049 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     code  \\\n",
       "0                                       function_tokens\\n   \n",
       "1       def getall self key default _marker identity s...   \n",
       "2       def getone self key default _marker identity s...   \n",
       "3       def get self key default None return self geto...   \n",
       "4             def keys self return _KeysView self _impl\\n   \n",
       "...                                                   ...   \n",
       "187044  def count_additional_facts_unresolved conversa...   \n",
       "187045  def extract_fact_by_type fact_type intent enti...   \n",
       "187046  def extract_month_from_duration extracted_enti...   \n",
       "187047  def is_sufficient self classify_dict if len cl...   \n",
       "187048  def intent_percent_difference self intent_dict...   \n",
       "\n",
       "                                                  comment ref  \n",
       "0                                      docstring_tokens\\n      \n",
       "1        return a list of all values matching the key .\\n      \n",
       "2                    get first value matching the key .\\n      \n",
       "3                    get first value matching the key .\\n      \n",
       "4         return a new view of the dictionary 's keys .\\n      \n",
       "...                                                   ...  ..  \n",
       "187044  : param conversation : the current conversatio...      \n",
       "187045  returns the relevant information for a particu...      \n",
       "187046  \"takes a ner_duckling entity duration classifi...      \n",
       "187047  \"method which verifies the accuracy of the cla...      \n",
       "187048  method used to calculate the math for the perc...      \n",
       "\n",
       "[187049 rows x 3 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_testdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['function_tokens\\n',\n",
       " 'def getall self key default _marker identity self _title key res v for i k v in self _impl _items if i identity if res return res if not res and default is not _marker return default raise KeyError Key not found r key\\n',\n",
       " 'def getone self key default _marker identity self _title key for i k v in self _impl _items if i identity return v if default is not _marker return default raise KeyError Key not found r key\\n',\n",
       " 'def get self key default None return self getone key default\\n',\n",
       " 'def keys self return _KeysView self _impl\\n',\n",
       " 'def items self return _ItemsView self _impl\\n',\n",
       " 'def values self return _ValuesView self _impl\\n',\n",
       " 'def copy self return MultiDict self items\\n',\n",
       " 'def copy self return CIMultiDict self items\\n',\n",
       " 'def copy self cls self __class__ return cls self items\\n',\n",
       " 'def extend self args kwargs self _extend args kwargs extend self _extend_items\\n',\n",
       " 'def clear self self _impl _items clear self _impl incr_version\\n',\n",
       " 'def setdefault self key default None identity self _title key for i k v in self _impl _items if i identity return v self add key default return default\\n',\n",
       " 'def popone self key default _marker identity self _title key for i in range len self _impl _items if self _impl _items i 0 identity value self _impl _items i 2 del self _impl _items i self _impl incr_version return value if default is _marker raise KeyError key else return default\\n',\n",
       " 'def popall self key default _marker found False identity self _title key ret for i in range len self _impl _items 1 1 1 item self _impl _items i if item 0 identity ret append item 2 del self _impl _items i self _impl incr_version found True if not found if default is _marker raise KeyError key else return default else ret reverse return ret\\n',\n",
       " 'def popitem self if self _impl _items i self _impl _items pop 0 self _impl incr_version return i 1 i 2 else raise KeyError empty multidict\\n',\n",
       " 'def update self args kwargs self _extend args kwargs update self _update_items\\n',\n",
       " 'def chained_callable module callables callables callables if isinstance callables str else callables _callable rest getattr module name for name in callables def chained_call args kwargs return reduce lambda res c c res rest _callable args kwargs return chained_call if len rest 0 else _callable\\n',\n",
       " 'def get_driver driver_name str normalized_name driver_name lower return drivers normalized_name\\n',\n",
       " 'def on_color_change self led_num red float green float blue float None r_duty grayscale_correction red g_duty grayscale_correction green b_duty grayscale_correction blue start_index 4 led_num self leds start_index 3 r_duty self leds start_index 2 g_duty self leds start_index 1 b_duty\\n',\n",
       " 'def on_brightness_change self led_num int None brightness self _global_brightness self brightness_buffer led_num self leds 4 led_num self led_prefix brightness\\n',\n",
       " 'classmethod def led_prefix cls brightness float int brightness_byte grayscale_correction brightness max_in 1 max_out 31 prefix_byte brightness_byte 31 224 return prefix_byte\\n',\n",
       " 'def close self None self spi close\\n',\n",
       " 'staticmethod def spi_start_frame list return 0 0 0 0\\n',\n",
       " 'def show self None self spi xfer2 self spi_start_frame self spi xfer2 self leds if self __sk9822_compatibility_mode self spi xfer2 self spi_start_frame self spi xfer2 self spi_end_frame self num_leds\\n',\n",
       " 'staticmethod def spi_end_frame num_leds list return 0 num_leds 15 16\\n',\n",
       " 'def notify_user self message qos 0 None paho mqtt publish single topic self conf MQTT notification_path format prefix self conf MQTT prefix sys_name self conf sys_name payload message qos qos hostname self conf MQTT Broker host port self conf MQTT Broker port keepalive self conf MQTT Broker keepalive\\n',\n",
       " 'def on_connect self client userdata flags rc start_path self conf MQTT Path show_start stop_path self conf MQTT Path show_stop client subscribe start_path client subscribe stop_path logger info subscription on Broker host for start_path and stop_path format host self conf MQTT Broker host start_path start_path stop_path stop_path\\n',\n",
       " 'def on_message self client userdata msg topic str msg topic if type msg payload is bytes payload msg payload decode else payload str msg payload logger debug Incoming MQTT message n topic format topic payload format payload if topic self conf MQTT Path show_start payload_tree helpers mqtt parse_json_safely payload show_name payload_tree name try parameters payload_tree parameters except KeyError parameters logger info MQTT command interpreted START the show with given parameters format show_name parameters self stop_running_show self start_show show_name parameters elif topic self conf MQTT Path show_stop logger info MQTT command interpreted STOP the running show self stop_running_show\\n',\n",
       " 'def start_show self show_name str parameters dict None if show_name not in shows logger error Show name was not found format name show_name return try show shows show_name self strip parameters show check_runnable except InvalidStrip InvalidConf InvalidParameters as error_message logger error error_message self start_show clear return logger info Starting the show show_name self show_process Process target show start name show_name self show_process start self mqtt publish topic self conf MQTT Path show_current payload show_name qos 1 retain True\\n',\n",
       " 'def stop_show self show_name str None if show_name self show_process name or show_name all self stop_running_show\\n',\n",
       " 'def stop_running_show self timeout_sec float 1 None if self show_process is_alive os kill self show_process pid signal SIGINT self show_process join timeout_sec if self show_process is_alive logger info show_name is running Terminating format show_name self show_process name self show_process terminate else logger debug no show running nothing to stop self mqtt publish topic self conf MQTT Path show_current payload qos 1 retain True\\n',\n",
       " 'def run self None logger info Starting name format name self conf sys_name logger info Initializing LED strip driver get_driver self conf Strip driver self strip driver num_leds self conf Strip num_leds max_clock_speed_hz self conf Strip max_clock_speed_hz max_global_brightness self conf Strip max_brightness_percent self strip set_global_brightness_percent self conf Strip initial_brightness_percent self strip sync_up logger info Connecting to the MQTT Broker if self conf MQTT username is not None self mqtt username_pw_set self conf MQTT username self conf MQTT password self mqtt connect self conf MQTT Broker host self conf MQTT Broker port self conf MQTT Broker keepalive logger info name is ready format name self conf sys_name self start_show clear try signal signal signal SIGTERM self stop_controller self mqtt loop_forever except KeyboardInterrupt self stop_controller finally logger critical MQTTControl py has exited\\n',\n",
       " 'def stop_controller self signum None frame None del self strip\\n',\n",
       " 'staticmethod def unknown param_name str None if param_name debug_str Parameter name is unknown format name param_name else debug_str Parameter is unknown return InvalidParameters debug_str\\n',\n",
       " 'staticmethod def missing param_name str None if param_name debug_str Parameter name is missing format name param_name else debug_str Parameter is missing return InvalidParameters debug_str\\n',\n",
       " 'def get_logo filename str logo str with open filename encoding unicode_escape as file contents file read return contents rstrip rstrip\\n',\n",
       " 'def get_version filename str version str with open filename as file contents file read return contents rstrip rstrip\\n',\n",
       " 'def __del__ self self close del self color_buffer self synced_red_buffer self synced_green_buffer self synced_blue_buffer del self brightness_buffer self synced_brightness_buffer logger info Driver successfully closed\\n',\n",
       " 'property def __frozen self return self __is_frozen\\n',\n",
       " 'abstractmethod def close self None pass\\n',\n",
       " 'def freeze self None self __frozen True\\n',\n",
       " 'def unfreeze self None self __frozen False\\n',\n",
       " 'def get_pixel self led_num int tuple return self color_buffer led_num\\n',\n",
       " 'def set_pixel self led_num int red float green float blue float None if led_num 0 return if led_num self num_leds return if not self __frozen self color_buffer led_num red green blue self on_color_change led_num red green blue\\n',\n",
       " 'abstractmethod def on_color_change self led_num red float green float blue float None\\n',\n",
       " 'def set_pixel_bytes self led_num int rgb_color int None red green blue self color_bytes_to_tuple rgb_color self set_pixel led_num red green blue\\n',\n",
       " 'staticmethod def color_tuple_to_bytes red float green float blue float int red round red green round green blue round blue return red 16 green 8 blue\\n',\n",
       " 'staticmethod def color_bytes_to_tuple rgb_color int tuple r rgb_color 16711680 16 g rgb_color 65280 8 b rgb_color 255 return r g b\\n',\n",
       " 'abstractmethod def show self None pass\\n',\n",
       " 'def rotate self positions int 1 None self color_buffer self color_buffer positions self color_buffer positions for led_num in range self num_leds r g b self get_pixel led_num self on_color_change led_num r g b\\n',\n",
       " 'def set_brightness self led_num int brightness float None if self __frozen return if brightness 0 0 brightness 0 0 elif brightness 1 0 brightness 1 0 self brightness_buffer led_num brightness self on_brightness_change led_num\\n',\n",
       " 'abstractmethod def on_brightness_change self led_num int None pass\\n',\n",
       " 'def set_global_brightness self brightness float None if brightness 0 0 self _global_brightness 0 0 elif brightness self __max_global_brightness self _global_brightness self __max_global_brightness else self _global_brightness brightness for led_num in range self num_leds self on_brightness_change led_num\\n',\n",
       " 'def set_global_brightness_percent self brightness float None self set_global_brightness 100 brightness\\n',\n",
       " 'def clear_buffer self None for led_num in range self num_leds self set_pixel led_num 0 0 0\\n',\n",
       " 'def clear_strip self None self clear_buffer self show\\n',\n",
       " 'def sync_up self None logger info sync up for led_num red green blue in enumerate self color_buffer self synced_red_buffer led_num red self synced_green_buffer led_num green self synced_blue_buffer led_num blue self synced_brightness_buffer led_num self brightness_buffer led_num\\n',\n",
       " 'def sync_down self None logger info sync down for led_num _ in enumerate self color_buffer red self synced_red_buffer led_num green self synced_green_buffer led_num blue self synced_blue_buffer led_num self color_buffer led_num red green blue self on_color_change led_num red green blue self brightness_buffer led_num self synced_brightness_buffer led_num self on_brightness_change led_num\\n',\n",
       " 'def get_from_topic hierarchy_level int topic str str hierarchy topic split sep return hierarchy hierarchy_level\\n',\n",
       " 'def parse_json_safely payload str dict if payload try unpacked json loads payload except Exception as error logger debug Could not parse this payload format error return else if type unpacked is not dict logger debug This payload is not a JSON object return return unpacked else logger debug Payload is empty return\\n',\n",
       " 'def numeric candidate param_name str None minimum float None maximum float None if param_name debug_str Parameter name must be a number format name param_name else debug_str Parameter must be a number if minimum is not None and maximum is not None debug_str between min and max format min minimum max maximum elif minimum is not None debug_str format minimum elif maximum is not None debug_str format maximum debug_str got format candidate if type candidate not in float int raise InvalidParameters debug_str if minimum is not None and candidate minimum raise InvalidParameters debug_str if maximum is not None and candidate maximum raise InvalidParameters debug_str\\n',\n",
       " 'def not_negative_numeric candidate param_name str None if type candidate not in float int or candidate 0 if param_name debug_str Parameter name must be a non negative number format name param_name else debug_str Parameter must be a not negative number raise InvalidParameters debug_str\\n',\n",
       " 'def positive_numeric candidate param_name str None if type candidate not in float int or candidate 0 if param_name debug_str Parameter name must be a positive number format name param_name else debug_str Parameter must be a positive number raise InvalidParameters debug_str\\n',\n",
       " 'def integer candidate param_name str None minimum float None maximum float None if param_name debug_str Parameter name must be an integer format name param_name else debug_str Parameter must be an integer if minimum is not None and maximum is not None debug_str between min and max format min minimum max maximum elif minimum is not None debug_str format minimum elif maximum is not None debug_str format maximum debug_str got format candidate if type candidate is not int raise InvalidParameters debug_str if minimum is not None and candidate minimum raise InvalidParameters debug_str if maximum is not None and candidate maximum raise InvalidParameters debug_str\\n',\n",
       " 'def not_negative_integer candidate param_name str None if type candidate is not int or candidate 0 if param_name debug_str Parameter name must be a non negative integer format name param_name else debug_str Parameter must be a not negative integer raise InvalidParameters debug_str\\n',\n",
       " 'def positive_integer candidate param_name str None if type candidate is not int or candidate 0 if param_name debug_str Parameter name must be a positive integer format name param_name else debug_str Parameter must be a positive integer raise InvalidParameters debug_str\\n',\n",
       " 'def boolean candidate param_name str None if type candidate is not bool if param_name debug_str Parameter name must be a boolean value format name param_name else debug_str Parameter must be a boolean value raise InvalidParameters debug_str\\n',\n",
       " 'def rgb_color_tuple candidate param_name str None if param_name debug_str Parameter name must be an RGB color tuple format name param_name else debug_str Parameter must be an RGB color tuple if type candidate is not tuple raise InvalidParameters debug_str if len candidate is not 3 raise InvalidParameters debug_str for component in candidate try numeric component minimum 0 maximum 255 except InvalidParameters raise InvalidParameters debug_str\\n',\n",
       " 'def grayscale_correction lightness float max_in float 255 0 max_out int 255 Corrects the non linear human perception of the led brightness according to the CIE 1931 standard This is commonly mistaken for gamma correction gamma vs lightness _ admonition CIE 1931 Lightness correction cie1931 source _ The human perception of brightness is not linear to the duty cycle of an LED The relation between the perceived lightness math Y and the technical lightness math L was described by the CIE math nowrap begin align Y Y_ max cdot g L 16 116 quad quad 0 le L le 100 text with quad g t begin cases 3 cdot delta 2 cdot t frac 4 29 t le delta t 3 t delta end cases quad quad delta frac 6 29 end align For more efficient computation these two formulas can be simplified to math Y begin cases L 902 33 L le 8 L 16 116 3 L 8 end cases 0 le Y le 1 qquad 0 le L le 100 gamma vs lightness For more information read here https goo gl 9Ji129 cie1931 source formula from Wikipedia https en wikipedia org wiki Lab_color_space Reverse_transformation _ param lightness linear brightness value between 0 and max_in param max_in maximum value for lightness param max_out maximum output integer value 255 for 8 bit LED drivers return the correct PWM duty cycle for humans to see the desired lightness as integer if lightness 0 return 0 elif lightness max_in return max_out l_star lightness max_in 100 if l_star 8 duty_cycle l_star 902 33 else duty_cycle l_star 16 116 3 return round duty_cycle max_out\\n',\n",
       " 'def wheel wheel_pos float if wheel_pos 254 wheel_pos 254 if wheel_pos 85 color wheel_pos 3 255 wheel_pos 3 0 elif wheel_pos 170 wheel_pos 85 color 255 wheel_pos 3 0 wheel_pos 3 else wheel_pos 170 color 0 wheel_pos 3 255 wheel_pos 3 return color\\n',\n",
       " 'def linear_dim undimmed tuple factor float tuple dimmed for i in undimmed i factor dimmed i return dimmed\\n',\n",
       " 'def add_tuples tuple1 tuple tuple2 tuple if len tuple1 is not len tuple2 return None sum_of_two for i in range len tuple1 sum_of_two append tuple1 i tuple2 i return tuple sum_of_two\\n',\n",
       " 'def blend_whole_strip_to_color strip LEDStrip color tuple fadetime_sec float 2 None transition SmoothBlend strip transition set_color_for_whole_strip color transition blend time_sec fadetime_sec\\n',\n",
       " 'def set_pixel self led_num int red float green float blue float try verify rgb_color_tuple red green blue except exceptions InvalidParameters as error_message self logger error error_message self target_colors led_num red green blue\\n',\n",
       " 'def set_color_for_whole_strip self red float green float blue float for led_num in range self strip num_leds self set_pixel led_num red green blue\\n',\n",
       " 'def blend self time_sec float 2 blend_function types FunctionType BlendFunctions linear_blend initial_colors for led_num in range self strip num_leds initial_colors append self strip get_pixel led_num now time perf_counter end_time time perf_counter time_sec while now end_time fade_progress end_time now time_sec for led_num in range self strip num_leds color blend_function initial_colors led_num self target_colors led_num fade_progress self strip set_pixel led_num color self strip show now time perf_counter for led_num in range self strip num_leds self strip set_pixel led_num self target_colors led_num self strip show\\n',\n",
       " 'def update_settings_tree base ConfigTree update ConfigTree ConfigTree updated deepcopy base for _ key in enumerate update if type update key is ConfigTree if key in base updated key update_settings_tree base key update key else updated key update key else updated key update key return updated\\n',\n",
       " 'def get_configuration default_filename str defaults yml user_filename str config yml ConfigTree with open default_filename r as file defaults yaml load file logger info Successfully parsed as default configuration format default_filename with open user_filename r as file user_config yaml load file logger info Successfully parsed as user configuration format user_filename configuration update_settings_tree base defaults update user_config for path_name in configuration MQTT Path path_template configuration MQTT Path path_name path path_template format prefix configuration MQTT prefix sys_name configuration sys_name configuration MQTT Path path_name path return configuration\\n',\n",
       " 'def init_parameters self None self register pause_sec 0 verify not_negative_numeric self register num_steps_per_cycle None verify positive_integer self register num_cycles float inf verify positive_integer\\n',\n",
       " 'def check_runnable self None if self p value pause_sec is None raise InvalidParameters Missing parameter pause_sec if self p value num_steps_per_cycle is None raise InvalidParameters Missing parameter num_steps_per_cycle if self p value num_cycles is None raise InvalidParameters Missing parameter num_cycles\\n',\n",
       " 'abstractmethod def before_start self None pass\\n',\n",
       " 'def shutdown self None pass\\n',\n",
       " 'abstractmethod def update self current_step int current_cycle int bool raise NotImplementedError Please implement the update method\\n',\n",
       " 'def run self None self before_start self strip show current_cycle 0 while True for currentStep in range self p value num_steps_per_cycle need_repaint self update currentStep current_cycle if need_repaint self strip show self sleep self p value pause_sec current_cycle 1 if current_cycle self p value num_cycles break self cleanup\\n',\n",
       " 'def transit lon1 lon2 lon1 Math AngNormalize lon1 lon2 Math AngNormalize lon2 lon12 Math AngDiff lon1 lon2 cross 1 if lon1 0 and lon2 0 and lon12 0 else 1 if lon2 0 and lon1 0 and lon12 0 else 0 return cross\\n',\n",
       " 'def transitdirect lon1 lon2 lon1 math fmod lon1 720 0 lon2 math fmod lon2 720 0 return 0 if lon2 0 and lon2 360 or lon2 360 else 1 0 if lon1 0 and lon1 360 or lon1 360 else 1\\n',\n",
       " 'def Clear self self _num 0 self _crossings 0 if not self _polyline self _areasum Set 0 self _perimetersum Set 0 self _lat0 self _lon0 self _lat1 self _lon1 Math nan\\n',\n",
       " 'def AddPoint self lat lon if self _num 0 self _lat0 self _lat1 lat self _lon0 self _lon1 lon else _ s12 _ _ _ _ _ S12 self _earth GenInverse self _lat1 self _lon1 lat lon self _mask self _perimetersum Add s12 if not self _polyline self _areasum Add S12 self _crossings PolygonArea transit self _lon1 lon self _lat1 lat self _lon1 lon self _num 1\\n',\n",
       " 'def AddEdge self azi s if self _num 0 _ lat lon _ _ _ _ _ S12 self _earth GenDirect self _lat1 self _lon1 azi False s self _mask self _perimetersum Add s if not self _polyline self _areasum Add S12 self _crossings PolygonArea transitdirect self _lon1 lon self _lat1 lat self _lon1 lon self _num 1\\n',\n",
       " 'def Compute self reverse sign if self _polyline area Math nan if self _num 2 perimeter 0 if not self _polyline area 0 return self _num perimeter area if self _polyline perimeter self _perimetersum Sum return self _num perimeter area _ s12 _ _ _ _ _ S12 self _earth GenInverse self _lat1 self _lon1 self _lat0 self _lon0 self _mask perimeter self _perimetersum Sum s12 tempsum Accumulator self _areasum tempsum Add S12 crossings self _crossings PolygonArea transit self _lon1 self _lon0 if crossings 1 tempsum Add 1 if tempsum 0 else 1 self _area0 2 if not reverse tempsum Negate if sign if tempsum Sum self _area0 2 tempsum Add self _area0 elif tempsum Sum self _area0 2 tempsum Add self _area0 elif tempsum Sum self _area0 tempsum Add self _area0 elif tempsum Sum 0 tempsum Add self _area0 area 0 tempsum Sum return self _num perimeter area\\n',\n",
       " 'def TestPoint self lat lon reverse sign if self _polyline area Math nan if self _num 0 perimeter 0 if not self _polyline area 0 return 1 perimeter area perimeter self _perimetersum Sum tempsum 0 if self _polyline else self _areasum Sum crossings self _crossings num self _num 1 for i in 0 if self _polyline else 0 1 _ s12 _ _ _ _ _ S12 self _earth GenInverse self _lat1 if i 0 else lat self _lon1 if i 0 else lon self _lat0 if i 0 else lat self _lon0 if i 0 else lon self _mask perimeter s12 if not self _polyline tempsum S12 crossings PolygonArea transit self _lon1 if i 0 else lon self _lon0 if i 0 else lon if self _polyline return num perimeter area if crossings 1 tempsum 1 if tempsum 0 else 1 self _area0 2 if not reverse tempsum 1 if sign if tempsum self _area0 2 tempsum self _area0 elif tempsum self _area0 2 tempsum self _area0 elif tempsum self _area0 tempsum self _area0 elif tempsum 0 tempsum self _area0 area 0 tempsum return num perimeter area\\n',\n",
       " 'def TestEdge self azi s reverse sign if self _num 0 return 0 Math nan Math nan num self _num 1 perimeter self _perimetersum Sum s if self _polyline return num perimeter Math nan tempsum self _areasum Sum crossings self _crossings _ lat lon _ _ _ _ _ S12 self _earth GenDirect self _lat1 self _lon1 azi False s self _mask tempsum S12 crossings PolygonArea transitdirect self _lon1 lon _ s12 _ _ _ _ _ S12 self _earth GenInverse lat lon self _lat0 self _lon0 self _mask perimeter s12 tempsum S12 crossings PolygonArea transit lon self _lon0 if crossings 1 tempsum 1 if tempsum 0 else 1 self _area0 2 if not reverse tempsum 1 if sign if tempsum self _area0 2 tempsum self _area0 elif tempsum self _area0 2 tempsum self _area0 elif tempsum self _area0 tempsum self _area0 elif tempsum 0 tempsum self _area0 area 0 tempsum return num perimeter area\\n',\n",
       " 'def CurrentPoint self return self _lat1 self _lon1\\n',\n",
       " 'def Area earth points polyline poly PolygonArea earth polyline for p in points poly AddPoint p lat p lon return poly Compute False True\\n',\n",
       " 'def norm x y r math hypot x y return x r y r\\n',\n",
       " 'def polyval N p s x y float 0 if N 0 else p s while N 0 N 1 s 1 y y x p s return y\\n',\n",
       " 'def AngRound x z 1 16 0 y abs x if y z y z z y return 0 y if x 0 else y\\n',\n",
       " 'def _parse_date date_string try return np where isinstance parse date_string datetime datetime parse date_string Error except Exception as e return You entered an incorrect date Check your date format\\n',\n",
       " 'def _dateformatter date_string return parse date_string strftime Y m d\\n',\n",
       " 'def _dateRanger originalArray minutes list map str range 0 60 15 hours list map str range 0 24 times for l in hours if int l 10 l 0 l for k in minutes if k 0 k 00 times append 0 1 format l k if isinstance originalArray str Check user input to retrieve date query return np where len originalArray 0 crazy _parse_date originalArray elif isinstance originalArray list if len originalArray 1 return np array parse join originalArray elif len originalArray 2 return np array list map lambda x parse x originalArray else cleaner np vectorize _dateformatter converted cleaner originalArray tolist dates np arange converted 0 converted 1 dtype datetime64 D dates list map lambda x datetime datetime combine x datetime datetime min time dates tolist if len originalArray 2 adder np datetime64 parse converted 1 date adder datetime datetime combine adder tolist datetime datetime min time return np append dates adder else pass return np array dates\\n',\n",
       " 'def _gdeltRangeString element coverage None version 2 0 minutes list map str range 0 60 15 hours list map str range 0 24 times for l in hours if int l 10 l 0 l for k in minutes if k 0 k 00 times append 0 1 format l k element element tolist hour datetime datetime now hour multiplier datetime datetime now minute 15 multiple 15 multiplier conditioner multiplier 1 if not isinstance element list if element date datetime datetime now date if coverage and int version 1 converted np array list map lambda x np datetime64 parse str element x tolist strftime Y m d H M S times hour 4 conditioner else converted datetime datetime now replace minute multiple second 0 strftime Y m d H M S elif coverage and int version 1 converted restOfDay np array list map lambda x np datetime64 parse str element x tolist strftime Y m d H M S times else converted element replace minute int multiple second 0 strftime Y m d H M S if parse converted datetime datetime now converted element replace minute 45 second 0 hour 23 strftime Y m d H M S else if isinstance element list is True converted list map lambda x datetime datetime combine x datetime time min datetime timedelta minutes 45 hours 23 strftime Y m d H M S element else converted datetime datetime combine element datetime time min datetime timedelta minutes 45 hours 23 strftime Y m d H M S if coverage and int version 1 converted for i in element converted append np array list map lambda x np datetime64 parse str i x tolist strftime Y m d H M S times converted np concatenate converted axis 0 if len converted tolist 5 192 warnText This query will download 0 files and likely exhaust your memory with possibly 10s of GBs of data in this single query Hit Ctr C to kill this query if you do not want to continue format len converted tolist warnings warn warnText if int version 1 if isinstance converted list is True converted list map lambda x np where parse x parse 2013 04 01 parse x strftime Y m d H M S 8 np where parse x parse 2006 01 01 and int version 1 parse x strftime Y m d H M S 4 parse x strftime Y m d H M S 6 converted converted list map lambda x x tolist converted converted list set converted else converted np where parse converted parse 2013 04 01 parse converted strftime Y m d H M S 8 np where parse converted parse 2006 01 01 and int version 1 parse converted strftime Y m d H M S 4 parse converted strftime Y m d H M S 6 tolist return converted\\n',\n",
       " 'def test_separate_rangestring_24hours self date_array np array datetime datetime 2017 8 6 0 0 dtype object bottom_number datetime datetime now time hour 4 ranger_test _gdeltRangeString date_array coverage True return self assertGreaterEqual ranger_test shape 0 bottom_number\\n',\n",
       " 'mock patch object gdelt parallel requests get def test_parallel_events2_pass self mock_B ver sys version_info major spam pd read_pickle os path join gdelt base BASE_DIR data events2samp gz compression gzip drop CAMEOCodeDescription axis 1 spam columns np arange len spam columns spam 26 27 28 spam 26 27 28 astype str if ver 3 buffer StringIO spam to_csv buffer sep t header False index False else buffer BytesIO spam to_csv buffer sep t header False index False inMemoryOutputFile BytesIO zipFile ZipFile inMemoryOutputFile w zipFile writestr OEBPS 20170701234500 export CSV zip buffer getvalue zipFile close response mock_B response status_code 200 response content inMemoryOutputFile getvalue response return_value inMemoryOutputFile url http data gdeltproject org gdeltv2 20170701234500 export CSV zip res _mp_worker url table events return self assertTrue res 0 1 2 3 equals spam 0 1 2 3 Returned dataframe\\n',\n",
       " 'mock patch object gdelt parallel requests get def test_parallel_404_warnings self mock_B response mock_B response status_code 404 url http data gdeltproject org gdeltv2 20170727234500 export CSV zip exp GDELT does not have a url for date time 20170727234500 GDELT did not return data for date time 20170727234500 with warnings catch_warnings record True as w warnings simplefilter always res _mp_worker url table events return self assertEqual str w 0 message str w 1 message exp Returned dataframe\\n',\n",
       " 'mock patch object gdelt parallel requests get def test_parallel_gkgv2_pass self mock_B spam pd read_pickle os path join gdelt base BASE_DIR data gkg2samp gz compression gzip spam columns np arange len spam columns ver sys version_info major if ver 3 buffer StringIO spam to_csv buffer sep t header False index False else buffer BytesIO spam to_csv buffer sep t header False index False encoding utf 8 inMemoryOutputFile BytesIO zipFile ZipFile inMemoryOutputFile w zipFile writestr OEBPS 20170701234500 gkg csv zip buffer getvalue zipFile close response mock_B response status_code 200 response content inMemoryOutputFile getvalue response return_value inMemoryOutputFile url http data gdeltproject org gdeltv2 20170701234500 gkg csv zip res _mp_worker url table gkg return self assertTrue res 0 1 2 3 equals spam 0 1 2 3 Returned dataframe\\n',\n",
       " 'mock patch object gdelt parallel requests get def test_parallel_mentions_pass self mock_B spam pd read_pickle os path join gdelt base BASE_DIR data mentionssamp gz compression gzip spam columns np arange len spam columns ver sys version_info major if ver 3 buffer StringIO spam to_csv buffer sep t header False index False else buffer BytesIO spam to_csv buffer sep t header False index False encoding utf 8 inMemoryOutputFile BytesIO zipFile ZipFile inMemoryOutputFile w zipFile writestr OEBPS 20170701234500 mentions CSV zip buffer getvalue zipFile close response mock_B response status_code 200 response content inMemoryOutputFile getvalue response return_value inMemoryOutputFile url http data gdeltproject org gdeltv2 20170701234500 mentions CSV zip res _mp_worker url table gkg return self assertTrue res 0 1 2 3 equals spam 0 1 2 3 Returned dataframe\\n',\n",
       " 'def _events1Heads conte requests get https raw githubusercontent com linwoodc3 gdeltPyR master utils schema_csvs GDELT_1 0_event_Column_Labels_Header_Row_Sep2016 tsv data BytesIO conte content eventsDbHeaders pd read_csv data delimiter t usecols tableId return eventsDbHeaders tableId tolist\\n',\n",
       " 'def _urlBuilder dateString version table events translation False if version 2 base http data gdeltproject org gdeltv2 if version 1 base http data gdeltproject org if table events if version 1 base events if not translation caboose export CSV zip else caboose translation export CSV zip elif table mentions if not translation caboose mentions CSV zip else caboose translation mentions CSV zip elif table gkg if version 1 base gkg if isinstance dateString str comp _testdate dateString if comp parse 2013 Apr 1 raise Exception GDELT 1 0 Global Knowledge Graph requires dates greater than or equal to April 1 2013 elif isinstance dateString list or isinstance dateString np ndarray if not np all list map lambda x x parse 2013 04 01 list map _testdate dateString raise Exception GDELT 1 0 Global Knowledge Graph requires dates greater than or equal to April 1 2013 if not translation caboose gkg csv zip else caboose translation gkg csv zip else raise ValueError You entered an incorrect GDELT table type Choose between events mentions and gkg if isinstance dateString list is True or isinstance dateString np ndarray is True newdate olddateString dateString date dateString for l in date if len l 4 test str datetime datetime strptime l Y newdate append test elif len l 6 test str datetime datetime strptime l Y m newdate append test else test str parse str l newdate append test if version 1 if table gkg base if not np all list map lambda x x parse 2013 04 01 list map _testdate dateString return list map lambda x base x zip if _testdate x date parse 2013 04 01 date else base x caboose dateString else return list map lambda x base x caboose olddateString elif isinstance dateString str is True or len dateString 1 if version 1 if table events if len dateString 4 comp datetime datetime strptime dateString Y elif len dateString 6 comp datetime datetime strptime dateString Y m else comp parse dateString if comp parse 2013 Apr 01 caboose zip elif table events caboose export CSV zip if isinstance dateString list is True or isinstance dateString np ndarray is True dateString dateString 0 if parse dateString 0 parse 2013 Apr 01 caboose zip return base dateString caboose\\n',\n",
       " 'def _geofilter frame try import geopandas as gpd try filresults frame frame ActionGeo_Lat notnull frame ActionGeo_Long notnull except filresults frame frame actiongeolat notnull frame actiongeolong notnull gdf gpd GeoDataFrame filresults assign geometry _parallelize_dataframe filresults crs init epsg 4326 gdf columns list map lambda x x replace _ lower gdf columns final gdf gdf geometry notnull return final except raise ValueError The shaper function did not complete check your input or make sure shapely geopandas fiona are installed\\n',\n",
       " 'def _shaper row try from shapely geometry import Point except raise ImportError You need to install shapely to use this feature try import fiona except raise ImportError You need to install fiona to use this feature try import geopandas except raise ImportError You need to install geopandas to use this feature geometry Point row ActionGeo_Long row ActionGeo_Lat return geometry\\n',\n",
       " 'def _rooturl row s row SOURCEURL r re compile http A Za z0 9 r2 re compile https A Za z0 9 if r search s return r search s group elif r2 search s return r2 search s group\\n',\n",
       " 'def _testdate dateString if len dateString 4 comp datetime datetime strptime dateString Y elif len dateString 6 comp datetime datetime strptime dateString Y m else comp parse dateString return comp\\n',\n",
       " 'def _tableinfo table cameo version 2 table table lower valid cameo events gkg vgkg iatv graph ments mentions cloudviz cloud vision vision if table not in valid raise ValueError You entered this is not a valid table name Choose from format table join valid if table cameo tabs pd read_json os path join BASE_DIR data cameoCodes json dtype cameoCode str GoldsteinScale np float64 tabs set_index cameoCode drop False inplace True elif table events and float version 1 0 tabs pd read_csv os path join BASE_DIR data events1 csv elif table events and float version 2 0 tabs pd read_csv os path join BASE_DIR data events2 csv elif table gkg or table graph tabs pd read_csv os path join BASE_DIR data gkg2 csv elif table mentions or table ments if float version 2 0 raise ValueError GDELT 1 0 does not have a mentions table else tabs pd read_csv os path join BASE_DIR data mentions csv elif table cloud vision or table vgkg or table cloudviz or table vision tabs pd read_csv os path join BASE_DIR data visualgkg csv elif table iatv or table tv tabs pd read_csv os path join BASE_DIR data iatv csv return tabs\\n',\n",
       " 'def test_codedataframe self f os path join BASE_DIR data cameoCodes json resp pd read_json f dtype cameoCode str GoldsteinScale np float64 resp set_index cameoCode drop False inplace True print This is format gdelt __file__ return self assertTrue resp equals codes\\n',\n",
       " 'def _mp_worker url table None warnings filterwarnings ignore have mixed types Specify dtype time sleep 0 001 r requests get url timeout 5 if r status_code 404 message GDELT does not have a url for date time 0 format re search 0 9 4 18 url group warnings warn message start datetime datetime now try buffer BytesIO r content if table events frame pd read_csv buffer compression zip sep t header None warn_bad_lines False dtype 26 str 27 str 28 str elif table gkg frame pd read_csv buffer compression zip sep t header None warn_bad_lines False else frame pd read_csv buffer compression zip sep t header None warn_bad_lines False buffer flush buffer close return frame except try message GDELT did not return data for date time 0 format re search 0 9 4 18 url group warnings warn message except message No data returned for 0 format r url warnings warn message\\n',\n",
       " 'def _date_input_check date version if isinstance date str if date if parse date datetime datetime now raise ValueError Your date is greater than the current date Please enter a relevant date elif parse date parse Feb 18 2015 and int version 1 raise ValueError GDELT 2 0 only supports Feb 18 2015 Present queries currently Try another date if version 1 and parse date date datetime datetime now date raise ValueError You entered today s date for a GDELT 1 0 query GDELT 1 0 s most recent data is always the trailing day i e 0 Please retry your query format np datetime64 datetime datetime now date np timedelta64 1 D if datetime datetime now hour 6 and parse date date datetime datetime now date datetime timedelta days 1 and version 1 raise BaseException GDELT 1 0 posts the latest daily update by 6AM EST The next update will appear in 0 format str datetime datetime combine datetime datetime now datetime datetime min time datetime timedelta hours 6 minutes 0 seconds 0 datetime datetime now elif isinstance date list or isinstance date np ndarray newdate for l in date if len l 4 test str datetime datetime strptime l Y newdate append test elif len l 6 test str datetime datetime strptime l Y m newdate append test else try test str parse str l except test l newdate append test if parse test parse Feb 18 2015 and version 2 raise ValueError GDELT 2 0 only supports Feb 18 2015 Present queries currently Try another date date newdate if len date 1 try if parse join date datetime datetime now raise ValueError Your date is greater than the current date Please enter a relevant date elif version 2 and parse join date parse Feb 18 2015 and int version 1 raise ValueError GDELT 2 0 only supports Feb 18 2015 Present queries currently Try another date except raise ValueError One or more of your input date strings does not parse to a date format Check input elif len date 2 and isinstance date list or isinstance date np ndarray try list map parse date except Exception as exc exc_type exc_value exc_traceback sys exc_info traceback print_tb exc_traceback limit 1 file sys stdout traceback print_exception exc_type exc_value exc_traceback limit 2 file sys stdout raise ValueError One or more of your input date strings does not parse to a date format Check input if not bool parse date 0 parse date 1 raise ValueError Start date greater than or equal to end date Check your entered date query elif not np all np logical_not np array list map parse date datetime datetime now raise ValueError One of your dates is greater than the current date Check your entered date query elif len date 2 try map parse date except Exception as exc exc_type exc_value exc_traceback sys exc_info traceback print_tb exc_traceback limit 1 file sys stdout traceback print_exception exc_type exc_value exc_traceback limit 2 file sys stdout raise ValueError One or more of your input date strings does not parse to a date format Check input if not np all np logical_not np array list map parse date datetime datetime now raise ValueError One or more of your input date strings is greater than today s date Check input elif np any np logical_not np array list map parse date parse Feb 18 2015 True and int version 1 raise ValueError GDELT 2 0 only supports Feb 18 2015 Present queries currently Try another date elif version 1 if not np all np logical_not np array list map lambda x parse x date dtype datetime64 D np datetime64 datetime datetime now date raise ValueError You have today s date in your query for GDELT 1 0 GDELT 1 0 s most recent datais always the trailing day i e 0 Please retry your query format np datetime64 datetime datetime now date np timedelta64 1 D if datetime datetime now hour 6 and datetime datetime now date datetime timedelta days 1 in list map lambda x parse x date date if datetime datetime now hour 6 raise BaseException GDELT 1 0 posts the latest daily update by 6AM EST The next update will appear in 0 format str datetime datetime combine datetime datetime now datetime datetime min time datetime timedelta hours 6 minutes 0 seconds 0 datetime datetime now\\n',\n",
       " 'def test_gdelt_geofilter_events2 self spam pd read_pickle os path join gdelt base BASE_DIR data events2samp gz compression gzip drop CAMEOCodeDescription axis 1 try import geopandas as gpd geo _geofilter spam return self assertIsInstance geo gpd GeoDataFrame except warnings warn Geopandas is not installed Unable to test _geofilter exp Blank return self assertEqual Blank exp\\n',\n",
       " 'mock patch object gdelt getHeaders requests get def test_events2_headers self mock_B spam pandas read_csv os path join gdelt base BASE_DIR data events2 csv encoding utf 8 spam columns tableId dataType Description spam spam assign Empty NULLABLE tableId dataType Empty Description ver sys version_info major if ver 3 buffer StringIO spam to_csv buffer index False data buffer getvalue encode utf 8 else buffer BytesIO spam to_csv buffer index False encoding utf 8 data buffer getvalue response mock_B response content data res _events2Heads return self assertIsInstance res list Returned dataframe\\n',\n",
       " 'mock patch object gdelt getHeaders requests get def test_events1_headers self mock_B spam pandas read_csv os path join gdelt base BASE_DIR data events1 csv name spam columns tableId df_bytes spam to_string index False encode utf 8 response mock_B response content df_bytes res _events1Heads ver sys version_info major exp pd read_csv BytesIO response content print exp return self assertIsInstance exp pandas DataFrame Returned dataframe\\n',\n",
       " 'mock patch object gdelt getHeaders requests get def test_mentions_headers self mock_B spam pandas read_csv os path join gdelt base BASE_DIR data mentions csv spam columns tableId dataType Description ver sys version_info major if ver 3 buffer StringIO spam to_csv buffer sep t index False data buffer getvalue encode utf 8 else buffer BytesIO spam to_csv buffer sep t index False encoding utf 8 data buffer getvalue response mock_B response content data res _mentionsHeads return self assertIsInstance spam pandas DataFrame Returned dataframe\\n',\n",
       " 'mock patch object gdelt getHeaders requests get def test_gkg_headers self mock_B spam pandas read_csv os path join gdelt base BASE_DIR data gkg2 csv spam columns tableId dataType Description ver sys version_info major if ver 3 buffer StringIO spam to_csv buffer sep t index False data buffer getvalue encode utf 8 else buffer BytesIO spam to_csv buffer sep t index False encoding utf 8 data buffer getvalue response mock_B response content data res _gkgHeads return self assertIsInstance spam pandas DataFrame Returned dataframe\\n',\n",
       " 'def test_urlgetter self dd pd read_pickle os path join gdelt base BASE_DIR data events2samp gz compression gzip drop CAMEOCodeDescription axis 1 test dd drop_duplicates SOURCEURL 9 exp np array www cvilletomorrow org www thetowntalk com www comicsbeat com www nbcnews com www thearabweekly com www thestar com my www thetimes co uk timesofindia indiatimes com www woad com dtype object resp test apply _rooturl axis 1 values return np testing assert_array_equal exp resp\\n',\n",
       " 'def test_shaper self dd pd read_pickle os path join gdelt base BASE_DIR data events2samp gz compression gzip drop CAMEOCodeDescription axis 1 try fin dd apply _shaper axis 1 return self assertIsInstance fin pd Series except exp You need to install shapely to use this feature with self assertRaises Exception as context fin dd apply _shaper axis 1 the_exception context exception return self assertIsInstance str the_exception str Not installed\\n',\n",
       " 'def test_cameo self codes pd read_json os path join gdelt base BASE_DIR data cameoCodes json dd pd read_pickle os path join gdelt base BASE_DIR data events2samp gz compression gzip drop CAMEOCodeDescription axis 1 da dd apply lambda x _cameos dd codes return self assertIsInstance da pd Series\\n',\n",
       " 'def testtable_events2 self tabs pd read_csv os path join BASE_DIR data events2 csv return self assertTrue tabs equals _tableinfo events version 2\\n',\n",
       " 'def testtable_gkg2 self tabs pd read_csv os path join BASE_DIR data gkg2 csv return self assertTrue tabs equals _tableinfo gkg version 2\\n',\n",
       " 'def testtable_events1 self tabs pd read_csv os path join BASE_DIR data events1 csv return self assertTrue tabs equals _tableinfo events version 1\\n',\n",
       " 'def testtable_mentspass self tabs pd read_csv os path join BASE_DIR data mentions csv return self assertTrue tabs equals _tableinfo ments version 2\\n',\n",
       " 'def testtable_mentsfail self exp GDELT 1 0 does not have a mentions table with self assertRaises Exception as context checked _tableinfo ments version 1 the_exception context exception return self assertEqual exp str the_exception\\n',\n",
       " 'def testtable_cloudviz self tabs pd read_csv os path join BASE_DIR data visualgkg csv return self assertTrue tabs equals _tableinfo vision version 2\\n',\n",
       " 'def testtable_iatv self tabs pd read_csv os path join BASE_DIR data iatv csv return self assertTrue tabs equals _tableinfo iatv version 2\\n',\n",
       " 'def testtable_cameo self tabs pd read_json os path join BASE_DIR data cameoCodes json dtype cameoCode str GoldsteinScale np float64 return self assertEqual _tableinfo cameo shape tabs shape\\n',\n",
       " 'def testtable_fail self table bobby valid cameo events gkg vgkg iatv graph ments mentions cloudviz cloud vision vision exp You entered this is not a valid table name Choose from format table join valid with self assertRaises Exception as context checked _tableinfo bobby version 1 the_exception context exception return self assertEqual exp str the_exception\\n',\n",
       " 'def test_separate_dates self date_sequence 2016 Jun 10 2016 Jun 15 2016 Jun 25 ranger_test _dateRanger date_sequence exp np array datetime datetime 2016 6 10 0 0 datetime datetime 2016 6 15 0 0 datetime datetime 2016 6 25 0 0 dtype object return np testing assert_array_equal exp ranger_test\\n',\n",
       " 'def test_multidf_gpd_creation self events2 pd read_pickle os path join gdelt base BASE_DIR data events2samp gz compression gzip try events2 events2 assign geometry _parallelize_dataframe events2 return self assertTrue geometry in events2 columns except exp You need to install shapely to use this feature with self assertRaises Exception as context events2 events2 assign geometry _parallelize_dataframe events2 the_exception context exception return self assertIsInstance str the_exception str Not installed\\n',\n",
       " 'mock patch gdelt gdelt Search def test_request_response_version2_events self mock_get gd2 gdelt gdelt version 2 events2 pd read_pickle os path join gdelt base BASE_DIR data events2samp gz compression gzip mock_get return_value mock Mock mock_get return_value dataframe events2 response gd2 Search 2017 Jul 1 table events coverage False return self assertTrue response dataframe equals events2\\n',\n",
       " 'mock patch gdelt gdelt Search def test_request_response_version1_events self mock_get gd1 gdelt gdelt version 1 events1 pd read_pickle os path join gdelt base BASE_DIR data events1samp gz compression gzip mock_get return_value mock Mock mock_get return_value dataframe events1 response gd1 Search 2017 Jul 1 table events coverage False return self assertTrue response dataframe equals events1\\n',\n",
       " 'def findTopology self self listInfo self topologyMatrix buildTopology getTopology self anchorIp self mode\\n',\n",
       " 'def getListIP self listIp if self listInfo is None self findTopology for i in range 0 len self listInfo listIp append self listInfo i routerId return listIp\\n',\n",
       " 'def getTopology self if self topologyMatrix is None self findTopology return self topologyMatrix\\n',\n",
       " 'def findUtilization self addr index self returnIndex addr if index is None return None router router append self findRouterObjFromAddr addr if router 0 is None a a append addr router self getRoutersList a ifs router 0 get_interfaces for i in range 0 len ifs ip ifs i get_address_if for j in range 0 self listInfo index nRoutes if ip self listInfo index str j _ip self listInfo index str j _name ifs i get_name self listInfo index str j _id ifs i get_id self listInfo index str j _speed ifs i get_if_speed self listInfo index str j _utilization ifs i get_in_out_utilization my_router getRouterInfo get_utilization_single_router_polling router self communityString for interface in my_router get_interfaces i self returnNameIndex index interface get_name if i is not None self listInfo index str i _utilization interface get_in_out_utilization return 1\\n',\n",
       " 'def getUtilization self addr if self listInfo is None self findTopology res self findUtilization addr if res is None return None index self returnIndex addr if index is None return None if 0_name not in list self listInfo index keys res self findUtilization addr if res is None return None output for i in range 0 self listInfo index nRoutes output self listInfo index str i _name output self listInfo index str i _name append self listInfo index str i _utilization output self listInfo index str i _name append self listInfo index str i _speed return output\\n',\n",
       " 'def getTunnel self ip if self listInfo is None self findTopology if ip in self confTunnelsDictionary or ip in self lspTableDictionary is False self findTunnel ip results for name in list self lspTableDictionary ip keys attributes self lspTableDictionary ip name getAttributeDict if attributes mplsTunnelRole 1 results name attributes Computed Path for i in results results i insert 0 ip results i pop for j in range 1 len results i k results i j results i j self returnRouter k for i in results results i self removeDuplicates results i return results\\n',\n",
       " 'def findTopology self self listInfo self topologyMatrix buildTopology getTopology self anchorIp\\n',\n",
       " 'def findUtilization self addr index None for k in range 0 len self listInfo if self listInfo k routerId addr index k if index is None return None a a append addr r getRouterInfo get_routers_list a self communityString ifs r 0 get_interfaces for i in range 0 len ifs ip ifs i get_address_if for j in range 0 self listInfo index nRoutes if ip self listInfo index str j _ip self listInfo index str j _name ifs i get_name self listInfo index str j _id ifs i get_id self listInfo index str j _speed ifs i get_if_speed self listInfo index str j _utilization ifs i get_in_out_utilization\\n',\n",
       " 'def __init__ self database namespace dabble if not isinstance database Database raise Exception database argument is not a pymongo database Database self namespace namespace self tests database s tests namespace self results database s results namespace self results ensure_index t ASCENDING i ASCENDING\\n',\n",
       " 'def list_tests self return t _id for t in self tests find fields _id\\n',\n",
       " 'def get_identity self raise Exception Not implemented Use a sub class of IdentityProvider\\n',\n",
       " 'def save_test self test_name alternatives steps raise Exception Not implemented Use a sub class of ResultStorage\\n',\n",
       " 'def record self identity test_name alternative action raise Exception Not implemented Use a sub class of ResultStorage\\n',\n",
       " 'def has_action self identity test_name alternative action raise Exception Not implemented Use a sub class of ResultStorage\\n',\n",
       " 'def set_alternative self identity test_name alternative raise Exception Not implemented Use a sub class of ResultStorage\\n',\n",
       " 'def get_alternative self identity test_name raise Exception Not implemented Use a sub class of ResultStorage\\n',\n",
       " 'def report self test_name a b raise Exception Not implemented Use a sub class of ResultStorage\\n',\n",
       " 'def list_tests self raise Exception Not implemented Use a sub class of ResultStorage\\n',\n",
       " 'def find_lines filename pattern if exists filename with file filename r as fp for line in fp try data json loads line except continue matches True for key value in pattern iteritems matches matches and key in data and data key value if matches yield data\\n',\n",
       " 'def find_line filename pattern for line in find_lines filename pattern return line return None\\n',\n",
       " 'def append_line filename line global lock data json dumps line separators n with lock with file filename a as fp fp seek 0 SEEK_END fp write data\\n',\n",
       " 'def __init__ self directory global lock self directory abspath directory if not exists self directory raise Exception directory s does not exist self directory lock FileLock join self directory lock dabble self tests_path join self directory tests dabble self results_path join self directory results dabble self alts_path join self directory alts dabble\\n',\n",
       " 'def list_tests self return t t for t in find_lines self tests_path\\n',\n",
       " 'def duplicate_object_hook ordered_pairs json_dict for key val in ordered_pairs existing_val json_dict get key if not existing_val json_dict key val elif isinstance existing_val list existing_val append val else json_dict key existing_val val return json_dict\\n',\n",
       " 'pytest mark parametrize access_func lambda pkt pkt 1 lambda pkt pkt icmp lambda pkt pkt ICMP lambda pkt pkt icmp def test_layer_access icmp_packet access_func assert access_func icmp_packet layer_name upper ICMP assert binascii unhexlify access_func icmp_packet data b abcdefghijklmnopqrstuvwabcdefghi\\n',\n",
       " 'def __init__ self remote_host remote_interface remote_port 2002 bpf_filter None only_summaries False decryption_key None encryption_type wpa pwk decode_as None disable_protocol None tshark_path None override_prefs None Creates a new remote capture which will connect to a remote machine which is running rpcapd Use the sniff method to get packets Note The remote machine should have rpcapd running in null authentication mode n Be warned that the traffic is unencrypted param remote_host The remote host to capture on IP or hostname Should be running rpcapd param remote_interface The remote interface on the remote machine to capture on Note that on windows it is not the device display name but the true interface name i e Device NPF_ param remote_port The remote port the rpcapd service is listening on param bpf_filter A BPF tcpdump filter to apply on the cap before reading param only_summaries Only produce packet summaries much faster but includes very little information param decryption_key Key used to encrypt and decrypt captured traffic param encryption_type Standard of encryption used in captured traffic must be either WEP WPA PWD or WPA PWK Defaults to WPA PWK param decode_as A dictionary of decode_criterion_string decode_as_protocol that are used to tell tshark to decode protocols in situations it wouldn t usually for instance tcp port 8888 http would make it attempt to decode any port 8888 traffic as HTTP See tshark documentation for details param tshark_path Path of the tshark binary param override_prefs A dictionary of tshark preferences to override PREFERENCE_NAME PREFERENCE_VALUE param disable_protocol Tells tshark to remove a dissector for a specifc protocol interface rpcap s d s remote_host remote_port remote_interface super RemoteCapture self __init__ interface bpf_filter bpf_filter only_summaries only_summaries decryption_key decryption_key encryption_type encryption_type tshark_path tshark_path decode_as decode_as disable_protocol disable_protocol override_prefs override_prefs\\n',\n",
       " 'def check_output popenargs kwargs if stdout in kwargs raise ValueError stdout argument not allowed it will be overridden process subprocess Popen popenargs stdout subprocess PIPE kwargs output unused_err process communicate retcode process poll if retcode cmd kwargs get args if cmd is None cmd popenargs 0 raise RuntimeError Program failed to run Retcode d Cmd s retcode cmd return output\\n',\n",
       " 'def get_process_path tshark_path None process_name tshark config get_config possible_paths config get tshark tshark_path if tshark_path is not None possible_paths insert 0 tshark_path if sys platform startswith win for env in ProgramFiles x86 ProgramFiles program_files os getenv env if program_files is not None possible_paths append os path join program_files Wireshark s exe process_name else os_path os getenv PATH usr bin usr sbin usr lib tshark usr local bin for path in os_path split possible_paths append os path join path process_name for path in possible_paths if os path exists path return path raise TSharkNotFoundException TShark not found Try adding its location to the configuration file Search these paths format possible_paths\\n',\n",
       " 'def get_tshark_display_filter_flag tshark_path None tshark_version get_tshark_version tshark_path if LooseVersion tshark_version LooseVersion 1 10 0 return Y else return R\\n',\n",
       " 'def get_tshark_interfaces tshark_path None parameters get_process_path tshark_path D with open os devnull w as null tshark_interfaces check_output parameters stderr null decode ascii return line split 0 for line in tshark_interfaces splitlines\\n',\n",
       " 'pytest fixture def lazy_simple_capture request return make_test_capture request\\n',\n",
       " 'pytest fixture def simple_capture lazy_simple_capture lazy_simple_capture load_packets return lazy_simple_capture\\n',\n",
       " 'pytest fixture def simple_summary_capture request return make_test_capture request only_summaries True\\n',\n",
       " 'def __init__ self layers None frame_info None number None length None captured_length None sniff_time None interface_captured None if layers is None self layers else self layers layers self frame_info frame_info self number number self interface_captured interface_captured self captured_length captured_length self length length self sniff_timestamp sniff_time\\n',\n",
       " 'def __getitem__ self item if isinstance item int return self layers item for layer in self layers if layer layer_name item lower return layer raise KeyError Layer does not exist in packet\\n',\n",
       " 'def __contains__ self item try self item return True except KeyError return False\\n',\n",
       " 'property def _packet_string self return Packet Length s s self length os linesep\\n',\n",
       " 'def __getattr__ self item for layer in self layers if layer layer_name item return layer raise AttributeError No attribute named s item\\n',\n",
       " 'def get_multiple_layers self layer_name return layer for layer in self layers if layer layer_name lower layer_name lower\\n',\n",
       " 'def __init__ self ring_file_size 1024 num_ring_files 1 ring_file_name tmp pyshark pcap interface None bpf_filter None display_filter None only_summaries False decryption_key None encryption_type wpa pwk decode_as None disable_protocol None tshark_path None override_prefs None include_raw False super LiveRingCapture self __init__ interface bpf_filter bpf_filter display_filter display_filter only_summaries only_summaries decryption_key decryption_key encryption_type encryption_type tshark_path tshark_path decode_as decode_as disable_protocol disable_protocol override_prefs override_prefs include_raw include_raw self ring_file_size ring_file_size self num_ring_files num_ring_files self ring_file_name ring_file_name\\n',\n",
       " 'def get_parameters self packet_count None params super LiveRingCapture self get_parameters packet_count packet_count params b filesize str self ring_file_size b files str self num_ring_files w self ring_file_name P return params\\n',\n",
       " 'def get self item default None try return getattr self item except AttributeError return default\\n',\n",
       " 'def get_field self name field self _all_fields get name if field is not None return field for field_name field in self _all_fields items if self _sanitize_field_name name self _sanitize_field_name field_name return field\\n',\n",
       " 'def _get_all_field_lines self for field in self _get_all_fields_with_alternates for line in self _get_field_or_layer_repr field yield line\\n',\n",
       " 'def __init__ self layer_name layer_dict full_name None is_intermediate False self _layer_name layer_name if not full_name self _full_name self _layer_name else self _full_name full_name self _is_intermediate is_intermediate self _wrapped_fields if not isinstance layer_dict dict self value layer_dict self _all_fields return self _all_fields layer_dict\\n',\n",
       " 'def get_field self name field self _wrapped_fields get name if field is None is_fake False field self _get_internal_field_by_name name if field is None is_fake self _is_fake_field name if not is_fake raise AttributeError No such field s name field self _make_wrapped_field name field is_fake is_fake self _wrapped_fields name field return field\\n',\n",
       " 'def _get_internal_field_by_name self name field self _all_fields get name self _all_fields get s s self _full_name name if field is not None return field for field_name in self _all_fields if field_name endswith s name return self _all_fields field_name\\n',\n",
       " 'def _make_wrapped_field self name field is_fake False full_name None if not full_name full_name s s self _full_name name if is_fake field key value for key value in self _all_fields items if key startswith full_name if isinstance field dict if name endswith _tree name name replace _tree full_name s s self _full_name name return JsonLayer name field full_name full_name is_intermediate is_fake elif isinstance field list return self _make_wrapped_field name field_part full_name self _full_name split 0 for field_part in field return LayerFieldsContainer LayerField name name value field\\n',\n",
       " 'def has_field self dotted_name parts dotted_name split cur_layer self for part in parts if part in cur_layer field_names cur_layer cur_layer get_field part else return False return True\\n',\n",
       " 'property def showname_value self if self showname and in self showname return self showname split 1 1\\n',\n",
       " 'property def binary_value self return binascii unhexlify self raw_value\\n',\n",
       " 'property def int_value self return int self raw_value\\n',\n",
       " 'property def hex_value self return int self raw_value 16\\n',\n",
       " 'def __init__ self pipe display_filter None only_summaries False decryption_key None encryption_type wpa pwk decode_as None disable_protocol None tshark_path None override_prefs None use_json False include_raw False super PipeCapture self __init__ display_filter display_filter only_summaries only_summaries decryption_key decryption_key encryption_type encryption_type decode_as decode_as disable_protocol disable_protocol tshark_path tshark_path override_prefs override_prefs use_json use_json include_raw include_raw self _pipe pipe\\n',\n",
       " 'def get_parameters self packet_count None params super PipeCapture self get_parameters packet_count packet_count params r return params\\n',\n",
       " 'def __init__ self input_file None keep_packets True display_filter None only_summaries False decryption_key None encryption_type wpa pwk decode_as None disable_protocol None tshark_path None override_prefs None use_json False output_file None include_raw False super FileCapture self __init__ display_filter display_filter only_summaries only_summaries decryption_key decryption_key encryption_type encryption_type decode_as decode_as disable_protocol disable_protocol tshark_path tshark_path override_prefs override_prefs use_json use_json output_file output_file include_raw include_raw self input_filename input_file if not isinstance input_file basestring self input_filename input_file name if not os path exists self input_filename raise FileNotFoundError str self input_filename self keep_packets keep_packets self _packet_generator self _packets_from_tshark_sync\\n',\n",
       " 'def next self if not self keep_packets return self _packet_generator send None elif self _current_packet len self _packets packet self _packet_generator send None self _packets packet return super FileCapture self next_packet\\n',\n",
       " 'def __init__ self interface None bpf_filter None display_filter None only_summaries False decryption_key None encryption_type wpa pwk output_file None decode_as None disable_protocol None tshark_path None override_prefs None capture_filter None monitor_mode None use_json False include_raw False super LiveCapture self __init__ display_filter display_filter only_summaries only_summaries decryption_key decryption_key encryption_type encryption_type output_file output_file decode_as decode_as disable_protocol disable_protocol tshark_path tshark_path override_prefs override_prefs capture_filter capture_filter use_json use_json include_raw include_raw self bpf_filter bpf_filter self monitor_mode monitor_mode if sys platform win32 and monitor_mode raise WindowsError Monitor mode is not supported by the Windows platform if interface is None self interfaces get_tshark_interfaces tshark_path elif isinstance interface basestring self interfaces interface else self interfaces interface\\n',\n",
       " 'def get_parameters self packet_count None params super LiveCapture self get_parameters packet_count packet_count params r return params\\n',\n",
       " 'def packet_from_xml_packet xml_pkt psml_structure None if not isinstance xml_pkt lxml objectify ObjectifiedElement parser lxml objectify makeparser huge_tree True xml_pkt lxml objectify fromstring xml_pkt parser if psml_structure return _packet_from_psml_packet xml_pkt psml_structure return _packet_from_pdml_packet xml_pkt\\n',\n",
       " 'def __init__ self bpf_filter None display_filter None only_summaries False decryption_key None encryption_type wpa pwk decode_as None disable_protocol None tshark_path None override_prefs None use_json False linktype LinkTypes ETHERNET include_raw False super InMemCapture self __init__ display_filter display_filter only_summaries only_summaries decryption_key decryption_key encryption_type encryption_type decode_as decode_as disable_protocol disable_protocol tshark_path tshark_path override_prefs override_prefs use_json use_json include_raw include_raw self bpf_filter bpf_filter self _packets_to_write None self _current_linktype linktype self _current_tshark None\\n',\n",
       " 'def get_parameters self packet_count None params super InMemCapture self get_parameters packet_count packet_count params i return params\\n',\n",
       " 'def parse_packet self binary_packet return self parse_packets binary_packet 0\\n',\n",
       " 'def parse_packets self binary_packets if not binary_packets raise ValueError Must supply at least one packet parsed_packets if not self _current_tshark self eventloop run_until_complete self _get_tshark_process for binary_packet in binary_packets self _write_packet binary_packet def callback pkt parsed_packets append pkt if len parsed_packets len binary_packets raise StopCapture self eventloop run_until_complete self _get_parsed_packet_from_tshark callback return parsed_packets\\n',\n",
       " 'def feed_packet self binary_packet linktype LinkTypes ETHERNET warnings warn Deprecated method Use InMemCapture parse_packet instead self _current_linktype linktype pkt self parse_packet binary_packet self close self _packets append pkt return pkt\\n',\n",
       " 'def feed_packets self binary_packets linktype LinkTypes ETHERNET self _current_linktype linktype parsed_packets self parse_packets binary_packets self _packets extend parsed_packets self close return parsed_packets\\n',\n",
       " 'def download_image url out_folder video_name click echo nDownloading image for video_name if not isdir join out_folder images makedirs join out_folder images try url_part url split 1 _ url_last_extension splitext url_part out_file join out_folder images video_name url_last_extension urlretrieve url out_file click echo Successfully downloaded video_name thumbnail return video_name url_last_extension except click echo nUnable to download thumbnail for video_name return False\\n',\n",
       " 'def copy_with_transformations itemsrc itemdst transformations metadata video_src valid_transformations t for t in transformations if t applies itemsrc if valid_transformations inpath itemsrc for transformation in valid_transformations 1 temp tempfile NamedTemporaryFile transformation apply inpath temp name itemdst video_src inpath temp name final_transformation valid_transformations 1 final_transformation apply inpath itemdst itemdst metadata video_src else shutil copy2 itemsrc itemdst\\n',\n",
       " 'def copy_files paths section transformations metadata None video_src None src_dir dst_dir paths num_files count_files src_dir print progress ProgressBar Copying files from section num_copied 0 if isfile src_dir and not src_dir endswith yaml copy_with_transformations src_dir dst_dir transformations metadata video_src else for srcpath _ filenames in walk src_dir if srcpath endswith metadata continue relpath srcpath len src_dir 1 dstpath join dst_dir relpath if not exists dstpath makedirs dstpath for name in filenames if name endswith yaml or name startswith continue itemsrc join srcpath name itemdst join dstpath name copy_with_transformations itemsrc itemdst transformations metadata video_src num_copied 1 progress calculate_update num_copied num_files\\n',\n",
       " 'def count_files directory files if isdir directory for path _ filenames in walk directory if path endswith metadata continue for name in filenames if name endswith yaml or name startswith continue files append name return len files\\n',\n",
       " 'def readconfig important_keys retprop False if not isfile config yaml click echo There is no configuration file logging debug There is no configuration file sys exit 1 with open config yaml as data_file conf_data yaml load data_file return verify_and_extract_main_config conf_data important_keys retprop\\n',\n",
       " 'def verify_and_extract_main_config conf_data important_keys retprop if retprop properties for key in important_keys key_path key split key_dict conf_data for path in key_path try key_dict key_dict path except click echo The field key is required Please make sure it exists in the main config yaml file sys exit 1 if retprop properties append key_dict if retprop if len properties 1 return properties 0 else return properties else return conf_data\\n',\n",
       " 'def verify_section_config src_dir sections video_src error_list for section in sections if not section startswith video_src if not isfile join src_dir section section_config yaml error_list append section if error_list click echo click style These section s don t have a config yaml create one fg red click echo click style join error_list fg yellow bold True logging debug These section s don t have a config yaml create one join error_list sys exit 1 if not exists join src_dir video_src click echo click style nError video_src specified doesn t exist n fg red logging error Video Source Directory video_src specified doesn t exist sys exit 1 click echo click style Finished sanity check Continue fg green\\n',\n",
       " 'def __init__ self kwargs self link_color kwargs color self online_link kwargs link self tracking_code kwargs code self header_html str self _create_header flags re DOTALL re IGNORECASE self link_re re compile a href https a s flags flags self link_target_re re compile target s flags flags self link_style_re re compile style s flags flags\\n',\n",
       " 'def _create_tracking_tag self dst if not self tracking_code return None img_url_attr v 1 t pageview ec page ea open cm site img_url_attr cs offlinedevsite img_url_attr cn OfflineDevContent img_url_attr tid self tracking_code img_url_attr dp dst img_url_attr cid str int uuid uuid1 int 96 str int uuid uuid1 int 96 img_src http www google analytics com collect urlencode img_url_attr tracking_img_tag img src img_src return tracking_img_tag\\n',\n",
       " 'def _create_header self info_header open templates html_header html r read soup BeautifulSoup info_header if self online_link template_link soup find a class abcde template_link href self online_link return soup\\n',\n",
       " 'def applies self src return src endswith html\\n',\n",
       " 'def apply self src dst finaldst metadata video video_data finaldst metadata video html open src r read html self transform html dst or html with open dst w as out_file out_file write html\\n',\n",
       " 'def transform self html dst l_color self link_color or green def transform_link value Transforms link Args value value Returns Transformed link attr_append link attr value group 0 value group 1 attr_new attr attr_new count re subn self link_target_re target _blank attr_new if count 0 attr_append target _blank attr_new count re subn self link_style_re style 1 color s l_color attr_new if count 0 attr_append style color s l_color return link replace attr attr_new attr_append html re sub self link_re transform_link html flags re DOTALL re IGNORECASE html re sub body 1 s self header_html html flags flags tracking_tag str self _create_tracking_tag dst html re sub body s 1 tracking_tag html flags flags return html\\n',\n",
       " 'def __init__ self message width 30 progress_symbol empty_symbol self width width if self width 0 self width 0 self message message self progress_symbol progress_symbol self empty_symbol empty_symbol\\n',\n",
       " 'def update self progress total_blocks self width filled_blocks int round progress 100 float total_blocks empty_blocks total_blocks filled_blocks progress_bar self progress_symbol filled_blocks self empty_symbol empty_blocks if not self message self message progress_message r 0 1 2 format self message progress_bar progress sys stdout write progress_message sys stdout flush\\n',\n",
       " 'def calculate_update self done total progress int round done float total 100 self update progress\\n',\n",
       " 'def analyze_content size with open config yaml as data_file conf_data yaml load data_file src_dir conf_data source main_path total_size get_size src_dir 1000000 0 if not size single_layered_disc int math ceil total_size 4700 dual_layered_disc int math ceil total_size 8500 flash int math ceil total_size 16000 click echo The total size of content is 0 MB format total_size click echo You need 0 single layered DVD disc s or 1 dual layered DVD disc s to copy content format single_layered_disc dual_layered_disc click echo OR You need 0 16GB flash drive s to copy content format flash else device_number int math ceil total_size int size click echo The total size of content is 0 MB format total_size click echo You need 0 storage device of this size to copy content format device_number\\n',\n",
       " 'def get_size start_path total_size 0 for dirpath _ filenames in walk start_path for filename in filenames filename_path join dirpath filename total_size getsize filename_path return total_size\\n',\n",
       " 'def test_get_sections_without_ignore_path self result get_sections self dirpath expected docs sdk tutorial videos self assertEqual result expected\\n',\n",
       " 'def test_get_sections_with_ignore_path self videos_src join self dirpath videos result get_sections self dirpath videos_src expect docs sdk tutorial self assertEqual result expect\\n',\n",
       " 'def test_get_sections_with_ignore_two_paths self videos_src join self dirpath videos sdk_src join self dirpath sdk result get_sections self dirpath videos_src sdk_src expect docs tutorial self assertEqual result expect\\n',\n",
       " 'def test_verify_section_config_case1 self with self assertRaises SystemExit as cm verify_section_config self dirpath self sections self video_src self assertEqual cm exception code 1\\n',\n",
       " 'def test_verify_section_config_case2 self open join self dirpath docs section_config yaml w open join self dirpath sdk section_config yaml w with self assertRaises SystemExit as cm verify_section_config self dirpath self sections self video_src self assertEqual cm exception code 1\\n',\n",
       " 'def test_verify_section_config_case3 self open join self dirpath docs section_config yaml w open join self dirpath sdk section_config yaml w open join self dirpath tutorial section_config yaml w try verify_section_config self dirpath self sections self video_src except self assertTrue False\\n',\n",
       " 'def test_without_youtube_link self value generate_one_metadata self assertEqual value False\\n',\n",
       " 'def test_invalid_youtube_url self url https www youtube com watch v Ipl rLRxOrs index 56 list PLsko1y3G value generate_one_metadata url self assertEqual value None\\n',\n",
       " 'def test_transform_case1 self html_in html div div html html_out html div div html result self t transform html_in self filename self assertEquals result html_out\\n',\n",
       " 'def test_transform_case2 self html_in html a href http a html html_out html a href http target _blank style color green a html result self t transform html_in self filename self assertEquals result html_out\\n',\n",
       " 'def test_transform_case3 self html_in html a href homepage a html html_out html a href homepage a html result self t transform html_in self filename self assertEquals result html_out\\n',\n",
       " 'def test_transform_case4 self html_in html a href http a a href homepage a html html_out html a href http target _blank style color green a a href homepage a html result self t transform html_in self filename self assertEquals result html_out\\n',\n",
       " 'def test_transform_case5 self html_in html body body html result self t transform html_in self filename self assertIn img result\\n',\n",
       " 'def test_transform_case6 self html_in html body a href http a body html result self t transform html_in self filename self assertIn img result self assertIn a href http target _blank style color green result\\n',\n",
       " 'def check_platform if platform startswith win32 or platform startswith cygwin if version_info 0 3 and version_info 1 1 click echo Python version not supported Install python 3 x x and try again\\n',\n",
       " 'click group def cli pass\\n',\n",
       " 'click command help Process files to an archive click option size s help Maximum size in MB click option formt f help Converts to zip iso zipiso format click option v verbose is_flag True help Enables verbose mode def bundle verbose size None formt None if verbose console logging StreamHandler formatter logging Formatter name 4s levelname 8s message s console setFormatter formatter logging getLogger addHandler console check_platform compile_sections if formt and formt lower iso makeiso size elif formt and formt lower zip makezip size elif formt and formt lower zipiso makezip size makeiso size\\n',\n",
       " 'click command help Bundle only video file s click option size s help Maximum size in MB click option formt f help Converts to iso format click option v verbose is_flag True help Enables verbose mode def bundle_videos verbose size None formt None if verbose console logging StreamHandler formatter logging Formatter name 4s levelname 8s message s console setFormatter formatter logging getLogger addHandler console check_platform compile_videos if formt and formt lower iso makeiso size\\n',\n",
       " 'click command help Generate title author and thumbnail for YouTube videos click option typ t help Specify type single for a single file auto to get URLs from videos_url yaml def generate typ if typ single url click prompt Enter the Youtube video link w url elif typ auto generate_video_metadata else click echo Error wrong type specified logging error Wrong type specified for generate command return False\\n',\n",
       " 'click command help Shows the application logs def log for line in open nkata log readlines click echo line rstrip\\n',\n",
       " 'click command help Package output files into a zip or ISO file click option size s help Maximum size of storage medium in MB click option formt f help Valid options are zip iso or zipiso default zipiso def convert formt size None if formt lower iso makeiso size elif formt lower zip makezip size else makezip size makeiso size\\n',\n",
       " 'click command help Calculate the number of discs needed to copy content memory needed click option size s help Maximum size in MB def analyze size None analyze_content size\\n',\n",
       " 'def generate_one_metadata link if link template metadata_content_generator link if template dst click prompt Where would you like to save the data generated default getcwd if dst if not exists dst makedirs dst write_file open join dst metadata yaml w write_file write template return True else click echo Error Can t get metadata for this video make sure the urlis correct logging error Error Can t get metadata for this video make sure the url is correct else click echo Error No URL specified Try Again logging error No URL specified Try Again return False\\n',\n",
       " 'def generate_video_metadata try with open config yaml as data_file conf_data yaml load data_file except click echo click style Oops No root config yaml file Check config yaml example for an example and tryagain fg red logging debug Oops There is no configuration file return False src_dir conf_data source main_path video_src conf_data source video_source or videos for section _ _ in walk join src_dir video_src src_path join section videos_url yaml try with open src_path as data_file conf_data yaml load data_file metadata_path join section metadata for url_obj in conf_data urls video_name url_obj video_name click echo Generating metadata for video_name video file url url_obj url template metadata_content_generator url if template if not isdir metadata_path makedirs metadata_path write_file open join metadata_path video_name _metadata yaml w write_file write template else click echo Error Can t get metadata for video_name logging error Error Can t get metadata for video_name except continue click echo Done\\n',\n",
       " 'def metadata_content_generator url link http www youtube com oembed url url format json res requests get link if res status_code is not 200 return False metadata res json title metadata title author_name metadata author_name author_url metadata author_url thumbnail_url metadata thumbnail_url sub_title author_name description Watch the Official on author_url Author Name author_name title_string n title title sub_title_string n sub_title sub_title description_string n description description thumbnail_string n thumbnail_url thumbnail_url return title_string sub_title_string description_string thumbnail_string\\n',\n",
       " 'def to_iso source destination filelist None if isfile destination unlink destination if filelist tmpdir mkdtemp for item in filelist rel item len source 1 dst join tmpdir rel if not isdir dirname dst makedirs dirname dst if not isdir item link item dst source tmpdir if platform startswith darwin system hdiutil makehybrid iso joliet o s s destination source click echo Finished elif platform startswith linux system mkisofs r J o s s destination source click echo Finished else click echo platform not supported for converting to ISO files Try to download ISO maker tool from http www magiciso com tutorials miso iso creator htm logging debug platform not supported for converting to ISO files Try to download ISO maker tool from http www magiciso com tutorials miso iso creator htm\\n',\n",
       " 'def compile_sections try click echo click style nReading and verifying configuration file fg green logging info Reading and verifying configuration file important_keys division project_title project_subtitle source main_path source video_source destination main_path absolute_link_color tracking_code conf_data readconfig important_keys except click echo Error in main config file That s in config yaml in the root directory of nkata Fix then try again Pull from the sample source directory if necessary return title conf_data project_title sub_title conf_data project_subtitle src_dir conf_data source main_path video_src conf_data source video_source dst conf_data destination main_path link_color conf_data absolute_link_color tracking_code conf_data tracking_code folder_name conf_data output_folder_name if not folder_name folder_name goc config title title sub_title sub_title tracking_code tracking_code link_color link_color if conf_data division division_process append True division_values list conf_data division values division_list item for sublist in division_values for item in sublist division_main_list extend list conf_data division keys verify_section_config src_dir list set division_list video_src for div in conf_data division dst_dir join dst folder_name div if not isdir dst_dir makedirs dst_dir if not exists src_dir message nError Source Directory src_dir specified doesn t exist n click echo click style message fg red logging error Source Directory src_dir specified doesn t exist return False sections get_divisions conf_data division div video_src kwargs page_index div None sections 1 process_sections src_dir dst_dir config sections 0 kwargs page_index list else division_process append False dst_dir join dst folder_name if not isdir dst_dir makedirs dst_dir if not exists src_dir message nError Source Directory src_dir specified doesn t exist n click echo click style message fg red logging debug Source Directory src_dir specified doesn t exist return False sections get_sections src_dir join src_dir video_src path_to folder_name verify_section_config src_dir sections video_src kwargs main_page_index path_to single None process_sections src_dir dst_dir config sections kwargs click echo click style Finished fg green logging info Finished\\n',\n",
       " 'def process_sections src_dir dst_dir config sections kwargs page_index path_to typ division kwargs for section in sections src_path join src_dir section dst_path join dst_dir section config_file_path join src_path section_config yaml if not isfile src_path try with open config_file_path as data_file config_data yaml load data_file title config_data title online_link config_data online_link or metadata config_data metadata page_index append section path_to title metadata except message Error in section config file Check sample and try again click echo click style message fg red logging debug Oops There is no configuration file for section sys exit else online_link None bundle_content_section src_path dst_path section config online_link if division compile_videos division dst_dir path_to elif division is None compile_videos if not typ generate_template dst_dir config title config sub_title config tracking_code page_index None True\\n',\n",
       " 'def process_video_sections sections folder_name transformations video_transformation kwargs src_dir dst_dir video_src title sub_title tracking_code dst path_to kwargs meta list division_meta list for section in sections src_path join src_dir video_src section dst_path join dst_dir section if not isfile join src_dir video_src section try with open join src_dir video_src section section_config yaml as video_file config_data yaml load video_file video_subtitle config_data video_subtitle video_summary config_data video_summary metadata config_data metadata template_path config_data template_path except click echo section video configuration file doesn t exist or has an error Using default values logging debug section video configuration file doesn t exist or has an error Using default values video_summary section video_subtitle template_path None metadata None if template_path and not isfile template_path click echo Template specified for section video s doesn t exist Using default template logging debug Template specified for section video s doesn t exist Using default template template_path None con filename section index html title section meta append con division_meta append con page_index append path_to section videos division_meta division_meta paths src_path dst_path videos_path join src_dir folder_name video_src bundle_video_section paths section metadata transformations videos_path video_transformation generate_video_list_html dst_path video_subtitle video_summary path_to template_path if division_process and division_process 0 dst_folder join dst folder_name generate_template dst_folder title tracking_code division_main_list True else main_page_index append video_src folder_name video_src meta generate_template join dst folder_name title sub_title tracking_code main_page_index\\n',\n",
       " 'def compile_videos division None div_dir None path_to None click echo Processing videos try with open config yaml as data_file conf_data yaml load data_file except message Oops There is no configuration file Check sample and try again click echo click style message fg red logging debug Oops There is no configuration file return False src_dir conf_data source main_path video_src conf_data source video_source or videos dst conf_data destination main_path link_color conf_data absolute_link_color tracking_code conf_data tracking_code title conf_data project_title sub_title conf_data project_subtitle folder_name conf_data output_folder_name if not folder_name folder_name goc base_path basename normpath video_src dst_dir join dst folder_name base_path if not exists join src_dir video_src message nError Video Source Directory video_src specified doesn t exist n click echo click style message fg red logging error Video Source Directory video_src specified doesn t exist return False video_transformation VideoTransformation tracking_code JINJA_ENVIRONMENT transformations list transformations append HtmlTransformation color link_color code tracking_code link False transformations append video_transformation sections get_sections join src_dir video_src join src_dir video_src metadata if not division kwargs src_dir dst_dir video_src title sub_title tracking_code dst path_to process_video_sections sections folder_name transformations video_transformation kwargs else kwargs src_dir div_dir video_src title sub_title tracking_code dst path_to process_video_sections division folder_name transformations video_transformation kwargs if not isdir join dst folder_name img makedirs join dst folder_name img shutil copy2 img video_image svg join dst folder_name img if not isdir join dst folder_name img makedirs join dst folder_name img shutil copy2 img back arrow svg join dst folder_name img\\n',\n",
       " 'def generate_template dst_dir title sub_title tracking_code page_index division None back None write_file open join dst_dir index html w created_at datetime now strftime d m Y H M S template_values project_title title project_subtitle sub_title created_at created_at tracking_code tracking_code sections page_index back back if not division template JINJA_ENVIRONMENT get_template templates homepage html else template JINJA_ENVIRONMENT get_template templates division_homepage html write_file write template render template_values\\n',\n",
       " 'def makeiso size None try click echo nReading and verifying configuration file result readconfig division destination main_path output_folder_name True division result 0 dst result 1 folder_name result 2 except click echo Unable to read information from config yaml Fix it or check it out from github then try again return if not division iso_maker join dst folder_name dst folder_name size else if not isdir join dst iso copytree join dst folder_name img join dst iso img symlinks True for div in division dst_dir join dst goc div iso_maker dst_dir join dst iso div size copy2 join dst folder_name index html join dst iso\\n',\n",
       " 'def iso_maker dst_dir dst div size if not isdir dst_dir click echo Error No output for div s Have you run the bundle command div return if size max_size int size parts split dst_dir max_size for i part in enumerate parts click echo Packaging content in a ISO format d d i 1 len parts to_iso dst_dir join dst s d iso div i 1 filelist part else click echo Packaging content in a ISO format to_iso dst_dir join dst s iso div\\n',\n",
       " 'def zip_maker dst_dir dst div size if not isdir dst_dir click echo Error No output for div s Have you run the bundle command div return if size max_size int size parts split dst_dir max_size for i part in enumerate parts click echo Packaging content in a zip file d d i 1 len parts to_zip dst_dir join dst s d zip div i 1 False filelist part else click echo Packaging content in a zip file to_zip dst_dir join dst s zip div False\\n',\n",
       " 'def makezip size None try click echo nReading and verifying configuration file result readconfig division destination main_path output_folder_name True division result 0 dst result 1 folder_name result 2 except click echo Unable to read information from config yaml Fix then try again return if not division zip_maker join dst folder_name dst folder_name size else if not isdir join dst zip copytree join dst folder_name img join dst zip img symlinks True for div in division dst_dir join dst goc div zip_maker dst_dir join dst zip div size copy2 join dst folder_name index html join dst zip\\n',\n",
       " 'def split dst_dir max_size parts current_part current_free max_size for srcpath dirnames filenames in walk dst_dir for name in filenames src join srcpath name info stat src size 1 0 info st_size 1024 1024 if current_free size parts append current_part current_part current_free max_size current_part append src current_free size if not dirnames and not filenames current_part append srcpath parts append current_part return parts\\n',\n",
       " 'def __init__ self tracking_code jinjaenv self tracking_code tracking_code self jinjaenv jinjaenv self list_of_videos list self new_list list\\n',\n",
       " 'def applies self src _ extension splitext src return extension in self EXTENSIONS\\n',\n",
       " 'def apply self itemsrc itemdst finaldst metadata videos_src finaldst_dir finaldst_name dirname finaldst basename finaldst video_name extension splitext finaldst_name video_type video s extension 1 html_name s_ s html video_name extension finaldst_base _ split finaldst _ finaldst_base_path split finaldst_base itemsrc_base _ split itemsrc if len self splitpath finaldst len self splitpath videos_src 2 video_source_path join finaldst_base_path html_files html_name back index html data_file join itemsrc_base metadata video_name _metadata yaml try with open data_file as data_file conf_data yaml load data_file title conf_data title sub_title conf_data sub_title description conf_data description thumbnail_url conf_data thumbnail_url if thumbnail_url image download_image thumbnail_url finaldst_base video_name if image image_path join finaldst_base_path images image else image_path else image_path except title sub_title description image_path video_name else video_source_path join html_files html_name back index html if metadata and isfile metadata video_name title sub_title description thumbnail_url self process_meta_data video_name metadata if thumbnail_url image download_image thumbnail_url finaldst_base video_name if image image_path join images image else image_path else image_path else title sub_title description image_path video_name video_info title sub_title description image_path video_source join video_name extension if len title 50 title title 0 45 if finaldst_base_path in self new_list self list_of_videos append video_name video_source_path title sub_title image_path None else self new_list append finaldst_base_path self list_of_videos append finaldst_base_path kwargs video_name video_source_path title sub_title image_path None self list_of_videos append kwargs video_detail video_name video_source video_type video_info self generate_html finaldst_dir html_name video_detail back shutil copy2 itemsrc itemdst\\n',\n",
       " 'def process_meta_data self video_name metadata if metadata video_name data_file metadata video_name try with open data_file as data_file conf_data yaml load data_file except click echo nOops There is no metadata for video_name Try again logging debug nOops There is no metadata for video_name return False title conf_data title sub_title conf_data sub_title description conf_data description thumbnail_url conf_data thumbnail_url return title sub_title description thumbnail_url\\n',\n",
       " 'def generate_html self dst_path html_name video_detail back video_name video_source video_type video_info video_detail if not exists join dst_path html_files makedirs join dst_path html_files write_file open join dst_path html_files html_name w template_values video_name video_name video_type video_type video_source video_source tracking_code self tracking_code title video_info 0 sub_title video_info 1 description video_info 2 back back template self jinjaenv get_template templates video html write_file write template render template_values\\n',\n",
       " 'def generate_video_list_html self dst_dir video_subtitle video_summary path_to template_pth None if not self list_of_videos return write_file open join dst_dir index html w template_values video_summary video_summary video_subtitle video_subtitle tracking_code self tracking_code list self list_of_videos division_back path_to if template_pth template self jinjaenv get_template template_pth write_file write template render template_values else template self jinjaenv get_template templates videos_list html write_file write template render template_values self list_of_videos\\n',\n",
       " 'def splitpath self path maxdepth 20 head tail split path return self splitpath head maxdepth 1 tail if maxdepth and head and head path else head or tail\\n',\n",
       " 'def to_zip dir_path None zip_file_path None include_dir_in_path True filelist None if not zip_file_path zip_file_path dir_path zip if not isdir dir_path logging error dirPath argument must point to a directory dir_path does not raise OSError dirPath argument must point to a directory s does not dir_path parent_dir dir_to_zip split dir_path def trim_path path coverts to Zip format Args path path to be trimmed Returns Path to the zip archive archive_path path replace parent_dir 1 if parent_dir archive_path archive_path replace sep 1 if not include_dir_in_path archive_path archive_path replace dir_to_zip sep 1 return normcase archive_path if not filelist filelist for archive_dir_path dir_names file_names in walk dir_path for file_name in file_names filelist append join archive_dir_path file_name if not file_names and not dir_names filelist append archive_dir_path out_fil zipfile ZipFile zip_file_path w compression zipfile ZIP_DEFLATED allowZip64 True for file_path in filelist if isdir file_path zip_info zipfile ZipInfo trim_path file_path out_fil writestr zip_info else out_fil write file_path trim_path file_path out_fil close click echo Finished\\n',\n",
       " 'def bundle_content_section src_path dst_path section config online_link logging info Start bundling files from section transformations if online_link and not http in online_link online_link http online_link link_color config link_color tracking_code config tracking_code html_transform HtmlTransformation color link_color code tracking_code link online_link transformations append html_transform paths src_path dst_path copy_files paths section transformations click echo n logging info Finish bundling files from section\\n',\n",
       " 'def bundle_video_section paths vid metadata transformations videos_src logging info Start bundling videos from vid copy_files paths vid transformations metadata videos_src click echo n logging info Finish bundling videos from vid\\n',\n",
       " 'def get_divisions division ignored_sections def is_content_division item Removes ignored_section Args item section to be checked Returns False if item is to be ignored else returns true if item in ignored_sections return False item_list item split if item_list 0 in ignored_sections return False return True return item for item in division if is_content_division item item replace ignored_sections 0 for item in division if not is_content_division item\\n',\n",
       " 'def get_sections src_dir ignored_paths def is_content_section item Removes ignored_paths and section starting with Args item Section to be checked Returns False if item is to be ignored returns True otherwise if item startswith return False path join src_dir item if path in ignored_paths return False return True return item for item in listdir src_dir if is_content_section item\\n',\n",
       " 'cached_property def messenger self if hasattr self slackuser return self slackuser messenger return None\\n',\n",
       " 'def parse self query session_id pass\\n',\n",
       " 'def __init__ self action self action action\\n',\n",
       " 'def send self text pass\\n',\n",
       " 'def send_text self text quick_replies None pass\\n',\n",
       " 'def send_image self image_url quick_replies None pass\\n',\n",
       " 'def get_latest self pass\\n',\n",
       " 'app route def hello return Hello\\n',\n",
       " 'def leaveOneOut_error Y D P Tf Yt Gd Yg features all gwr_gamma None taxi_norm bydestination errors for k in range len Y with tf Graph as_default X_train X_test Y_train Y_test build_features Y D P Tf Yt Gd Yg k features taxi_norm F1 X_train shape 1 x1 tf placeholder tf float32 None F1 name numeric_features_set1 y tf placeholder tf float32 None 1 name label W tf Variable tf random_normal F1 name weight b tf Variable tf random_normal 1 name bias y_est tf add tf reduce_sum tf multiply x1 W b objective tf reduce_mean tf squared_difference y y_est train_step tf train GradientDescentOptimizer 0 1 minimize objective tf_mae tf reduce_mean tf abs y y_est sess tf InteractiveSession sess run tf global_variables_initializer train_step run feed_dict x1 X_train y Y_train None yarray np array Y_test reshape 1 1 mae tf_mae eval feed_dict x1 X_test None y yarray errors append mae return np mean errors np mean errors np mean Y\\n',\n",
       " 'def __init__ self shp rec None self bbox box shp bbox self polygon Polygon shp points self count total 0 self timeHist total np zeros 24 if rec None self CA rec 7\\n',\n",
       " 'def containCrime self cr if self bbox contains cr point if self polygon contains cr point return True return False\\n',\n",
       " 'def plotTimeHist self keys None if len self timeHist 1 return else if keys is None keys self timeHist keys values self timeHist key for key in keys plt figure for val in values plt plot val plt legend keys plt show\\n',\n",
       " 'def plot_corina_features feats generate_corina_features header feats 0 f feats 1 plt figure figsize 10 18 for idx in range len header y e idx for e in f plt subplot 4 2 idx 1 plt plot y plt axvline x 24 lw 3 color r ls plt axvline x 29 lw 3 color r ls plt axvline x 31 lw 3 ls color r plt title header idx plt show\\n',\n",
       " 'def shift self x_bits self absolute x_bits\\n',\n",
       " 'def print_info self print identifier self identifier print absolute self absolute print relative self relative\\n',\n",
       " 'def slice_block self bit_list for i in range 0 self msb self lsb 1 bit_list append Bit self identifier self lsb i i return bit_list\\n',\n",
       " 'def print_info self print identifier self identifier print self msb downto self lsb\\n',\n",
       " 'def add_bit self new_bit for current_bit in self bit_list if current_bit absolute new_bit absolute return False self bit_list append new_bit return True\\n',\n",
       " 'def print_info self for current_bit in self bit_list current_bit print_info\\n',\n",
       " 'def print_line self line_number print PP 02d line_number end for i in range 0 self exp_prime success 0 for current_bit in self bit_list if current_bit absolute i success 1 if success 1 print o end else print x end print\\n',\n",
       " 'def generate_test_set exp_prime ceil_byte_exp_prime exp_prime rem exp_prime 8 if rem 0 ceil_byte_exp_prime exp_prime 8 exp_prime 8 nmb_bytes int ceil_byte_exp_prime 8 file_ptr_a open vhdl tv_a txt w file_ptr_b open vhdl tv_b txt w file_ptr_c open vhdl tv_c txt w for x in range 2 10 a 2 exp_prime x a_hex hex a 2 zfill int ceil_byte_exp_prime 4 b 2 exp_prime x b_hex hex b 2 zfill int ceil_byte_exp_prime 4 file_ptr_a write str a_hex n file_ptr_b write str b_hex n c a b 2 exp_prime 1 c hex c 2 zfill int nmb_bytes 2 file_ptr_c write str c n for x in range 0 999 a random getrandbits exp_prime a_hex hex a 2 zfill int ceil_byte_exp_prime 4 b random getrandbits exp_prime b_hex hex b 2 zfill int ceil_byte_exp_prime 4 file_ptr_a write str a_hex n file_ptr_b write str b_hex n c a b 2 exp_prime 1 c hex c 2 zfill int nmb_bytes 2 file_ptr_c write str c n\\n',\n",
       " 'def main exp_prime int input Enter exponent p for Mersenne prime 2 p 1 p dsp_mul_width_a int input Enter m for m n bit DSP multiplier m dsp_mul_width_b int input Enter n for m n bit DSP multiplier n m n x y mod_mul_mersenne op_decomp exp_prime dsp_mul_width_a dsp_mul_width_b dp_list mod_mul_mersenne digit_product_generation m n x y bit_list mod_mul_mersenne bit_slice_generation dp_list exp_prime pp_list mod_mul_mersenne partial_product_generation bit_list exp_prime mod_mul_mersenne print_partial_products pp_list mod_mul_mersenne generate_vhdl pp_list exp_prime m n x y mod_mul_mersenne generate_vhdl_tb pp_list exp_prime mod_mul_mersenne generate_test_set exp_prime print VHDL code successfully generated\\n',\n",
       " 'def generate_vhdl pp_list exp_prime m n x y file_ptr open vhdl mod_mul vhd w write_lib file_ptr write_entity file_ptr exp_prime m n x y write_architecture file_ptr pp_list exp_prime\\n',\n",
       " 'def write_lib file_ptr file_ptr write library ieee n file_ptr write use ieee std_logic_1164 all n file_ptr write use ieee numeric_std all n n\\n',\n",
       " 'def write_entity file_ptr exp_prime m n x y file_ptr write entity mod_mul is n file_ptr write t generic n file_ptr write t t EXP_PRIME integer str exp_prime n file_ptr write t t M integer str m n file_ptr write t t N integer str n n file_ptr write t t Y integer str y n file_ptr write t t X integer str x n file_ptr write t n file_ptr write t port n file_ptr write t t clk in std_ulogic n file_ptr write t t a_i in std_ulogic_vector EXP_PRIME 1 downto 0 n file_ptr write t t b_i in std_ulogic_vector EXP_PRIME 1 downto 0 n file_ptr write t t c_o out std_ulogic_vector EXP_PRIME 1 downto 0 n file_ptr write t n file_ptr write end mod_mul n n\\n',\n",
       " 'def write_architecture file_ptr pp_list exp_prime file_ptr write architecture behavioral of mod_mul is n n write_dsp_component file_ptr signal_list write_signal_declaration file_ptr pp_list len pp_list file_ptr write t begin n n write_generate_component file_ptr write_regroup_dp file_ptr pp_list exp_prime file_ptr write t process begin n file_ptr write t wait until rising_edge clk n write_adder_tree file_ptr signal_list exp_prime len pp_list file_ptr write t end process n file_ptr write end architecture behavioral\\n',\n",
       " 'def write_adder_tree file_ptr signal_list exp_prime number_pp for i in range 1 len signal_list j 0 for pp in signal_list i if j len signal_list i 1 1 file_ptr write t t pp std_ulogic_vector resize unsigned signal_list i 1 j pp length n else file_ptr write t t pp std_ulogic_vector resize unsigned signal_list i 1 j pp length resize unsigned signal_list i 1 j 1 pp length n j 2 file_ptr write t t c_red std_ulogic_vector resize unsigned pp EXP_PRIME str len signal_list 2 downto EXP_PRIME c_red length resize unsigned pp EXP_PRIME 1 downto 0 c_red length file_ptr write c_o std_ulogic_vector resize unsigned c_red EXP_PRIME downto EXP_PRIME c_o length resize unsigned c_red EXP_PRIME 1 downto 0 c_o length\\n',\n",
       " 'def write_dsp_component file_ptr file_ptr write t component dsp_mul is n file_ptr write t t port n file_ptr write t t t clk in std_ulogic n file_ptr write t t t a_i in std_ulogic_vector M 1 downto 0 n file_ptr write t t t b_i in std_ulogic_vector N 1 downto 0 n file_ptr write t t t p_o out std_ulogic_vector M N 1 downto 0 n file_ptr write t t n file_ptr write t end component n n\\n',\n",
       " 'def write_signal_declaration file_ptr pp_list number_pp file_ptr write t signal a_s std_ulogic_vector M X 1 downto 0 n file_ptr write t signal b_s std_ulogic_vector N Y 1 downto 0 n file_ptr write t signal c_red std_ulogic_vector EXP_PRIME downto 0 n file_ptr write t type a_array is array 0 to X 1 of std_ulogic_vector M 1 downto 0 n file_ptr write t type b_array is array 0 to Y 1 of std_ulogic_vector N 1 downto 0 n file_ptr write t signal digit_a_s a_array n file_ptr write t signal digit_b_s b_array n file_ptr write t type ab_element is array 0 to Y 1 of std_ulogic_vector M N 1 downto 0 n file_ptr write t type ab_array is array 0 to X 1 of ab_element n file_ptr write t signal ab ab_array n n number_pp len pp_list adder_tree_depth math ceil math log number_pp 2 signal_list for i in range adder_tree_depth 1 file_ptr write t signal for i in range 0 number_pp signal_list 0 append pp str i if i number_pp 1 file_ptr write pp str i else file_ptr write pp str i file_ptr write std_ulogic_vector EXP_PRIME 1 downto 0 n for i in range adder_tree_depth file_ptr write t signal for j in range 0 len signal_list i 2 if j len signal_list i 2 file_ptr write signal_list i j signal_list i j 1 file_ptr write std_ulogic_vector EXP_PRIME str i downto 0 n signal_list i 1 append signal_list i j signal_list i j 1 elif j len signal_list i 1 file_ptr write signal_list i j x file_ptr write std_ulogic_vector EXP_PRIME str i downto 0 n signal_list i 1 append signal_list i j x else file_ptr write signal_list i j signal_list i j 1 signal_list i 1 append signal_list i j signal_list i j 1 file_ptr write n return signal_list\\n',\n",
       " 'def write_generate_component file_ptr file_ptr write t a_s std_ulogic_vector resize unsigned a_i a_s length n file_ptr write t b_s std_ulogic_vector resize unsigned b_i b_s length n n file_ptr write t GEN_INPUT_A for i in 0 to X 1 generate n file_ptr write t t digit_a_s i a_s M i 1 1 downto M i n file_ptr write t end generate n n file_ptr write t GEN_INPUT_B for i in 0 to Y 1 generate n file_ptr write t t digit_b_s i b_s n i 1 1 downto n i n file_ptr write t end generate n n file_ptr write t GEN_DSP_I for i in 0 to X 1 generate n file_ptr write t t GEN_DSP_J for j in 0 to Y 1 generate n file_ptr write t t t M1 DSP_MUL n file_ptr write t t t port map n file_ptr write t t t t clk clk n file_ptr write t t t t p_o ab i j n file_ptr write t t t t a_i digit_a_s i n file_ptr write t t t t b_i digit_b_s j n file_ptr write t t t n file_ptr write t t end generate GEN_DSP_J n file_ptr write t end generate GEN_DSP_I n n\\n',\n",
       " 'def write_regroup_dp file_ptr pp_list exp_prime for i in range len pp_list file_ptr write t pp str i for j in range exp_prime 1 1 1 success 0 for bit in pp_list i bit_list if bit absolute j success 1 file_ptr write ab str bit identifier 0 str bit identifier 1 str bit relative if success 0 file_ptr write 0 if j 0 file_ptr write file_ptr write n\\n',\n",
       " 'def op_decomp exp_prime dsp_mul_width_a dsp_mul_width_b x math ceil exp_prime float dsp_mul_width_a y math ceil exp_prime float dsp_mul_width_b m n dsp_mul_width_a dsp_mul_width_b if x y x y y x m n n m print Required DSP slices x y print Multiplier Width x m y n print m n str m str str n print x y str x str str y return m n x y\\n',\n",
       " 'def digit_product_generation m n x y dp_list for j in range 0 y for i in range 0 x lsb i m j n msb lsb m n 1 identifier i j dp_list append DigitProduct identifier lsb msb return dp_list\\n',\n",
       " 'def bit_slice_generation dp_list exp_prime sliced_list for dp in dp_list sliced_list dp slice_block sliced_list remove_unused_bits sliced_list exp_prime sliced_list shift_bits sliced_list exp_prime return sliced_list\\n',\n",
       " 'def remove_unused_bits bit_list limit reduced_list for bit in bit_list if bit absolute 2 limit 1 reduced_list append bit return reduced_list\\n',\n",
       " 'def shift_bits bit_list limit shifted_list for bit in bit_list bit shift limit shifted_list append bit return shifted_list\\n',\n",
       " 'def partial_product_generation bit_list exp_prime pp_list while bit_list pp PartialProduct exp_prime tmp_bit_list for bit in bit_list check pp add_bit bit if check is False tmp_bit_list append bit bit_list tmp_bit_list pp_list append pp return pp_list\\n',\n",
       " 'def print_partial_products pp_list cycles math ceil math log2 len pp_list 3 print Required cycles cycles print PARTIAL PRODUCTS for i bit in enumerate pp_list bit print_line i print\\n',\n",
       " 'def generate_vhdl_tb pp_list exp_prime file_ptr open vhdl mod_mul_tb vhd w cycles math ceil math log2 len pp_list 3 write_lib file_ptr write_entity file_ptr exp_prime write_architecture file_ptr cycles\\n',\n",
       " 'def write_lib file_ptr file_ptr write Library ieee n file_ptr write use ieee std_logic_1164 all n file_ptr write use ieee numeric_std all n file_ptr write use ieee std_logic_textio all n file_ptr write use std textio all n n\\n',\n",
       " 'def write_entity file_ptr exp_prime ceil_byte_exp_prime exp_prime rem exp_prime 8 if rem 0 ceil_byte_exp_prime exp_prime 8 exp_prime 8 file_ptr write entity mod_mul_tb is n file_ptr write t generic n file_ptr write t t TB_WIDTH integer str exp_prime n file_ptr write t t TV_WIDTH integer str ceil_byte_exp_prime n file_ptr write t n file_ptr write end mod_mul_tb n n\\n',\n",
       " 'def write_component file_ptr file_ptr write t component mod_mul is n file_ptr write t t port n file_ptr write t t t clk in std_ulogic n file_ptr write t t t a_i in std_ulogic_vector TB_WIDTH 1 downto 0 n file_ptr write t t t b_i in std_ulogic_vector TB_WIDTH 1 downto 0 n file_ptr write t t t c_o out std_ulogic_vector TB_WIDTH 1 downto 0 n file_ptr write t t n file_ptr write t end component n n\\n',\n",
       " 'def write_architecture file_ptr cycles cwd os getcwd file_ptr write architecture behavioral of mod_mul_tb is n n write_component file_ptr file_ptr write t signal a std_ulogic_vector TB_WIDTH 1 downto 0 n file_ptr write t signal b std_ulogic_vector TB_WIDTH 1 downto 0 n file_ptr write t signal c std_ulogic_vector TB_WIDTH 1 downto 0 n file_ptr write t signal result_test std_ulogic_vector TB_WIDTH 1 downto 0 n file_ptr write t signal clk std_ulogic 0 n n file_ptr write t file file_a text open read_mode is str cwd vhdl tv_a txt n file_ptr write t file file_b text open read_mode is str cwd vhdl tv_b txt n file_ptr write t file file_c text open read_mode is str cwd vhdl tv_c txt n n file_ptr write type test_array is array 1000 1 downto 0 of std_ulogic_vector TV_WIDTH 1 downto 0 file_ptr write t signal a_v test_array n file_ptr write t signal b_v test_array n file_ptr write t signal result_v test_array n n file_ptr write t begin n file_ptr write t clk not clk after 5 ns n n file_ptr write t readf_testvectors process n file_ptr write t t variable rdline line n file_ptr write t t variable a_var b_var std_ulogic_vector TV_WIDTH 1 downto 0 n file_ptr write t t variable c_var std_ulogic_vector TV_WIDTH 1 downto 0 n file_ptr write t t begin n file_ptr write t t t for i in 0 to 1000 1 loop n file_ptr write t t t t read key n file_ptr write t t t t readline file_a rdline n file_ptr write t t t t hread rdline a_var n file_ptr write t t t t a_v i a_var n file_ptr write t t t t read plain n file_ptr write t t t t readline file_b rdline n file_ptr write t t t t hread rdline b_var n file_ptr write t t t t b_v i b_var n file_ptr write t t t t read cipher n file_ptr write t t t t readline file_c rdline n file_ptr write t t t t hread rdline c_var n file_ptr write t t t t result_v i c_var n file_ptr write t t t end loop n file_ptr write t t wait n file_ptr write t end process readf_testvectors n n file_ptr write t stim_results process n file_ptr write t t begin n file_ptr write t t wait for 100ns n file_ptr write t t for i in 0 to 1000 1 loop n file_ptr write t t t a a_v i TB_WIDTH 1 downto 0 n file_ptr write t t t b b_v i TB_WIDTH 1 downto 0 n file_ptr write t t t result_test result_v i TB_WIDTH 1 downto 0 n for i in range cycles 1 file_ptr write t t t wait until rising_edge clk n file_ptr write t t t assert c result_test report wrong result severity failure n file_ptr write t t end loop n file_ptr write t t report SUCCESS all modular multiplications successfully calculated n file_ptr write t t wait n file_ptr write t end process stim_results n n file_ptr write t DUT mod_mul port map n file_ptr write t t clk clk n file_ptr write t t a_i a TB_WIDTH 1 downto 0 n file_ptr write t t b_i b TB_WIDTH 1 downto 0 n file_ptr write t t c_o c TB_WIDTH 1 downto 0 n file_ptr write t n n file_ptr write t end architecture behavioral n\\n',\n",
       " 'def get_feature_config self feature_name raise NotImplementedError\\n',\n",
       " 'def set_feature_config self feature_name feature_data raise NotImplementedError\\n',\n",
       " 'def get_user_id self user return self get_user_property user id\\n',\n",
       " 'def get_user_property self user property_name default_value None raise NotImplementedError\\n',\n",
       " 'def get_groups self user return self get_user_property user groups\\n',\n",
       " 'def is_in_group self user group_or_groups if not isinstance group_or_groups list set group_or_groups group_or_groups user_groups self get_groups user return set group_or_groups set user_groups\\n',\n",
       " 'def __init__ self name groups None percentage None randomize False users None self name name if groups is None groups if ALL in groups assert NONE not in groups self groups groups if users is None users self users users if percentage is None percentage 0 self percentage percentage self randomize randomize\\n',\n",
       " 'def __repr__ self s if self name s append s self name for p in groups percentage randomize users v getattr self p s append s s p repr v s join s return Feature s s\\n',\n",
       " 'def __str__ self repr_string Feature NAME s format NAME self name config_string if len self users 0 config_string Users s self users if len self groups 0 config_string Groups s self groups if self percentage 0 config_string Percent PCT RAND format PCT self percentage RAND self randomize return repr_string config_string strip\\n',\n",
       " 'def can self user_storage user if self can_group user_storage user return True elif self can_user_id user_storage user return True elif self can_user_pct user_storage user return True else return False\\n',\n",
       " 'def can_group self user_storage user if ALL in self groups return True elif NONE in self groups return False else return user_storage is_in_group user self groups\\n',\n",
       " 'def can_user_id self user_storage user return user_storage get_user_id user in self users\\n',\n",
       " 'def can_user_pct self user_storage user user_id int user_storage get_user_id user if self randomize user_id zlib crc32 self name return user_id 100 self percentage\\n',\n",
       " 'def __init__ self feature_storage None user_storage None undefined_feature_access False if feature_storage is None from storage memory import MemoryFeatureStorage self feature_storage MemoryFeatureStorage else self feature_storage feature_storage if user_storage is None from storage memory import MemoryUserStorage self user_storage MemoryUserStorage else self user_storage user_storage self default_undefined_feature undefined_feature_access\\n',\n",
       " 'def add_feature self feature self feature_storage set_feature_config feature name feature_data feature\\n',\n",
       " 'def can self user feature_name feature self feature_storage get_feature_config feature_name if feature is None return self default_undefined_feature return feature can self user_storage user\\n',\n",
       " 'def sframe_to_scipy x column_name assert x column_name dtype dict The chosen column must be dict type representing sparse data x x add_row_number x x stack column_name feature value f graphlab feature_engineering OneHotEncoder features feature f fit x x f transform x mapping f feature_encoding x feature_id x encoded_features dict_keys apply lambda x x 0 i np array x id j np array x feature_id v np array x value width x id max 1 height x feature_id max 1 mat csr_matrix v i j shape width height return mat mapping\\n',\n",
       " 'def logpdf_diagonal_gaussian x mean cov n x shape 0 dim x shape 1 assert dim len mean and dim len cov scaled_x x dot diag 1 0 2 np sqrt cov scaled_mean mean 2 np sqrt cov return np sum np log np sqrt 2 np pi cov pairwise_distances scaled_x scaled_mean euclidean flatten 2\\n',\n",
       " 'def log_sum_exp x axis x_max np max x axis axis if axis 1 return x_max np log np sum np exp x x_max np newaxis axis 1 else return x_max np log np sum np exp x x_max axis 0\\n',\n",
       " 'def getMs counts N len counts M1 0 val1 0 M2 0 val2 0 Mstar 0 valstar 1 for index val in enumerate counts if index 2 continue if val valstar Mstar index valstar val elif index Mstar 0 1 N break for index val in enumerate counts if index Mstar break elif val val1 val1 val M1 index for index val in enumerate counts if index Mstar continue elif val val2 val2 val M2 index return M1 Mstar M2\\n',\n",
       " 'def complete_graph_dX X t tau gamma N This system is given in Proposition 2 3 taking Q S T I f_ SI k f_ QT k tau f_ IS k f_ TQ gamma dot Y 0 gamma Y 1 0 dot Y 1 2 gamma Y 2 0Y 0 gamma N 1 tau Y 1 dot Y 2 3 gamma Y 3 N 1 tau Y 1 2 gamma 2 N 2 Y 2 dot Y N N 1 tau Y N 1 N gamma Y N Note that X has length N 1 dX dX append gamma X 1 for k in range 1 N dX append k 1 gamma X k 1 N k 1 k 1 tau X k 1 N k k tau k gamma X k dX append N 1 tau X N 1 N gamma X N return scipy array dX\\n',\n",
       " 'def get_deg_probs G degree_count Counter G degree values degree_prob x degree_count x float G order for x in degree_count return degree_prob\\n',\n",
       " 'def star_graph_dX X t tau gamma N Y1vec 0 list X 0 N Y2vec list X N 0 dY1vec dY2vec for k in range 1 N dY1vec append N k 1 tau Y1vec k 1 k 1 tau Y2vec k 1 k gamma Y1vec k 1 N k tau k 1 gamma gamma Y1vec k dY1vec append tau Y1vec N 1 N 1 tau Y2vec N 1 N gamma Y1vec N dY2vec append gamma N 1 Y1vec 1 gamma Y2vec 1 0 for k in range 1 N dY2vec append 0 gamma Y1vec k 1 gamma k 1 Y2vec k 1 k tau 0 k gamma Y2vec k return scipy array dY1vec dY2vec\\n',\n",
       " 'def visualize G plot_times node_history pos None filetype png filenamebase tmp colorS 009a80 colorI ff2020 colorR gray show_edges True highlightSI False plot_args number_by_time False if pos is None pos nx spring_layout G for index time in enumerate plot_times plt clf S set I set R set for node in G nodes if node not in node_history S add node else changetimes node_history node 0 number_swaps len changetime for changetime in changetimes if changetime time status node_history node 1 number_swaps 1 if status S S add node elif status I I add node else R add node nx draw G plot_args pos pos node_color colorS nodelist list S edgelist nx draw G plot_args pos pos node_color colorI nodelist list I edgelist nx draw G plot_args pos pos node_color colorR nodelist list R edgelist if show_edges if not highlightSI nx draw G pos plot_args nodelist edgelist G edges else SIedges u v for u v in G edges if u in S and v in I or u in I and v in S nonSIedges edge for edge in G edges if edge not in SIedges nx draw G pos nodelist edgelist list SIedges edge_color colorI nx draw G pos nodelist edgelist list nonSIedges if number_by_time plt savefig filenamebase str time replace p filetype bbox_inches tight else plt savefig filenamebase str index replace p filetype bbox_inches tight\\n',\n",
       " 'def plot_network self G status pos None nodelist None colordict S 009a80 I ff2020 R gray nx_kwargs colorlist if pos is None pos nx spring_layout G if nodelist is None nodelist list G nodes I_nodes node for node in nodelist if status node I other_nodes node for node in nodelist if status node I random shuffle other_nodes nodelist other_nodes I_nodes edgelist list G edges else nodeset set nodelist edgelist edge for edge in G edges if edge 0 in nodeset and edge 1 in nodeset for node in nodelist colorlist append colordict status node nx draw_networkx_edges G pos edgelist edgelist ax self network_axes nx_kwargs nx draw_networkx_nodes G pos nodelist nodelist node_color colorlist ax self network_axes nx_kwargs self network_axes set_xticks self network_axes set_yticks\\n',\n",
       " 'def plot_timeseries_from_analytic_model self G tau gamma initial_status model EBCM_from_graph tmin 0 tmax 10 tcount 1001 SIR True colordict S 009a80 I ff2020 R gray kwargs initial_infecteds node for node in G nodes if initial_status node I if SIR initial_recovereds node for node in G nodes if initial_status node R t S I R model G tau gamma initial_infecteds initial_infecteds initial_recovereds initial_recovereds tmin tmin tmax tmax tcount tcount return_full_data False else t S I model G tau gamma initial_infecteds initial_infecteds tmin tmin tmax tmax tcount tcount return_full_data False R None self plot_timeseries t S S I I R R colordict colordict kwargs\\n',\n",
       " 'def generate_network Pk N ntries 100 counter 0 while counter ntries counter 1 ks for ctr in range N ks append Pk if sum ks 2 0 break if sum ks 2 1 raise EoN EoNError cannot generate even degree sum G nx configuration_model ks return G\\n',\n",
       " 'def _get_rate_functions_ G tau gamma transmission_weight None recovery_weight None if transmission_weight is None trans_rate_fxn lambda x y tau else trans_rate_fxn lambda x y tau G edge x y transmission_weight if recovery_weight is None rec_rate_fxn lambda x gamma else rec_rate_fxn lambda x gamma G node x recovery_weight return trans_rate_fxn rec_rate_fxn\\n',\n",
       " 'def subsample report_times times status1 status2 None status3 None Given S I and or R as lists of numbers of nodes of the given status at given times returns them subsampled at specific report_times Arguments report_times iterable ordered times at which we want to know state of system times iterable ordered times at which we have the system state assumed no change between these times statusX X one of 1 2 or 3 iterable order corresponds to times generally S I or R number of nodes in given status Returns If only status1 is defined report_status1 scipy array gives status1 subsampled just at report_times If more are defined then it returns a list either report_status1 report_status2 or report_status1 report_status2 report_status3 SAMPLE USE import networkx as nx import EoN import scipy import matplotlib pyplot as plt in this example we will run 100 stochastic simulations Each simulation will produce output at a different set of times In order to calculate an average we will use subsample to find the epidemic sizes at a specific set of times given by report_times G nx fast_gnp_random_graph 10000 0 001 tau 1 gamma 1 report_times scipy linspace 0 5 101 Ssum scipy zeros len report_times Isum scipy zeros len report_times Rsum scipy zeros len report_times iterations 100 for counter in range iterations t S I R EoN fast_SIR G tau gamma initial_infecteds range 10 t S I and R have an entry for every single event newS newI newR EoN subsample report_times t S I R could also do newI EoN subsample report_times t I plt plot report_times newS linewidth 1 alpha 0 4 plt plot report_times newI linewidth 1 alpha 0 4 plt plot report_times newR linewidth 1 alpha 0 4 Ssum newS Isum newI Rsum newR Save Ssum float iterations Iave Isum float iterations Rave Rsum float iterations plt plot report_times Save linewidth 5 label average plt plot report_times Iave linewidth 5 plt plot report_times Rave linewidth 5 plt legend loc upper right plt savefig tmp pdf If only one of the sample times is given then returns just that If report_times goes longer than times then this simply assumes the system freezes in the final state This uses a recursive approach if multiple arguments are defined if report_times 0 times 0 raise EoN EoNError report_times 0 times 0 report_status1 next_report_index 0 next_observation_index 0 while next_report_index len report_times while next_observation_index len times and times next_observation_index report_times next_report_index candidate status1 next_observation_index next_observation_index 1 report_status1 append candidate next_report_index 1 report_status1 scipy array report_status1 if status2 is not None if status3 is not None report_status2 report_status3 subsample report_times times status2 status3 return report_status1 report_status2 report_status3 else report_status2 subsample report_times times status2 return report_status1 report_status2 else return report_status1\\n',\n",
       " 'def get_time_shift times L threshold Identifies the first time at which L crosses a threshold Useful for shifting times Arguments times list or scipy array ordered the times we have observations L a list or scipy array order of L corresponds to times threshold number a threshold value Returns t number the first time at which L reaches or exceeds a threshold SAMPLE USE import networkx as nx import EoN import scipy import matplotlib pyplot as plt in this example we will run 20 stochastic simulations We plot the unshifted curves grey and the curves shifted so that t 0 when 1 have been infected I R 0 01N red plt clf just clearing any previous plotting N 100000 kave 10 G nx fast_gnp_random_graph N kave N 1 tau 0 2 gamma 1 report_times scipy linspace 0 5 101 Ssum scipy zeros len report_times Isum scipy zeros len report_times Rsum scipy zeros len report_times iterations 20 for counter in range iterations R 0 while R 1 1000 if an epidemic doesn t happen repeat t S I R EoN fast_SIR G tau gamma print R 1 plt plot t I linewidth 1 color gray alpha 0 4 tshift EoN get_time_shift t I R 0 01 N plt plot t tshift I color red linewidth 1 alpha 0 4 plt savefig timeshift_demonstration pdf for index t in enumerate times if L index threshold break return t\\n',\n",
       " 'def complete_graph_dX X t tau gamma N This system is given in Proposition 2 3 taking Q S T I f_ SI k f_ QT k tau f_ IS k f_ TQ gamma dot Y 0 gamma Y 1 0 dot Y 1 2 gamma Y 2 0Y 0 gamma N 1 tau Y 1 dot Y 2 3 gamma Y 3 N 1 tau Y 1 2 gamma 2 N 2 Y 2 dot Y N N 1 tau Y N 1 N gamma Y N Note that X has length N 1 dX dX append gamma X 1 for k in range 1 N dX append k 1 gamma X k 1 N k 1 k 1 tau X k 1 N k k tau k gamma X k dX append N 1 tau X N 1 N gamma X N return scipy array dX\\n',\n",
       " 'def star_graph_dX X t tau gamma N Y1vec 0 list X 0 N Y2vec list X N 0 dY1vec dY2vec for k in range 1 N dY1vec append N k 1 tau Y1vec k 1 k 1 tau Y2vec k 1 k gamma Y1vec k 1 N k tau k 1 gamma gamma Y1vec k dY1vec append tau Y1vec N 1 N 1 tau Y2vec N 1 N gamma Y1vec N dY2vec append gamma N 1 Y1vec 1 gamma Y2vec 1 0 for k in range 1 N dY2vec append 0 gamma Y1vec k 1 gamma k 1 Y2vec k 1 k tau 0 k gamma Y2vec k return scipy array dY1vec dY2vec\\n',\n",
       " 'def __init__ self sasl_client_factory mechanism trans self _trans trans self sasl_client_factory sasl_client_factory self sasl None self mechanism mechanism self __wbuf BufferIO self __rbuf BufferIO self opened False self encode None\\n',\n",
       " 'def plot_photometry data_file model_file overplot_file None autosave False saveformat svg title None figsize 10 5 data read_simtoi_photometry data_file model read_simtoi_photometry model_file assert len data len model fig top bottom plt subplots 2 1 sharex True sharey False figsize figsize top errorbar data date data mag yerr data mag_err fmt label Data if overplot_file overplot_data read_simtoi_photometry overplot_file top plot overplot_data date overplot_data mag label Model overplot_data close else top errorbar model date model mag fmt x label Model top invert_yaxis top grid True top set_ylabel Magnitude top legend numpoints 1 residuals model mag data mag bottom errorbar data date residuals yerr data mag_err fmt color red bottom invert_yaxis bottom grid True bottom set_ylabel Residuals m d mag if autosave save_plot data_file saveformat saveformat else plt show\\n',\n",
       " 'def v2_plot data_file model_file autosave False saveformat svg title None figsize 10 5 data read_ccoifits_v2 data_file uv_scale 1000000 0 model read_ccoifits_v2 model_file uv_scale 1000000 0 assert len data len model fig top bottom plt subplots 2 1 sharex True sharey False figsize figsize legend_data legend_model plot_v2 data model top legend_residuals legend_lb legend_ub plot_v2_residuals data model bottom legend top legend legend_data legend_model legend_residuals legend_lb legend_ub Data Model Residuals Lower Higher numpoints 1 loc upper center bbox_to_anchor 0 5 0 05 ncol 5 legend draw_frame False if autosave save_plot data_file saveformat saveformat else plt show\\n',\n",
       " 'def plot_v2 data model axis if data None or model None return None None v2_data data v2 v2_err_ub data v2_err v2_err_lb data v2_err for i in range 0 len v2_data if v2_err_lb i v2_data i v2_err_lb i 0 99999 v2_data i plotline_data axis errorbar data uv_mag v2_data yerr v2_err_lb v2_err_ub fmt label Data plotline_model axis errorbar model uv_mag model v2 fmt x label Model axis set_ylabel V 2 axis grid True axis set_yscale log return plotline_data plotline_model\\n',\n",
       " 'def plot_v2_residuals data model axis if data None or model None return None None None ylimits 5 5 residuals model v2 data v2 errors residuals data v2_err errors lb ub clip errors ylimits plotline_residuals axis errorbar data uv_mag errors fmt color red label Error plotline_lb axis errorbar data uv_mag lb fmt v color red plotline_ub axis errorbar data uv_mag ub fmt color red axis set_ylabel Residuals m d sigma axis set_xlabel Baseline length mega lambda axis set_ylim 6 6 axis grid True return plotline_residuals plotline_lb plotline_ub\\n',\n",
       " 'def t3_plot data_file model_file autosave False saveformat svg title None figsize 10 10 data read_ccoifits_t3 data_file uv_scale 1000000 0 model read_ccoifits_t3 model_file uv_scale 1000000 0 assert len data len model fig A B C D plt subplots 4 1 sharex True sharey False figsize figsize legend_data legend_model plot_t3_amp data model A legend_residuals legend_lb legend_ub plot_t3_amp_residuals data model B plot_t3_phi data model C plot_t3_phi_residuals data model D legend A legend legend_data legend_model legend_residuals legend_lb legend_ub Data Model Residuals Lower Higher numpoints 1 loc upper center bbox_to_anchor 0 5 0 05 ncol 5 legend draw_frame False if autosave save_plot data_file saveformat saveformat else plt show\\n',\n",
       " 'def plot_t3_amp data model axis ylim None if data None or model None return None None t3_data data t3_amp t3_err_ub data t3_amp_err t3_err_lb data t3_amp_err for i in range 0 len t3_data if t3_err_lb i t3_data i t3_err_lb i 0 99999 t3_data i plotline_data axis errorbar data uv_mag t3_data yerr t3_err_lb t3_err_ub fmt label Data plotline_model axis errorbar model uv_mag model t3_amp fmt x label Model axis set_ylabel T_3 A axis grid True axis set_yscale log return plotline_data plotline_model\\n',\n",
       " 'def plot_t3_amp_residuals data model axis if data None or model None return None None ylimits 5 5 residuals model t3_amp data t3_amp errors residuals data t3_amp_err errors lb ub clip errors ylimits plotline_residuals axis errorbar data uv_mag errors fmt color red label Error plotline_lb axis errorbar data uv_mag lb fmt v color red plotline_ub axis errorbar data uv_mag ub fmt color red axis set_ylabel Residuals m d sigma axis set_ylim 6 6 axis grid True return plotline_residuals plotline_lb plotline_ub\\n',\n",
       " 'def plot_t3_phi data model axis if data None or model None return None None plotline_data axis errorbar data uv_mag data t3_phi yerr data t3_phi_err fmt label Data plotline_model axis errorbar model uv_mag model t3_phi fmt x label Model axis set_ylabel T_3 phi deg axis grid True axis set_ylim 200 200 return plotline_data plotline_model\\n',\n",
       " 'def plot_t3_phi_residuals data model axis if data None or model None return None None ylimits 5 5 residuals model t3_phi data t3_phi errors residuals data t3_phi_err errors lb ub clip errors ylimits plotline_residuals axis errorbar data uv_mag errors fmt color red label Error plotline_lb axis errorbar data uv_mag lb fmt v color red plotline_ub axis errorbar data uv_mag ub fmt color red axis set_ylabel Residuals m d sigma axis set_xlabel Triplet length mega lambda axis set_ylim 6 6 axis grid True return plotline_residuals plotline_lb plotline_ub\\n',\n",
       " 'def clip values limits good values copy low values copy high values copy lb limits 0 ub limits 1 for i in range 0 len values value values i if value lb good i float NaN high i float NaN low i lb elif value ub good i float NaN low i float NaN high i ub else high i float NaN low i float NaN return good low high\\n',\n",
       " 'def plot_uv v2_filename t3_filename autosave False saveformat svg figsize 10 10 v2 read_ccoifits_v2 v2_filename uv_scale 1000000 0 t3 read_ccoifits_t3 t3_filename uv_scale 1000000 0 fig plt figure figsize figsize ax1 fig add_subplot 1 1 1 aspect equal ax1 scatter v2 v v2 u c green marker s label V2 ax1 scatter v2 v v2 u c green marker s ax1 scatter t3 v1 t3 u1 c red marker x label T3 ax1 scatter t3 v1 t3 u1 c red marker x ax1 scatter t3 v2 t3 u2 c red marker x ax1 scatter t3 v2 t3 u2 c red marker x ax1 scatter t3 v3 t3 u3 c red marker x ax1 scatter t3 v3 t3 u3 c red marker x ax1 grid True ax1 set_ylabel V mega lambda ax1 set_xlabel U mega lambda ax1 legend numpoints 1 if autosave save_plot data_file saveformat saveformat else plt show\\n',\n",
       " 'def main directory usage Usage prog options directory parser OptionParser usage usage parser add_option autosave dest autosave action store_true default False help Automatically save the plots default False parser add_option savefmt dest savefmt action store type string default svg help Automatic save file format default default options args parser parse_args directory args 0 filenames os listdir directory if options autosave try matplotlib use options savefmt except pass for filename in filenames if re search _model phot filename model_file directory filename data_file directory re sub _model filename plot_photometry data_file model_file autosave options autosave saveformat options savefmt elif re search _model_v2 txt filename model_file directory filename data_file directory re sub _model_v2 txt _v2 txt filename v2_plot data_file model_file autosave options autosave saveformat options savefmt elif re search _model_t3 txt filename model_file directory filename data_file directory re sub _model_t3 txt _t3 txt filename t3_plot data_file model_file autosave options autosave saveformat options savefmt\\n',\n",
       " 'def factory args cxn name args assembler lower assembler ASSEMBLERS name return assembler args cxn\\n',\n",
       " 'def command_line_args parser group parser add_argument_group optional assembler arguments group add_argument no long reads action store_true help Do not use long reads during assembly Abyss Trinity Velvet group add_argument kmer type int default 64 help k mer size The default is 64 for Abyss and 31 for Velvet Note the maximum kmer length for Velvet is 31 Abyss Velvet group add_argument mpi action store_true help Use MPI for this assembler The assembler must have been compiled to use MPI Abyss group add_argument bowtie2 action store_true help Use bowtie2 during assembly Trinity max_mem max 1 0 math floor psutil virtual_memory available 1024 3 2 group add_argument max memory default max_mem metavar MEMORY type int help Maximum amount of memory to use in gigabytes The default is Trinity Spades format max_mem group add_argument exp coverage expected coverage type int default 30 help The expected coverage of the region The default is 30 Velvet group add_argument ins length type int default 300 help The size of the fragments used in the short read library The default is 300 Velvet group add_argument min contig length type int default 100 help The minimum contig length used by the assembler itself The default is 100 Velvet group add_argument cov cutoff default off help Read coverage cutoff value Must be a positive float value or auto or off The default is off Spades\\n',\n",
       " 'def default_kmer kmer assembler if assembler velvet and kmer 31 kmer 31 return kmer\\n',\n",
       " 'def default_cov_cutoff cov_cutoff if cov_cutoff in off auto return cov_cutoff err Read coverage cutoff value Must be a positive float value or auto or off try value float cov_cutoff except ValueError log fatal err if value 0 log fatal err return cov_cutoff\\n',\n",
       " 'def find_program assembler_name program assembler_arg option True if assembler_arg assembler_name and option and not which program err We could not find the program You either need to install it or you need to adjust the PATH environment variable with the path option so that aTRAM can find it format program sys exit err\\n',\n",
       " 'def reverse_complement seq return seq translate COMPLEMENT 1\\n',\n",
       " 'def is_protein seq return IS_PROTEIN search seq\\n',\n",
       " 'def fasta_file_has_protein query_files for query_file in query_files with open query_file as in_file for query in SeqIO parse in_file fasta if is_protein str query seq return True return False\\n',\n",
       " 'def __init__ self args cxn self args args self blast_only False self steps self file self state iteration 0 query_target query_file blast_db cxn cxn\\n',\n",
       " 'def initialize_iteration self blast_db query_file iteration self set_state blast_db query_file iteration self file long_reads self file output self iter_file output fasta self file paired_1 self iter_file paired_1 fasta self file paired_2 self iter_file paired_2 fasta self file single_1 self iter_file single_1 fasta self file single_2 self iter_file single_2 fasta self file single_any self iter_file single_any fasta self file paired_count 0 self file single_1_count 0 self file single_2_count 0 self file single_any_count 0\\n',\n",
       " 'def set_state self blast_db query_file iteration self state blast_db blast_db self state query_file query_file self state iteration iteration if iteration 1 self state query_target query_file\\n',\n",
       " 'def iter_dir self return util iter_dir self args temp_dir self state blast_db self state query_target self state iteration\\n',\n",
       " 'def iter_file self file_name rel_path join self iter_dir file_name return rel_path\\n',\n",
       " 'def work_path self return self iter_dir\\n',\n",
       " 'def run self try log info Assembling shards with iteration format self args assembler self state iteration self assemble except TimeoutError msg Time ran out for the assembler after HH MM SS format datetime timedelta seconds self args timeout log error msg raise TimeoutError msg except CalledProcessError as cpe msg The assembler failed with error str cpe log error msg raise RuntimeError msg\\n',\n",
       " 'def no_blast_hits self if not db sra_blast_hits_count self state cxn self state iteration log info No blast hits in iteration format self state iteration return True return False\\n',\n",
       " 'def nothing_assembled self if not exists self file output or not getsize self file output log info No new assemblies in iteration format self state iteration return True return False\\n',\n",
       " 'def assembled_contigs_count self high_score count db assembled_contigs_count self state cxn self state iteration self args bit_score self args contig_length if not count log info No contigs had a bit score greater than and are at least bp long in iteration The highest score for this iteration is format self args bit_score self args contig_length self state iteration high_score return count\\n',\n",
       " 'def no_new_contigs self count if count db iteration_overlap_count self state cxn self state iteration self args bit_score self args contig_length log info No new contigs were found in iteration format self state iteration return True return False\\n',\n",
       " 'def assemble self for step in self steps cmd step log subcommand cmd self args temp_dir self args timeout self post_assembly\\n',\n",
       " 'def post_assembly self\\n',\n",
       " 'staticmethod def parse_contig_id header return header split 0\\n',\n",
       " 'def write_input_files self log info Writing assembler input files iteration format self state iteration self write_paired_input_files self write_single_input_files\\n',\n",
       " 'def write_paired_input_files self with open self file paired_1 w as end_1 open self file paired_2 w as end_2 for row in db get_blast_hits_by_end_count self state cxn self state iteration 2 self file paired_count 1 out_file end_1 if row seq_end 1 else end_2 out_file write n format row seq_name row seq_end out_file write n format row seq\\n',\n",
       " 'def write_single_input_files self with open self file single_1 w as end_1 open self file single_2 w as end_2 open self file single_any w as end_any for row in db get_blast_hits_by_end_count self state cxn self state iteration 1 if row seq_end 1 out_file end_1 seq_end 1 self file single_1_count 1 elif row seq_end 2 out_file end_2 seq_end 2 self file single_2_count 1 else out_file end_any seq_end self file single_any_count 1 out_file write n format row seq_name seq_end out_file write n format row seq\\n',\n",
       " 'def final_output_prefix self blast_db query blast_db basename blast_db query splitext basename query 0 return _ format self args output_prefix blast_db query\\n',\n",
       " 'def write_final_output self blast_db query prefix self final_output_prefix blast_db query self write_filtered_contigs prefix self write_all_contigs prefix\\n',\n",
       " 'def write_filtered_contigs self prefix if self args no_filter return count db all_assembled_contigs_count self state cxn self args bit_score self args contig_length if not count return file_name format prefix filtered_contigs fasta contigs db get_all_assembled_contigs self state cxn self args bit_score self args contig_length with open file_name w as output_file for contig in contigs self output_assembled_contig output_file contig\\n',\n",
       " 'def write_all_contigs self prefix count db all_assembled_contigs_count self state cxn if not count return file_name format prefix all_contigs fasta contigs db get_all_assembled_contigs self state cxn with open file_name w as output_file for contig in contigs self output_assembled_contig output_file contig\\n',\n",
       " 'staticmethod def output_assembled_contig output_file contig seq contig seq suffix if contig query_strand and contig hit_strand and contig query_strand contig hit_strand seq bio reverse_complement seq suffix _REV header _ iteration contig_id score n format contig iteration contig contig_id suffix contig iteration contig contig_id contig bit_score output_file write header output_file write n format seq\\n',\n",
       " 'def get_single_ends self single_ends if self file single_1_count single_ends append self file single_1 if self file single_2_count single_ends append self file single_2 if self file single_any_count single_ends append self file single_any return single_ends\\n',\n",
       " 'def simple_state self return blast_db self state blast_db iteration self state iteration query_file self state query_file query_target self state query_target\\n',\n",
       " 'def create_db temp_dir fasta_file shard cmd makeblastdb dbtype nucl in out cmd cmd format fasta_file shard log subcommand cmd temp_dir\\n',\n",
       " 'def against_sra args state hits_file shard cmd if args protein and state iteration 1 cmd append tblastn cmd append db_gencode format args db_gencode else cmd append blastn cmd append evalue format args evalue cmd append outfmt 15 cmd append max_target_seqs format args max_target_seqs cmd append out format hits_file cmd append db format shard cmd append query format state query_file command join cmd log subcommand command args temp_dir\\n',\n",
       " 'def against_contigs blast_db query_file hits_file kwargs cmd if kwargs protein cmd append tblastn cmd append db_gencode format kwargs db_gencode else cmd append blastn cmd append db format blast_db cmd append query format query_file cmd append out format hits_file cmd append outfmt 15 command join cmd log subcommand command kwargs temp_dir\\n',\n",
       " 'def all_shard_paths blast_db pattern blast nhr format blast_db files glob glob pattern if not files err No blast shards found Looking for Verify the work dir and file prefix options format pattern 4 log fatal err return sorted f 4 for f in files\\n',\n",
       " 'def output_file_name temp_dir shrd_path shard_name basename shrd_path file_name results json format shard_name return join temp_dir file_name\\n',\n",
       " 'def temp_db_name temp_dir blast_db file_name basename blast_db return join temp_dir file_name\\n',\n",
       " 'def get_raw_hits json_file with open json_file as blast_file raw blast_file read if not raw return try obj json loads raw except json decoder JSONDecodeError err Blast output is not in JSON format You may need to upgrade blast log fatal err return obj BlastOutput2 0 report results search get hits\\n',\n",
       " 'def hits json_file hits_list raw_hits get_raw_hits json_file for raw in raw_hits for i desc in enumerate raw description hit dict desc hit len raw len hit update raw hsps i hits_list append hit return hits_list\\n',\n",
       " 'def command_line_args parser group parser add_argument_group optional blast arguments group add_argument db gencode type int default 1 metavar CODE help The genetic code to use during blast runs The default is 1 group add_argument evalue type float default 1e 10 help The default evalue is 1e 10 group add_argument max target seqs type int default 100000000 metavar MAX help Maximum hit sequences per shard Default is calculated based on the available memory and the number of shards\\n',\n",
       " 'def default_max_target_seqs max_target_seqs blast_db max_memory if not max_target_seqs all_shards all_shard_paths blast_db max_target_seqs int 2 max_memory len all_shards 1000000 0 return max_target_seqs\\n',\n",
       " 'def default_shard_count shard_count sra_files if not shard_count total_fasta_size 0 for file_name in sra_files file_size os path getsize file_name if file_name lower endswith q file_size 2 total_fasta_size file_size shard_count int total_fasta_size 250000000 0 shard_count shard_count if shard_count else 1 return shard_count\\n',\n",
       " 'def make_blast_output_dir blast_db output_dir os path dirname blast_db if output_dir and output_dir not in os makedirs output_dir exist_ok True\\n',\n",
       " 'def touchup_blast_db_names blast_dbs pattern atram _preprocessor log blast_ d 3 nhr nin nsq sqlite db db_names for blast_db in blast_dbs db_names append re sub pattern 1 blast_db re I re X return db_names\\n',\n",
       " 'def find_program program if not which makeblastdb and which tblastn and which blastn err We could not find the programs You either need to install it or you need adjust the PATH environment variable with the path option so that aTRAM can find it format program sys exit err\\n',\n",
       " 'def __init__ self args cxn super __init__ args cxn self steps self blast_only True\\n',\n",
       " 'def write_final_output self blast_db query prefix self final_output_prefix blast_db query file_name fasta format prefix with open file_name w as output_file for row in db get_sra_blast_hits self state cxn 1 output_file write n format row seq_name row seq_end output_file write n format row seq\\n',\n",
       " 'def connect blast_db check_version False clean False db_name sqlite db format blast_db if clean and exists db_name os remove db_name if check_version and not exists db_name err Could not find the database file format db_name sys exit err cxn sqlite3 connect db_name cxn execute PRAGMA page_size format 2 16 cxn execute PRAGMA busy_timeout 10000 cxn execute PRAGMA journal_mode OFF cxn execute PRAGMA synchronous OFF if check_version check_versions cxn return cxn\\n',\n",
       " 'def aux_db cxn temp_dir blast_db query_name db_dir join temp_dir db os makedirs db_dir exist_ok True db_name _ _temp sqlite db format basename blast_db basename query_name db_name join db_dir db_name sql ATTACH DATABASE AS aux format db_name cxn execute sql\\n',\n",
       " 'def aux_detach cxn cxn execute DETACH DATABASE aux\\n',\n",
       " 'def check_versions cxn version get_version cxn if version DB_VERSION err The database was built with version but you are running version You need to rebuild the atram database by running atram_preprocessor py again format version DB_VERSION sys exit err\\n',\n",
       " 'def create_metadata_table cxn cxn execute DROP TABLE IF EXISTS metadata sql CREATE TABLE metadata label TEXT value TEXT cxn execute sql with cxn sql INSERT INTO metadata label value VALUES cxn execute sql version DB_VERSION cxn commit\\n',\n",
       " 'def get_version cxn sql SELECT value FROM metadata WHERE label try result cxn execute sql version return result fetchone 0 except sqlite3 OperationalError return 1 0\\n',\n",
       " 'def create_sequences_table cxn cxn execute DROP TABLE IF EXISTS sequences sql CREATE TABLE sequences seq_name TEXT seq_end TEXT seq TEXT cxn execute sql\\n',\n",
       " 'def create_sequences_index cxn sql CREATE INDEX sequences_index ON sequences seq_name seq_end cxn execute sql\\n',\n",
       " 'def insert_sequences_batch cxn batch if batch sql INSERT INTO sequences seq_name seq_end seq VALUES cxn executemany sql batch cxn commit\\n',\n",
       " 'def get_sequence_count cxn result cxn execute SELECT COUNT FROM sequences return result fetchone 0\\n',\n",
       " 'def get_shard_cut cxn offset sql SELECT seq_name FROM sequences ORDER BY seq_name LIMIT 1 OFFSET result cxn execute sql format offset cut result fetchone 0 return cut\\n',\n",
       " 'def get_sequences_in_shard cxn start end sql SELECT seq_name seq_end seq FROM sequences WHERE seq_name AND seq_name return cxn execute sql start end\\n',\n",
       " 'def create_sra_blast_hits_table cxn cxn execute DROP TABLE IF EXISTS aux sra_blast_hits sql CREATE TABLE aux sra_blast_hits iteration INTEGER seq_name TEXT seq_end TEXT shard TEXT cxn execute sql sql CREATE INDEX aux sra_blast_hits_index ON sra_blast_hits iteration seq_name seq_end cxn execute sql\\n',\n",
       " 'def insert_blast_hit_batch cxn batch if batch sql INSERT INTO aux sra_blast_hits iteration seq_end seq_name shard VALUES cxn executemany sql batch cxn commit\\n',\n",
       " 'def sra_blast_hits_count cxn iteration sql SELECT COUNT AS count FROM aux sra_blast_hits WHERE iteration result cxn execute sql iteration return result fetchone 0\\n',\n",
       " 'def get_sra_blast_hits cxn iteration sql SELECT seq_name seq_end seq FROM sequences WHERE seq_name IN SELECT DISTINCT seq_name FROM aux sra_blast_hits WHERE iteration ORDER BY seq_name seq_end cxn row_factory sqlite3 Row return cxn execute sql iteration\\n',\n",
       " 'def get_blast_hits_by_end_count cxn iteration end_count sql SELECT seq_name seq_end seq FROM sequences WHERE seq_name IN SELECT seq_name FROM sequences WHERE seq_name IN SELECT DISTINCT seq_name FROM aux sra_blast_hits WHERE iteration GROUP BY seq_name HAVING COUNT ORDER BY seq_name seq_end cxn row_factory sqlite3 Row return cxn execute sql iteration end_count\\n',\n",
       " 'def create_contig_blast_hits_table cxn cxn execute DROP TABLE IF EXISTS aux contig_blast_hits sql CREATE TABLE aux contig_blast_hits iteration INTEGER contig_id TEXT description TEXT bit_score NUMERIC len INTEGER query_from INTEGER query_to INTEGER query_strand TEXT hit_from INTEGER hit_to INTEGER hit_strand TEXT cxn execute sql sql CREATE INDEX aux contig_blast_hits_index ON contig_blast_hits iteration bit_score len cxn execute sql\\n',\n",
       " 'def insert_contig_hit_batch cxn batch if batch sql INSERT INTO aux contig_blast_hits iteration contig_id description bit_score len query_from query_to query_strand hit_from hit_to hit_strand VALUES cxn executemany sql batch cxn commit\\n',\n",
       " 'def get_contig_blast_hits cxn iteration sql SELECT iteration contig_id description bit_score len query_from query_to query_strand hit_from hit_to hit_strand FROM aux contig_blast_hits WHERE iteration cxn row_factory sqlite3 Row return cxn execute sql iteration\\n',\n",
       " 'def create_assembled_contigs_table cxn cxn execute DROP TABLE IF EXISTS aux assembled_contigs sql CREATE TABLE aux assembled_contigs iteration INTEGER contig_id TEXT seq TEXT description TEXT bit_score NUMERIC len INTEGER query_from INTEGER query_to INTEGER query_strand TEXT hit_from INTEGER hit_to INTEGER hit_strand TEXT cxn execute sql sql CREATE INDEX aux assembled_contigs_index ON assembled_contigs iteration contig_id cxn execute sql\\n',\n",
       " 'def assembled_contigs_count cxn iteration bit_score length sql SELECT COUNT AS count FROM aux assembled_contigs WHERE iteration AND bit_score AND len result cxn execute sql iteration bit_score length return result fetchone 0\\n',\n",
       " 'def iteration_overlap_count cxn iteration bit_score length sql SELECT COUNT AS overlap FROM aux assembled_contigs AS curr_iter JOIN aux assembled_contigs AS prev_iter ON curr_iter contig_id prev_iter contig_id AND curr_iter iteration prev_iter iteration 1 WHERE curr_iter iteration AND curr_iter seq prev_iter seq AND curr_iter bit_score AND prev_iter bit_score AND curr_iter len result cxn execute sql iteration bit_score bit_score length return result fetchone 0\\n',\n",
       " 'def insert_assembled_contigs_batch cxn batch if batch sql INSERT INTO aux assembled_contigs iteration contig_id seq description bit_score len query_from query_to query_strand hit_from hit_to hit_strand VALUES cxn executemany sql batch cxn commit\\n',\n",
       " 'def get_assembled_contigs cxn iteration bit_score length sql SELECT contig_id seq FROM aux assembled_contigs WHERE iteration AND bit_score AND len return cxn execute sql iteration bit_score length\\n',\n",
       " 'def get_all_assembled_contigs cxn bit_score 0 length 0 sql SELECT iteration contig_id seq description bit_score len query_from query_to query_strand hit_from hit_to hit_strand FROM aux assembled_contigs WHERE bit_score AND len ORDER BY bit_score DESC iteration cxn row_factory sqlite3 Row return cxn execute sql bit_score length\\n',\n",
       " 'def all_assembled_contigs_count cxn bit_score 0 length 0 sql SELECT COUNT AS count FROM aux assembled_contigs WHERE bit_score AND len result cxn execute sql bit_score length return result fetchone 0\\n',\n",
       " 'def iter_dir temp_dir blast_db query_name iteration name _ _ 02d format basename blast_db basename query_name iteration return join temp_dir name\\n',\n",
       " 'def __init__ self args cxn super __init__ args cxn self steps self spades\\n',\n",
       " 'def work_path self return os path join self iter_dir spades\\n',\n",
       " 'def spades self cmd spades py only assembler threads format self args cpus memory format self args max_memory cov cutoff format self args cov_cutoff o format self work_path if self file paired_count cmd append pe1 1 format self file paired_1 cmd append pe1 2 format self file paired_2 if self file single_1_count cmd append s1 format self file single_1 if self file single_2_count cmd append s1 format self file single_2 if self file single_any_count cmd append s1 format self file single_any return join cmd\\n',\n",
       " 'def post_assembly self src os path join self work_path contigs fasta shutil move src self file output\\n',\n",
       " 'def __init__ self args cxn super __init__ args cxn self steps self trinity\\n',\n",
       " 'def work_path self return os path join self iter_dir trinity\\n',\n",
       " 'def trinity self cmd Trinity seqType fa max_memory G format self args max_memory CPU format self args cpus output format self work_path full_cleanup if not self args bowtie2 cmd append no_bowtie if self file paired_count cmd append left format self file paired_1 cmd append right format self file paired_2 else single_ends self get_single_ends if single_ends cmd append single format join single_ends if self file long_reads and not self args no_long_reads cmd append long_reads format self file long_reads return join cmd\\n',\n",
       " 'def post_assembly self src os path join self iter_dir trinity Trinity fasta shutil move src self file output\\n',\n",
       " 'def __init__ self args cxn super __init__ args cxn self steps self velveth self velvetg\\n',\n",
       " 'staticmethod def parse_contig_id header return header\\n',\n",
       " 'def velveth self cmd velveth format self work_path format self args kmer fasta if self file paired_count cmd append shortPaired format self file paired_1 self file paired_2 single_ends if self file single_1_count single_ends append format self file single_1 if self file single_2_count single_ends append format self file single_2 if self file single_any_count single_ends append format self file single_any if single_ends cmd append short format join single_ends if self file long_reads and not self args no_long_reads cmd append long format self file long_reads return join cmd\\n',\n",
       " 'def velvetg self cmd velvetg format self work_path ins_length format self args ins_length exp_cov format self args exp_coverage min_contig_lgth format self args min_contig_length return join cmd\\n',\n",
       " 'def post_assembly self src self iter_file contigs fa shutil move src self file output\\n',\n",
       " 'def __init__ self args cxn super __init__ args cxn self steps self abyss\\n',\n",
       " 'def abyss self cmd abyss pe C format self work_path E 0 k format self args kmer name format self file output if self args mpi cmd append np format self args cpus if self file paired_count cmd append in format self file paired_1 self file paired_2 single_ends self get_single_ends if single_ends cmd append se format join single_ends if self file long_reads and not self args no_long_reads cmd append long LONGREADS cmd append LONGREADS format self file long_reads return join cmd\\n',\n",
       " 'def post_assembly self src os path realpath self file output unitigs fa shutil copyfile src self file output\\n',\n",
       " 'def setup log_file blast_db query_file global LOGGER if not LOGGER log_file file_name log_file blast_db query_file handler logging FileHandler log_file handler setFormatter FORMATTER handler setLevel logging DEBUG stream logging StreamHandler stream setFormatter FORMATTER stream setLevel logging INFO LOGGER logging getLogger log_file LOGGER setLevel logging DEBUG LOGGER addHandler handler LOGGER addHandler stream info aTRAM version format db ATRAM_VERSION info join sys argv\\n',\n",
       " 'def file_name log_file blast_db query_file if log_file return log_file program splitext basename sys argv 0 0 if query_file query_file splitext basename query_file 0 return log format blast_db query_file program return log format blast_db program\\n',\n",
       " 'def subcommand cmd temp_dir timeout None LOGGER debug cmd with tempfile NamedTemporaryFile mode w dir temp_dir as log_output try subprocess check_call cmd shell True timeout timeout stdout log_output stderr log_output except subprocess CalledProcessError TimeoutError raise finally with open log_output name as log_input for line in log_input line line strip if line LOGGER debug line\\n',\n",
       " 'def info msg LOGGER info msg\\n',\n",
       " 'def error msg LOGGER error msg\\n',\n",
       " 'def fatal msg error msg sys exit 1\\n',\n",
       " 'patch atram write_query_seq def test_split_queries_none self write_query_seq self args query_split queries atram split_queries self args write_query_seq assert_not_called assert self args query queries\\n',\n",
       " 'patch atram write_query_seq def test_split_queries_some self write_query_seq self args query_split tests data split_queries1 txt queries atram split_queries self args split_files temp_dir_1 queries split_queries1_seq1_1_1 fasta temp_dir_1 queries split_queries1_seq2_2_2_2 fasta temp_dir_1 queries split_queries1_seq3_3 fasta temp_dir_1 queries split_queries1_seq1_1_4 fasta calls call split_files 0 seq1 1 A 10 call split_files 1 seq2 2 2 C 20 call split_files 2 seq3 G 30 call split_files 3 seq1 1 T 10 write_query_seq assert_has_calls calls assert split_files queries\\n',\n",
       " 'def preprocess args log setup args log_file args blast_db with db connect args blast_db clean True as cxn db create_metadata_table cxn db create_sequences_table cxn load_seqs args cxn log info Creating an index for the sequence table db create_sequences_index cxn shard_list assign_seqs_to_shards cxn args shard_count create_all_blast_shards args shard_list\\n',\n",
       " 'def load_seqs args cxn for arg clamp in mixed_ends end_1 1 end_2 2 single_ends if args get arg for file_name in args arg load_one_file cxn file_name arg clamp\\n',\n",
       " 'def load_one_file cxn file_name arg seq_end_clamp log info Loading into sqlite database format file_name is_fastq file_name lower endswith q parser FastqGeneralIterator if is_fastq else SimpleFastaParser with open file_name as sra_file batch for rec in parser sra_file title rec 0 strip seq rec 1 match blast PARSE_HEADER match title if match group 2 seq_name match group 1 if arg mixed_ends seq_end match group 2 else seq_end seq_end_clamp else seq_name title seq_end seq_end_clamp batch append seq_name seq_end seq if len batch db BATCH_SIZE db insert_sequences_batch cxn batch batch db insert_sequences_batch cxn batch\\n',\n",
       " 'def assign_seqs_to_shards cxn shard_count log info Assigning sequences to shards total db get_sequence_count cxn offsets np linspace 0 total 1 dtype int num shard_count 1 cuts db get_shard_cut cxn offset for offset in offsets cuts 1 cuts 1 z pairs cuts i 1 cuts i for i in range 1 len cuts return pairs\\n',\n",
       " 'def create_all_blast_shards args shard_list log info Making blast DBs with multiprocessing Pool processes args cpus as pool results for idx shard_params in enumerate shard_list 1 results append pool apply_async create_one_blast_shard args shard_params idx all_results result get for result in results log info Finished making blast all DBs format len all_results\\n',\n",
       " 'def create_one_blast_shard args shard_params shard_index shard 03d blast format args blast_db shard_index fasta_name _ 03d fasta format os path basename sys argv 0 3 shard_index fasta_path os path join args temp_dir fasta_name fill_blast_fasta args blast_db fasta_path shard_params blast create_db args temp_dir fasta_path shard\\n',\n",
       " 'def fill_blast_fasta blast_db fasta_path shard_params with db connect blast_db as cxn limit offset shard_params with open fasta_path w as fasta_file for row in db get_sequences_in_shard cxn limit offset seq_end format row 1 if row 1 else fasta_file write n format row 0 seq_end fasta_file write n format row 2\\n',\n",
       " 'def parse_command_line temp_dir_default description This script prepares data for use by the atram py script It takes fasta or fastq files of paired end or single end sequence reads and creates a set of atram databases You need to prepare the sequence read archive files so that the header lines contain only a sequence ID with the optional paired end suffix at the end of the header line The separator for the optional trailing paired end suffix may be a space a slash a dot or an underscore _ For example DBRHHJN1 427 H9YYAADXX 1 1101 10001 77019 1 GATTAA DBRHHJN1 427 H9YYAADXX 1 1101 10001 77019 2 ATAGCC DBRHHJN1 427 H9YYAADXX 1 1101 10006 63769 2 CGAAAA parser argparse ArgumentParser formatter_class argparse RawDescriptionHelpFormatter description textwrap dedent description parser add_argument version action version version prog s format db ATRAM_VERSION parser add_argument end 1 1 metavar FASTA_or_FASTQ nargs help Sequence read archive files that have only end 1 sequences The sequence names do not need an end suffix we will assume the suffix is always 1 The files are in fasta or fastq format You may enter more than one file or you may use wildcards parser add_argument end 2 2 metavar FASTA_or_FASTQ nargs help Sequence read archive files that have only end 2 sequences The sequence names do not need an end suffix we will assume the suffix is always 2 The files are in fasta or fastq format You may enter more than one file or you may use wildcards parser add_argument mixed ends m metavar FASTA_or_FASTQ nargs help Sequence read archive files that have a mix of both end 1 and end 2 sequences or single ends The files are in fasta or fastq format You may enter more than one file or you may use wildcards parser add_argument single ends 0 metavar FASTA_or_FASTQ nargs help Sequence read archive files that have only unpaired sequences Any sequence suffix will be ignored The files are in fasta or fastq format You may enter more than one file or you may use wildcards group parser add_argument_group preprocessor arguments blast_db os path join atram_ date today isoformat group add_argument b blast db output db default blast_db metavar DB help This is the prefix of all of the blast database files So you can identify different blast database sets You may include a directory as part of the prefix The default is format blast_db cpus min 10 os cpu_count 4 if os cpu_count 4 else 1 group add_argument cpus processes max processes type int default cpus help Number of CPU threads to use On this machine the default is format cpus group add_argument t temp dir metavar DIR help You may save intermediate files for debugging in this directory The directory must be empty group add_argument l log file help Log file full path The default is to use the DB and program name to come up with a name like DB _atram_preprocessor log group add_argument s shards number type int metavar SHARDS dest shard_count help Number of blast DB shards to create The default is to have each shard contain roughly 250MB of sequence data group add_argument path help If blast or makeblastdb is not in your PATH then use this to prepend directories to your path group add_argument sqlite temp dir metavar DIR help Use this directory to save temporary SQLITE3 files This is a possible fix for database or disk is full errors args vars parser parse_args if args path os environ PATH format args path os environ PATH if args sqlite_temp_dir os environ SQLITE_TMPDIR args sqlite_temp_dir elif args temp_dir os environ SQLITE_TMPDIR args temp_dir if not args temp_dir args temp_dir temp_dir_default else os makedirs args temp_dir exist_ok True all_files for arg in mixed_ends end_1 end_2 single_ends if args get arg all_files extend i for i in args arg args shard_count blast default_shard_count args shard_count all_files blast make_blast_output_dir args blast_db blast find_program makeblastdb return args\\n',\n",
       " 'def load_readline_favorites if not os path exists HISTORY_PATH return with open HISTORY_PATH as f for line in f ipython readline add_history line rstrip n\\n',\n",
       " 'def save_history print Saving plaintext history to s HISTORY_PATH lines record 2 n for record in ipython history_manager get_range with open HISTORY_PATH a as logfile logfile writelines lines ipython history_length\\n',\n",
       " 'def to_formula cnt C 12 O 8 N 1 C12 N O8 return join k str v if v 1 else for k v in sorted cnt items\\n',\n",
       " 'def get_monomer_cifs mon_path for root name in util sorted_cif_search mon_path if _ not in name yield os path join root name\\n',\n",
       " 'def check_formulas ccd for b in ccd atoms Counter a as_str 0 upper for a in b find _chem_comp_atom type_symbol formula cif as_string b find_value _chem_comp formula upper fdict util formula_to_dict formula if fdict atoms print s b name formula to_formula atoms\\n',\n",
       " 'def compare_monlib_with_ccd mon_path ccd PRINT_MISSING_ENTRIES False cnt 0 for path in get_monomer_cifs mon_path mon cif read path for mb in mon if mb name in comp_list continue assert mb name startswith comp_ name mb name 5 cb ccd find_block name if cb compare_chem_comp mb cb cnt 1 elif PRINT_MISSING_ENTRIES print Not in CCD name print Compared cnt monomers\\n',\n",
       " 'def gather_data writer csv writer sys stdout dialect excel tab writer writerow code na_chains vs vm d_min date group for path in util get_file_paths_from_args block cif read path sole_block code cif as_string block find_value _entry id na sum nucleotide in t 0 for t in block find _entity_poly type vs block find_value _exptl_crystal density_percent_sol vm block find_value _exptl_crystal density_Matthews d_min block find_value _refine ls_d_res_high dep_date_tag _pdbx_database_status recvd_initial_deposition_date dep_date parse_date block find_values dep_date_tag str 0 group block find_value _pdbx_deposit_group group_id writer writerow code na vs vm d_min dep_date group\\n',\n",
       " 'def get_file_paths_from_args parser argparse ArgumentParser usage prog s options path parser add_argument path nargs help argparse SUPPRESS parser add_argument only metavar LIST help Use only files that match names in this file args parser parse_args only None if args only with open args only as list_file only set line split 0 lower for line in list_file if line strip for arg in args path if os path isdir arg for root name extlen in sorted_cif_search arg if not only or name extlen lower in only yield os path join root name elif len arg 4 and arg isalnum pdb_dir os getenv PDB_DIR if not pdb_dir sys exit Error PDB_DIR not set where to look for arg yield os path join pdb_dir structures divided mmCIF arg 1 3 lower arg lower cif gz else yield arg\\n',\n",
       " 'def formula_to_dict formula fdict for elnum in formula split na sum e isalpha for e in elnum if na len elnum fdict elnum 1 elif na 0 fdict elnum na int elnum na return fdict\\n',\n",
       " 'def __init__ self host port bufsize 8192 self address host port self bufsize bufsize self sock socket socket socket AF_INET socket SOCK_DGRAM\\n',\n",
       " 'def send self msg append_null_terminator True if append_null_terminator msg msg x00 self sock sendto msg self address\\n',\n",
       " 'def recv self conform_address True data address self sock recvfrom self bufsize if conform_address self address address return data\\n',\n",
       " 'def enumerate_joint_ask X e P Q ProbDist X Y v for v in P variables if v X and v not in e for xi in P values X Q xi enumerate_joint Y extend e X xi P return Q normalize\\n',\n",
       " 'def enumerate_joint vars values P if not vars return P values Y vars 0 rest vars 1 return sum enumerate_joint rest extend values Y y P for y in P values Y\\n',\n",
       " 'def elimination_ask X e bn factors for var in reverse bn vars factors append Factor var e if is_hidden var X e factors sum_out var factors return pointwise_product factors normalize\\n',\n",
       " 'def __getitem__ self val return self prob val\\n',\n",
       " 'def __setitem__ self val p if val not in self values self values append val self prob val p\\n',\n",
       " 'def normalize self total sum self prob values if not 1 0 epsilon total 1 0 epsilon for val in self prob self prob val total return self\\n',\n",
       " 'def __getitem__ self values if isinstance values dict values tuple values var for var in self variables return self prob values\\n',\n",
       " 'def __setitem__ self values p if isinstance values dict values values var for var in self variables self prob values p for var val in zip self variables values if val not in self vals var self vals var append val\\n',\n",
       " 'def values self var return self vals var\\n',\n",
       " 'def T state action abstract\\n',\n",
       " 'def __init__ self distance direction self distance distance self direction direction\\n',\n",
       " 'def __init__ self distance direction flag_id self flag_id flag_id GameObject __init__ self distance direction\\n',\n",
       " 'def __init__ self distance direction dist_change dir_change speed self dist_change dist_change self dir_change dir_change self speed speed GameObject __init__ self distance direction\\n',\n",
       " 'def __init__ self distance direction dist_change dir_change speed team side uniform_number body_direction neck_direction self team team self side side self uniform_number uniform_number self body_direction body_direction self neck_direction neck_direction MobileObject __init__ self distance direction dist_change dir_change speed\\n',\n",
       " 'def connect self host port teamname version 11 if self __connected msg Cannot connect while already connected disconnect first raise sp_exceptions AgentConnectionStateError msg self __sock sock Socket host port self wm WorldModel handler ActionHandler self __sock self wm teamname teamname self msg_handler handler MessageHandler self wm self __parsing True self __msg_thread threading Thread target self __message_loop name message_loop self __msg_thread daemon True self __msg_thread start init_address self __sock address init_msg init s version d self __sock send init_msg teamname version while self __sock address init_address time sleep 0 0001 self __thinking False self __think_thread threading Thread target self __think_loop name think_loop self __think_thread daemon True self __connected True\\n',\n",
       " 'def play self if not self __connected msg Must be connected to a server to begin play raise sp_exceptions AgentConnectionStateError msg if self __thinking raise sp_exceptions AgentAlreadyPlayingError Agent is already playing self setup_environment self __thinking True self __should_think_on_data True self __think_thread start\\n',\n",
       " 'def disconnect self if not self __connected return self __parsing False self __thinking False self __sock send bye if self __msg_thread is_alive self __msg_thread join 0 01 if self __think_thread is_alive self __think_thread join 0 01 Agent __init__ self\\n',\n",
       " 'def __message_loop self while self __parsing raw_msg self __sock recv msg_type self msg_handler handle_message raw_msg if msg_type handler ActionHandler CommandType SENSE_BODY self __send_commands True self __should_think_on_data True\\n',\n",
       " 'def __think_loop self while self __thinking if self __send_commands self __send_commands False self wm ah send_commands if self __should_think_on_data self __should_think_on_data False self think else time sleep 0 0001\\n',\n",
       " 'def setup_environment self self in_kick_off_formation False\\n',\n",
       " 'def think self if not self __think_thread is_alive or not self __msg_thread is_alive raise Exception A thread died if not self in_kick_off_formation side_mod 1 if self wm side WorldModel SIDE_R side_mod 1 if self wm uniform_number 1 self wm teleport_to_point 5 side_mod 30 elif self wm uniform_number 2 self wm teleport_to_point 40 side_mod 15 elif self wm uniform_number 3 self wm teleport_to_point 40 side_mod 0 elif self wm uniform_number 4 self wm teleport_to_point 40 side_mod 15 elif self wm uniform_number 5 self wm teleport_to_point 5 side_mod 30 elif self wm uniform_number 6 self wm teleport_to_point 20 side_mod 20 elif self wm uniform_number 7 self wm teleport_to_point 20 side_mod 0 elif self wm uniform_number 8 self wm teleport_to_point 20 side_mod 20 elif self wm uniform_number 9 self wm teleport_to_point 10 side_mod 0 elif self wm uniform_number 10 self wm teleport_to_point 10 side_mod 20 elif self wm uniform_number 11 self wm teleport_to_point 10 side_mod 20 self in_kick_off_formation True return goal_pos None if self wm side WorldModel SIDE_R goal_pos 55 0 else goal_pos 55 0 if self wm is_before_kick_off if self wm uniform_number 9 if self wm is_ball_kickable self wm kick_to goal_pos 1 0 elif self wm ball is not None if self wm ball direction is not None and 7 self wm ball direction 7 self wm ah dash 50 else self wm turn_body_to_point 0 0 if self wm ball is not None self wm turn_neck_to_object self wm ball return else if self wm ball is None or self wm ball direction is None self wm ah turn 30 return if self wm is_ball_kickable self wm kick_to goal_pos 1 0 return else if 7 self wm ball direction 7 self wm ah dash 65 else self wm ah turn self wm ball direction 2 return\\n',\n",
       " 'def test_reliability self n 8 k 8 N 2 n transformation LTFArray transform_id combiner LTFArray combiner_xor instance LTFArray weight_array LTFArray normal_weights n n k k random_instance RandomState 41377 transform transformation combiner combiner challenges sample_inputs n N random_instance RandomState 1026842 reliabilities for challenge in challenges reliabilities append PropertyTest reliability instance reshape challenge 1 n assert_array_equal reliabilities repeat 0 0 N noisy_instance NoisyLTFArray weight_array NoisyLTFArray normal_weights n n k k random_instance RandomState 41377 transform transformation combiner combiner sigma_noise 15 0 random_instance RandomState 328030 for challenge in challenges reliability PropertyTest reliability noisy_instance reshape challenge 1 n self assertNotEqual reliability 0 0\\n',\n",
       " 'def test_reliability_set self n 8 k 3 N 2 n measurements 10 transformation LTFArray transform_id combiner LTFArray combiner_xor instances instance_count 3 for i in range instance_count instance LTFArray weight_array LTFArray normal_weights n n k k random_instance RandomState 41377 i transform transformation combiner combiner instances append instance challenges sample_inputs n N random_instance RandomState 64176 reliability_set PropertyTest reliability_set instances challenges measurements measurements self assertEqual len reliability_set N instance_count assert_array_equal reliability_set repeat 0 0 N instance_count noisy_instances for i in range instance_count noisy_instance NoisyLTFArray weight_array NoisyLTFArray normal_weights n n k k random_instance RandomState 41377 i transform transformation combiner combiner sigma_noise 0 5 random_instance RandomState 328028 i noisy_instances append noisy_instance noisy_reliability_set PropertyTest reliability_set noisy_instances challenges measurements measurements self assertNotEqual mean noisy_reliability_set 0 0\\n',\n",
       " 'def test_reliability_statistic self n 8 k 1 N 2 n instance_count 2 measurements 100 transformation LTFArray transform_id combiner LTFArray combiner_xor instances LTFArray weight_array LTFArray normal_weights n n k k random_instance RandomState 41377 i transform transformation combiner combiner for i in range instance_count challenges sample_inputs n N random_instance RandomState 1026832 property_test PropertyTest instances reliability_statistic property_test reliability_statistic challenges measurements measurements for key value in reliability_statistic items if key sv self assertEqual value 0 0 format key elif key samples self assertEqual len value instance_count N format key else self assertEqual value 0 0 format key noisy_instances NoisyLTFArray weight_array LTFArray normal_weights n n k k random_instance RandomState 41377 i transform transformation combiner combiner sigma_noise 0 5 random_instance RandomState 51902 for i in range instance_count noisy_property_test PropertyTest noisy_instances noisy_reliability_statistic noisy_property_test reliability_statistic challenges measurements measurements self assertNotEqual noisy_reliability_statistic mean 0 0\\n',\n",
       " 'def test_uniqueness self n 8 k 1 N 2 n instance_count 50 transformation LTFArray transform_id combiner LTFArray combiner_xor instances LTFArray weight_array LTFArray normal_weights n n k k random_instance RandomState 41377 i transform transformation combiner combiner for i in range instance_count challenges sample_inputs n N random_instance RandomState 1026832 uniqueness for challenge in challenges uniqueness append PropertyTest uniqueness instances reshape challenge 1 n self assertEqual round mean uniqueness 1 0 5\\n',\n",
       " 'def test_uniqueness_set self n 8 k 1 N 2 n instance_count 25 measurements 2 transformation LTFArray transform_id combiner LTFArray combiner_xor instances LTFArray weight_array LTFArray normal_weights n n k k random_instance RandomState 41377 i transform transformation combiner combiner for i in range instance_count challenges sample_inputs n N random_instance RandomState 1026832 uniqueness_set PropertyTest uniqueness_set instances challenges measurements measurements self assertEqual len uniqueness_set N measurements self assertEqual round mean uniqueness_set 1 0 5\\n',\n",
       " 'def test_uniqueness_statistic self n 8 k 1 N 2 n instance_count 11 measurements 1 transformation LTFArray transform_id combiner LTFArray combiner_xor instances LTFArray weight_array LTFArray normal_weights n n k k random_instance RandomState 41377 i transform transformation combiner combiner for i in range instance_count challenges sample_inputs n N random_instance RandomState 1026832 property_test PropertyTest instances uniqueness_statistic property_test uniqueness_statistic challenges measurements measurements self assertEqual round uniqueness_statistic mean 1 0 5\\n',\n",
       " 'mute def test_default self example main\\n',\n",
       " 'property def training_set self raise NotImplementedError users must define training_set to use this base class\\n',\n",
       " 'training_set setter abc abstractmethod def training_set self val raise NotImplementedError users must define training_set to use this base class\\n',\n",
       " 'abc abstractmethod def learn self raise NotImplementedError users must define learn to use this base class\\n',\n",
       " 'def stability_figure_data n k vote_count sigma_noise_ratio num reps random sigma_noise NoisyLTFArray sigma_noise_from_random_weights n 1 sigma_noise_ratio weights LTFArray normal_weights n k random_instance random instance_mv SimulationMajorityLTFArray weights LTFArray transform_atf LTFArray combiner_xor sigma_noise random_instance_noise random vote_count vote_count stabilities tools approx_stabilities instance_mv num reps random print join map str stabilities\\n',\n",
       " 'def test_8_1_puf self log_name LOG_PATH test_8_1_puf overall_desired_stability 0 8 mv_num_of_votes main 0 95 str overall_desired_stability 8 1 1 0 33 250 1 log_name log_name log_file open log_name log r line log_file readline self assertNotEqual line no stability where found which satisfy overall_desired_stability stability float line split t 6 self assertGreaterEqual stability overall_desired_stability\\n',\n",
       " 'def __init__ self log_name test_function challenge_count measurements challenge_seed ins_gen_function param_ins_gen super __init__ log_name log_name self log_name log_name self test_function test_function self challenge_count challenge_count self challenge_seed challenge_seed self measurements measurements self ins_gen_function ins_gen_function self param_ins_gen param_ins_gen self result None\\n',\n",
       " 'def run self instances self ins_gen_function self param_ins_gen n self param_ins_gen n challenge_prng RandomState self challenge_seed challenges array list sample_inputs n self challenge_count random_instance challenge_prng property_test PropertyTest instances logger self progress_logger self result self test_function property_test challenges measurements self measurements\\n',\n",
       " 'def analyze self assert self result is not None mean self result get mean float inf median self result get median float inf minimum self result get min float inf maximum self result get max float inf sample_variance self result get sv float inf samples self result get samples string_samples list map str samples samples_string join string_samples instance_param for value in self param_ins_gen values if callable value instance_param append value __name__ else instance_param append str value instance_param_str t join instance_param unique_id format join instance_param self challenge_count self measurements self challenge_seed msg t t t t f t f t f t f t f t f t t format instance_param_str self challenge_count self measurements self challenge_seed mean median minimum maximum sample_variance self measured_time unique_id samples_string self result_logger info msg\\n',\n",
       " 'classmethod def create_ltf_arrays cls n 8 k 1 instance_count 10 transformation LTFArray transform_id combiner LTFArray combiner_xor bias None mu 0 sigma 1 weight_random_seed 291 instances for seed_offset in range instance_count weight_array LTFArray normal_weights n k mu sigma random_instance RandomState weight_random_seed seed_offset instances append LTFArray weight_array weight_array transform transformation combiner combiner bias bias return instances\\n',\n",
       " 'classmethod def create_noisy_ltf_arrays cls n 8 k 1 instance_count 10 transformation LTFArray transform_id combiner LTFArray combiner_xor bias None mu 0 sigma 1 weight_random_seed 291 sigma_noise 0 5 noise_random_seed 801 instances for seed_offset in range instance_count weight_array LTFArray normal_weights n k mu sigma random_instance RandomState weight_random_seed seed_offset instances append NoisyLTFArray weight_array weight_array transform transformation combiner combiner sigma_noise sigma_noise random_instance RandomState noise_random_seed seed_offset bias bias return instances\\n',\n",
       " 'classmethod def create_mv_ltf_arrays cls n 8 k 1 instance_count 10 transformation LTFArray transform_id combiner LTFArray combiner_xor bias None mu 0 sigma 1 weight_random_seed 291 sigma_noise 0 5 noise_random_seed 801 vote_count 3 instances for seed_offset in range instance_count weight_array LTFArray normal_weights n k mu sigma random_instance RandomState weight_random_seed seed_offset instances append SimulationMajorityLTFArray weight_array weight_array transform transformation combiner combiner sigma_noise sigma_noise random_instance_noise RandomState noise_random_seed seed_offset bias bias vote_count vote_count return instances\\n',\n",
       " 'def test_learn_xor self instance_prng RandomState seed TestLogisticRegression seed_instance model_prng RandomState seed TestLogisticRegression seed_model instance LTFArray weight_array LTFArray normal_weights TestLogisticRegression n TestLogisticRegression k random_instance instance_prng transform LTFArray transform_id combiner LTFArray combiner_xor lr_learner LogisticRegression TrainingSet instance instance N TestLogisticRegression N TestLogisticRegression n TestLogisticRegression k transformation LTFArray transform_id combiner LTFArray combiner_xor weights_prng model_prng lr_learner learn\\n',\n",
       " 'def test_learn_ip_mod2 self instance_prng RandomState seed TestLogisticRegression seed_instance model_prng RandomState seed TestLogisticRegression seed_model instance LTFArray weight_array LTFArray normal_weights TestLogisticRegression n TestLogisticRegression k random_instance instance_prng transform LTFArray transform_id combiner LTFArray combiner_ip_mod2 lr_learner LogisticRegression TrainingSet instance instance N TestLogisticRegression N TestLogisticRegression n TestLogisticRegression k transformation LTFArray transform_id combiner LTFArray combiner_xor weights_prng model_prng lr_learner learn\\n',\n",
       " 'def __init__ self training_set degree debug False self training_set training_set self n len training_set challenges 0 self monomial_count 0 for k in range degree 1 self monomial_count ncr self n k self degree degree self fourier_coefficients self debug debug\\n',\n",
       " 'staticmethod def get_training_set_size n degree epsilon delta monomial_count 0 for k in range degree 1 monomial_count ncr n k return int 4 monomial_count np log 2 monomial_count delta epsilon\\n',\n",
       " 'def learn self processed 0 last 0 start_time time time for i in range self degree 1 for chi in self low_degree_chi i self fourier_coefficients append self approx_fourier_coefficient chi if not self debug continue processed 1 current int processed self monomial_count 100 if current last current_time time time time_diff current_time start_time duration_left np round 100 current time_diff current last current print r s percent complete Estimated time left s s current duration_left end if self debug print return FourierExpansionSign self fourier_coefficients\\n',\n",
       " 'def approx_fourier_coefficient self subset return FourierCoefficient subset tools approx_fourier_coefficient subset self training_set\\n',\n",
       " 'def low_degree_chi self degree for indices in combinations range self n degree yield np array 1 if i in indices else 0 for i in range self n dtype tools RESULT_TYPE\\n',\n",
       " 'def __init__ self log_name n k challenge_count seed_instance seed_instance_noise transformation combiner mu sigma sigma_noise_ratio seed_challenges desired_stability overall_desired_stability minimum_vote_count iterations bias None super __init__ log_name s 0x x_0_ i_ i_ i_ s_ s log_name seed_instance n k challenge_count transformation __name__ combiner __name__ self n n self k k self N challenge_count self seed_instance seed_instance self seed_instance_noise seed_instance_noise self seed_challenges seed_challenges self transformation transformation self combiner combiner self mu mu self sigma sigma self sigma_noise NoisyLTFArray sigma_noise_from_random_weights n 1 sigma_noise_ratio self bias bias self desired_stability desired_stability self overall_desired_stability overall_desired_stability self minimum_vote_count minimum_vote_count self maximum_vote_count 0 self vote_count 0 self result_overall_stab 0 0 self first_result_overall_stab 0 0 self result_vote_count 0 self iterations iterations self overall_stab 0 0\\n',\n",
       " 'def run self instance_prng RandomState self seed_instance noise_prng RandomState self seed_instance_noise challenge_prng RandomState self seed_challenges weight_array LTFArray normal_weights self n self k self mu self sigma random_instance instance_prng if isinstance self bias type list self bias np reshape np array self bias self k 1 self vote_count 0 while self overall_stab self overall_desired_stability self vote_count self vote_count 2 1 puf_instance SimulationMajorityLTFArray weight_array LTFArray transform_id LTFArray combiner_xor self sigma_noise random_instance_noise noise_prng vote_count self vote_count bias self bias self calculate_stabilities puf_instance challenge_prng self maximum_vote_count self vote_count self first_result_overall_stab self overall_stab self result_vote_count self vote_count while self minimum_vote_count self maximum_vote_count self vote_count self minimum_vote_count self maximum_vote_count 2 if self vote_count 2 0 self vote_count self vote_count 1 puf_instance SimulationMajorityLTFArray weight_array LTFArray transform_id LTFArray combiner_xor self sigma_noise random_instance_noise noise_prng vote_count self vote_count self calculate_stabilities puf_instance challenge_prng msg f t i t self overall_stab self vote_count if self overall_stab self overall_desired_stability self maximum_vote_count self vote_count 1 self result_vote_count self vote_count self result_overall_stab self overall_stab else self minimum_vote_count self vote_count 1 if self result_overall_stab 0 0 self result_overall_stab self first_result_overall_stab self progress_logger info msg\\n',\n",
       " 'def analyze self msg 0x x t0x x t i t i t i t i t f t f t self seed_instance self seed_challenges self n self k self N self result_vote_count self result_overall_stab self measured_time self result_logger info msg\\n',\n",
       " 'def calculate_stabilities self instance challenge_prng challenges tools random_inputs self n self N random_instance challenge_prng eval_array np zeros len challenges dtype tools RESULT_TYPE for i in range self iterations eval_array eval_array instance eval challenges stab_array np abs eval_array self iterations 2 self iterations num_goal_fulfilled 0 for i in range self N if stab_array i self desired_stability num_goal_fulfilled 1 self overall_stab num_goal_fulfilled self N\\n',\n",
       " 'def main args parser argparse ArgumentParser usage Experiment to determine the minimum number of votes required to achieve a desired given stability parser add_argument stab_c help Desired stability of the challenges type float choices 0 5 0 55 0 6 0 65 0 7 0 75 0 8 0 85 0 9 0 95 parser add_argument stab_all help Overall desired stability type float choices 0 5 0 55 0 6 0 65 0 7 0 75 0 8 0 85 0 9 0 95 parser add_argument n help Number of bits per Arbiter chain type int choices 8 16 24 32 48 64 128 parser add_argument k_max help Maximum number of Arbiter chains type int parser add_argument k_range help Number of step size between the number of Arbiter chains type int choices range 1 33 parser add_argument s_ratio help Ratio of standard deviation of the noise and weights type float parser add_argument N help Number of challenges to evaluate type int parser add_argument restarts help Number of restarts to the entire process type int parser add_argument log_name help Path to the main log file type str default my_num_of_votes args parser parse_args args if args k_max 0 stderr write Negative maximum number of Arbiter chains quit 1 seed_challenges 61440 iterations 10 n args n N args N experiments for i in range args restarts for k in range args k_range args k_max 1 args k_range log_name args log_name 0 format k exp ExperimentMajorityVoteFindVotes log_name log_name n n k k challenge_count N seed_instance 3235822174 i seed_instance_noise 3735928559 i transformation LTFArray transform_id combiner LTFArray combiner_xor mu 0 sigma 1 sigma_noise_ratio args s_ratio seed_challenges seed_challenges i desired_stability args stab_c overall_desired_stability args stab_all minimum_vote_count 1 iterations iterations bias None experiments append exp experimenter Experimenter args log_name experiments experimenter run\\n',\n",
       " 'def main instance LTFArray weight_array LTFArray normal_weights n 64 k 2 transform LTFArray transform_atf combiner LTFArray combiner_xor lr_learner LogisticRegression t_set tools TrainingSet instance instance N 12000 n 64 k 2 transformation LTFArray transform_atf combiner LTFArray combiner_xor model lr_learner learn accuracy 1 tools approx_dist instance model 10000 print Learned a 64bit 2 xor XOR Arbiter PUF from 12000 CRPs with accuracy f accuracy\\n',\n",
       " 'def __init__ self t_set n k transformation LTFArray transform_id combiner LTFArray combiner_xor weights_mu 0 weights_sigma 1 weights_prng RandomState logger None self iteration_count 0 self training_set t_set self n n self k k self weights_mu weights_mu self weights_sigma weights_sigma self weights_prng weights_prng self iteration_limit 10000 self convergence_decimals 2 self sign_combined_model_responses None self sigmoid_derivative full self training_set N None dtype float64 self transformation transformation self combiner combiner self transformed_challenges self transformation self training_set challenges k self converged False self logger logger assert self n len self training_set challenges 0\\n',\n",
       " 'property def training_set self return self __training_set\\n',\n",
       " 'training_set setter def training_set self val self __training_set val\\n',\n",
       " 'def gradient self model model_responses model ltf_eval self transformed_challenges combined_model_responses self combiner model_responses self sign_combined_model_responses sign combined_model_responses max_response_abs_value 50 max_response_abs_value_array full len combined_model_responses max_response_abs_value dtype float64 combined_model_responses sign combined_model_responses minimum max_response_abs_value_array np_abs combined_model_responses self sigmoid_derivative 0 5 2 1 exp combined_model_responses 1 self training_set responses def model_gradient_xor l Caculates the gradient of the xored response at index l param l int Index for weight array and challange array return array of float return combined_model_responses model_responses l def model_gradient_ip_mod2 l Caculates the gradient of the ip_mod2 combined responses at index l param l int Index for weight array and challange array return array of float if l 2 0 neighbor model_responses l 1 else neighbor model_responses l 1 maximum amax model_responses l neighbor 0 return array 0 if maximum i neighbor i else combined_model_responses i maximum i for i in range self training_set N if compare_functions self combiner LTFArray combiner_xor model_gradient model_gradient_xor elif compare_functions self combiner LTFArray combiner_ip_mod2 model_gradient model_gradient_ip_mod2 else raise Exception No gradient function known for combiner s self combiner ret array dot self sigmoid_derivative model_gradient l self transformed_challenges l for l in range self k return ret\\n',\n",
       " 'def learn self def log_state This method is used to log a snapshot of learning variables while running if self logger is None return self logger debug i t f t f t s self iteration_count distance norm updater step join map str model weight_array flatten seterr all raise model LTFArray weight_array LTFArray normal_weights self n self k self weights_mu self weights_sigma self weights_prng transform self transformation combiner self combiner updater self RPropModelUpdate model converged False distance 1 self iteration_count 0 log_state while not converged and self iteration_count self iteration_limit self iteration_count 1 gradient self gradient model model weight_array updater update gradient converged norm updater step 10 self convergence_decimals log_state if not converged self converged False else self converged True return model\\n',\n",
       " 'def setup_result_logger queue logger_name handler logging handlers QueueHandler queue root logging getLogger logger_name root addHandler handler root setLevel logging DEBUG return root\\n',\n",
       " 'def __init__ self log_name self progress_logger logging getLogger log_name self log_name log_name self progress_logger setLevel logging DEBUG self result_logger None self measured_time None\\n',\n",
       " 'abc abstractmethod def analyze self raise NotImplementedError users must define analysis to use this base class\\n',\n",
       " 'abc abstractmethod def run self raise NotImplementedError users must define run to use this base class\\n',\n",
       " 'def execute self queue logger_name self result_logger setup_result_logger queue logger_name file_handler logging FileHandler s log self log_name mode w file_handler setLevel logging DEBUG self progress_logger addHandler file_handler start_time time time self run self measured_time time time start_time self analyze\\n',\n",
       " 'def setup_logger logger_name root logging getLogger logger_name file_handler logging FileHandler filename s log logger_name mode w file_handler setLevel logging INFO root addHandler file_handler\\n',\n",
       " 'def log_listener queue configurer logger_name configurer logger_name while True try record queue get if record is None break logger logging getLogger record name logger handle record except Exception print Whoops Problem file sys stderr traceback print_exc file sys stderr\\n',\n",
       " 'def __init__ self log_name experiments cpu_limit 2 16 self experiments experiments self logger_name log_name self cpu_limit min cpu_limit multiprocessing cpu_count self semaphore multiprocessing BoundedSemaphore self cpu_limit\\n',\n",
       " 'def run self queue multiprocessing Queue 1 listener multiprocessing Process target log_listener args queue setup_logger self logger_name listener start active_jobs for exp in self experiments def run_experiment experiment queue semaphore logger_name This method is responsible to start the experiment and release the semaphore which coordinates the number of parallel running processes param experiment pypuf experiments experiment base Experiment A implementation of the base experiment class which should be executed param queue multiprocessing queue This is used to coordinate the processes in order to obtain results param semaphore multiprocessing BoundedSemaphore self cpu_limit A semaphore to limit the number of concurrent running experiments param logger_name String Path to or name to the log file of the experiment try experiment execute queue logger_name semaphore release except Exception as experiment semaphore release raise experiment job multiprocessing Process target run_experiment args exp queue self semaphore self logger_name self semaphore acquire def list_active_jobs update list of active jobs return multiprocessing Process list of active jobs still_active_jobs for j in active_jobs j join 0 if j exitcode is None still_active_jobs append j return still_active_jobs job start active_jobs list_active_jobs active_jobs append job for job in active_jobs job join queue put_nowait None listener join\\n',\n",
       " 'def __init__ self instances logger None self instances instances self logger logger\\n',\n",
       " 'staticmethod def statistic float_set float_set_size len float_set factor 1 float_set_size sample_mean factor np_sum float_set factor 1 float_set_size 1 sample_variance sqrt factor np_sum float_set sample_mean 2 minimum np_min float_set maximum np_max float_set median_dist median float_set return mean sample_mean median median_dist min minimum max maximum sv sample_variance samples float_set\\n',\n",
       " 'staticmethod def reliability instance challenge measurements 10 responses array instance eval challenge for _ in range measurements real_response sign np_sum responses axis 0 response_distances responses real_response return mean response_distances\\n',\n",
       " 'staticmethod def reliability_set instances challenges measurements 10 reliabilities n len challenges 0 for ins in instances for challenge in challenges shaped_challenge reshape challenge 1 n reliabilities append PropertyTest reliability ins shaped_challenge measurements measurements return reliabilities\\n',\n",
       " 'def reliability_statistic self challenges measurements 10 return PropertyTest statistic PropertyTest reliability_set self instances challenges measurements measurements\\n',\n",
       " 'staticmethod def uniqueness instances challenge responses array instance eval challenge 0 for instance in instances m len instances distance_sum 0 for u in range m 1 for v in range u 1 m distance_sum distance_sum responses u responses v return 2 m m 1 distance_sum\\n',\n",
       " 'staticmethod def uniqueness_set instances challenges measurements 1 n len challenges 0 uniqueness_set for challenge in challenges for _ in range measurements uniqueness_set append PropertyTest uniqueness instances reshape challenge 1 n return uniqueness_set\\n',\n",
       " 'def uniqueness_statistic self challenges measurements 10 return PropertyTest statistic PropertyTest uniqueness_set self instances challenges measurements measurements\\n',\n",
       " 'def main arguments parser ArgumentParser usage This tool can be used to check the python code in a directory on style violations The scripts default path is the directory in which this script is located in parser add_argument p path help a path to the directory where the python code should be analyzed default path os default type str dest path parser add_argument l max line length help maximal number of characters for each line default is 120 default 120 type int dest max_line_length parser add_argument e exclude_patterns help patterns to identify directories or files which should not be checked default are env env default env env type str dest exclude_patterns nargs args parser parse_args arguments max_line_length max line length 0 format args max_line_length excludes exclude 0 format join args exclude_patterns pep8_returncode call executable m pep8 max_line_length excludes args path files_to_check for root _ file_names in walk args path if reduce lambda x y x or y pattern in root for pattern in args exclude_patterns continue for filename in fn_filter file_names py if not reduce lambda x y x or y pattern in filename for pattern in args exclude_patterns files_to_check append path join root filename if not files_to_check files_to_check args path pylint_cmd executable m pylint max_line_length disable R pylint_cmd extend files_to_check pylint_returncode call pylint_cmd returncode 0 if pep8_returncode 0 or pylint_returncode 0 returncode 1 exit returncode\\n',\n",
       " 'def __init__ self s val self s s self val val\\n',\n",
       " 'def __init__ self fourier_coefficients self fourier_coefficients fourier_coefficients self n len fourier_coefficients 0 s\\n',\n",
       " 'def eval self inputs vals np array coefficient val tools chi_vectorized coefficient s inputs for coefficient in self fourier_coefficients T return np sum vals axis 1\\n',\n",
       " 'def eval self inputs return np sign super FourierExpansionSign self eval inputs\\n',\n",
       " 'def val self inputs return super FourierExpansionSign self eval inputs\\n',\n",
       " 'def main args if len args 10 or len args 11 sys stderr write LTF Array Simulator and Logistic Regression Learner n sys stderr write Usage n sys stderr write sim_learn py n k transformation combiner N restarts seed_instance seed_model log_name sys stderr write n number of bits per Arbiter chain n sys stderr write k number of Arbiter chains n sys stderr write transformation used to transform input before it is used in LTFs n sys stderr write currently available n sys stderr write id does nothing at all n sys stderr write atf convert according to natural Arbiter chain sys stderr write implementation n sys stderr write mm designed to achieve maximum PTF expansion length sys stderr write only implemented for k 2 and even n n sys stderr write lightweight_secure design by Majzoobi et al 2008 sys stderr write only implemented for even n n sys stderr write shift_lightweight_secure design like Majzoobi n sys stderr write et al 2008 but with the shift sys stderr write operation executed first sys stderr write only implemented for even n sys stderr write soelter_lightweight_secure design like Majzoobi n sys stderr write et al 2008 but one bit different sys stderr write only implemented for even n sys stderr write 1_n_bent one LTF gets bent input the others id sys stderr write 1_1_bent one bit gets bent input the others id sys stderr write this is proven to have maximum PTF n sys stderr write length for the model n sys stderr write polynomial challenges are interpreted as polynomials sys stderr write from GF 2 64 From the initial challenge c sys stderr write the i th Arbiter chain gets the coefficients sys stderr write of the polynomial c i 1 as challenge sys stderr write For now only challenges with length n 64 are accepted sys stderr write permutation_atf for each Arbiter chain first a pseudorandom permutation sys stderr write is applied and thereafter the ATF transform sys stderr write random Each Arbiter chain gets a random challenge derived from the sys stderr write original challenge using a PRNG n sys stderr write combiner used to combine the output bits to a single bit n sys stderr write currently available n sys stderr write xor output the parity of all output bits n sys stderr write ip_mod2 output the inner product mod 2 of all output sys stderr write bits even n only n sys stderr write N number of challenge response pairs in the training set sys stderr write restarts number of repeated initializations the learner n sys stderr write instances number of repeated initializations the instance n sys stderr write The number total learning attempts is restarts instances sys stderr write seed_instance random seed used for LTF array instance n sys stderr write seed_model random seed used for the model in first learning attempt sys stderr write log_name path to the logfile which contains results from all instances The tool will add a log to log_name The default path is sim_learn log quit 1 n int args 1 k int args 2 transformation_name args 3 combiner_name args 4 N int args 5 restarts int args 6 instances int args 7 seed_instance int args 8 16 seed_model int args 9 16 transformation None combiner None try transformation getattr LTFArray transform_ s transformation_name except AttributeError sys stderr write Transformation s unknown or currently not implemented n transformation_name quit try combiner getattr LTFArray combiner_ s combiner_name except AttributeError sys stderr write Combiner s unknown or currently not implemented n combiner_name quit log_name sim_learn if len args 11 log_name args 10 sys stderr write Learning s bit s XOR Arbiter PUF with s CRPs and s restarts n n n k N restarts sys stderr write Using n sys stderr write transformation s n transformation sys stderr write combiner s n combiner sys stderr write instance random seed 0x x n seed_instance sys stderr write model random seed 0x x n seed_model sys stderr write n experiments for j in range instances for start_number in range restarts l_name s_ i_ i log_name j start_number experiment ExperimentLogisticRegression log_name l_name n n k k N N seed_instance seed_instance j seed_model seed_model j start_number transformation transformation combiner combiner experiments append experiment experimenter Experimenter log_name experiments experimenter run str_format 15 t 10 t 8 t 8 t 8 t 8 t 18 t 15 t 6 t 8 t 8 t 8 headline str_format format seed_instance seed_model i n k N trans comb iter time accuracy model_values n sys stderr write headline log_file open log_name log r result log_file readline while result sys stderr write str_format format result split t result log_file readline log_file close\\n',\n",
       " 'def main example_reliability example_reliability_statistic\\n',\n",
       " 'def example_reliability n 8 k 8 transformation NoisyLTFArray transform_id combiner NoisyLTFArray combiner_xor weights NoisyLTFArray normal_weights n n k k instance NoisyLTFArray weight_array weights transform transformation combiner combiner sigma_noise NoisyLTFArray sigma_noise_from_random_weights n 0 5 challenge array 1 1 1 1 1 1 1 1 reliability PropertyTest reliability instance reshape challenge 1 n print The reliability is format reliability\\n',\n",
       " 'def example_reliability_statistic n 8 k 1 N 2 n instance_count 3 measurements 100 transformation NoisyLTFArray transform_id combiner NoisyLTFArray combiner_xor weights NoisyLTFArray normal_weights n n k k instances NoisyLTFArray weight_array weights transform transformation combiner combiner sigma_noise NoisyLTFArray sigma_noise_from_random_weights n 0 5 for _ in range instance_count challenges array list sample_inputs n N property_test PropertyTest instances reliability_statistic property_test reliability_statistic challenges measurements measurements print The reliability statistic is format reliability_statistic\\n',\n",
       " 'def example_uniqueness n 8 k 1 instance_count 3 transformation NoisyLTFArray transform_id combiner NoisyLTFArray combiner_xor weights NoisyLTFArray normal_weights n n k k instances NoisyLTFArray weight_array weights transform transformation combiner combiner sigma_noise NoisyLTFArray sigma_noise_from_random_weights n weights for _ in range instance_count challenge array 1 1 1 1 1 1 1 1 uniqueness PropertyTest uniqueness instances reshape challenge 1 n print The uniqueness is format uniqueness\\n',\n",
       " 'def example_uniqueness_statistic n 8 k 1 N 2 n instance_count 11 measurements 1 transformation NoisyLTFArray transform_id combiner NoisyLTFArray combiner_xor weights NoisyLTFArray normal_weights n n k k instances NoisyLTFArray weight_array weights transform transformation combiner combiner sigma_noise NoisyLTFArray sigma_noise_from_random_weights n weights for _ in range instance_count challenges array list sample_inputs n N property_test PropertyTest instances uniqueness_statistic property_test uniqueness_statistic challenges measurements measurements print The uniqueness statistic is format uniqueness_statistic\\n',\n",
       " 'abc abstractmethod def eval self inputs raise NotImplementedError users must define learn to use this base class\\n',\n",
       " 'def test_proper_dimension self arr zeros dtype dtype int64 item 30 with self assertRaisesRegex AssertionError arr must have at least one dimension append_last arr item\\n',\n",
       " 'def test_append_result self typ dtype int64 arr_check zeros 3 2 2 dtype typ arr zeros 3 2 1 dtype typ item 0 arr_res append_last arr item self assertTrue array_equal arr_check arr_res The arrays should be equal\\n',\n",
       " 'def test_append_dimensions self typ dtype int64 item 0 for dimensions in range 1 11 shape i for i in range dimensions shape_res i for i in range dimensions shape_res 1 shape_res 1 1 arr_check zeros shape_res dtype typ arr zeros shape dtype typ arr_res append_last arr item self assertTrue array_equal arr_res arr_check The arrays should be equal\\n',\n",
       " 'def test_training_set_challenges self n 8 k 1 transformation LTFArray transform_id combiner LTFArray combiner_xor N 1000 instance_prng RandomState 323562 weight_array LTFArray normal_weights n k random_instance instance_prng instance LTFArray weight_array weight_array transform transformation combiner combiner challenge_seed 700797 training_set_1 TrainingSet instance instance N N random_instance RandomState challenge_seed training_set_2 TrainingSet instance instance N N random_instance RandomState challenge_seed self assertTrue array_equal training_set_1 challenges training_set_2 challenges The challenges are not equal\\n',\n",
       " 'def test_random_input self n 8 rand_arr random_input n random_instance RandomState 368923 self assertEqual len rand_arr n Array must be of length n self assertEqual rand_arr dtype dtype RESULT_TYPE Array must be of type 0 format RESULT_TYPE array_sum sum rand_arr self assertNotEqual array_sum n All values are 1 self assertNotEqual array_sum 0 All values are 0 self assertGreater array_sum 0 The array should contain positive values only\\n',\n",
       " 'def test_all_inputs self n 8 N 2 n arr all_inputs n self check_multi_dimensional_array arr N n RESULT_TYPE\\n',\n",
       " 'def test_random_inputs self n 8 N 2 int n 2 rand_arrays random_inputs n N self check_multi_dimensional_array rand_arrays N n RESULT_TYPE\\n',\n",
       " 'def test_sample_inputs self n 8 N 2 n all_arr sample_inputs n N self check_multi_dimensional_array all_arr N n RESULT_TYPE N 2 int n 2 rand_arr sample_inputs n N self check_multi_dimensional_array rand_arr N n RESULT_TYPE\\n',\n",
       " 'def test_transform_challenge_01_to_11 self challenge_01 array 0 1 0 1 1 0 0 0 dtype RESULT_TYPE challenge_11 array 1 1 1 1 1 1 1 1 dtype RESULT_TYPE transformed_challenge transform_challenge_01_to_11 challenge_01 assert_array_equal challenge_11 transformed_challenge self assertEqual transformed_challenge dtype dtype RESULT_TYPE The array is not of type 0 format RESULT_TYPE\\n',\n",
       " 'def test_transform_challenge_11_to_01 self challenge_11 array 1 1 1 1 1 1 1 1 dtype RESULT_TYPE challenge_01 array 1 0 0 0 1 0 1 0 dtype RESULT_TYPE transformed_challenge transform_challenge_11_to_01 challenge_11 assert_array_equal challenge_01 transformed_challenge self assertEqual transformed_challenge dtype dtype RESULT_TYPE The array is not of type 0 format RESULT_TYPE\\n',\n",
       " 'def test_chi_vectorized self n 8 N 2 int n 2 s random_input n inputs random_inputs n N chi_arr chi_vectorized s inputs self assertEqual len chi_arr N The array must contain 0 arrays format N self assertEqual chi_arr dtype RESULT_TYPE The array must be of type 0 format RESULT_TYPE\\n',\n",
       " 'def test_poly_mult_div self n 8 k 2 N 2 int n 2 challenges_11 random_inputs n N challenges_01 array transform_challenge_11_to_01 c for c in challenges_11 dtype RESULT_TYPE irreducible_polynomial array 1 0 1 0 0 1 1 0 1 dtype RESULT_TYPE poly_mult_div challenges_01 irreducible_polynomial k self check_multi_dimensional_array challenges_01 N n RESULT_TYPE\\n',\n",
       " 'def check_multi_dimensional_array self arr arr_size sub_arr_size arr_type self assertEqual len arr arr_size The array must contain 0 arrays format arr_size for i in range arr_size self assertEqual len arr i sub_arr_size The sub array does not match the length of 0 format sub_arr_size self assertEqual arr dtype arr_type The array must be of type 0 format arr_type\\n',\n",
       " 'logging def test_run_and_analyze self logger lr16_4 ExperimentLogisticRegression LOG_PATH exp1 8 2 2 8 48879 48879 LTFArray transform_id LTFArray combiner_xor lr16_4 execute logger queue logger logger_name\\n',\n",
       " 'logging def test_fix_result self logger n 8 k 2 N 8 seed_instance 12248414 seed_model 96906782 seed_challenge 10 seed_distance 11 combiners get_functions_with_prefix combiner_ LTFArray transformations get_functions_with_prefix transform_ LTFArray def check_experiments experiment_1 experiment_2 This function compares the results of two experiments param experiment_1 ExperimentLogisticRegression param experiment_2 ExperimentLogisticRegression experiment_1 execute logger queue logger logger_name experiment_2 execute logger queue logger logger_name exp_1_result_log open experiment_1 log_name log r exp_2_result_log open experiment_2 log_name log r result_1 exp_1_result_log read result_2 exp_2_result_log read exp_1_result_log close exp_2_result_log close self assertFalse result_1 The experiment 0 log was empty format experiment_1 log_name self assertFalse result_2 The experiment log 0 was empty format experiment_2 log_name self assertTrue result_1 result_2 The results of 0 and 1 must be equal format experiment_1 log_name experiment_2 log_name def get_exp name k trans comb Experiment creation shortcut param name string Name of the experiment return ExperimentLogisticRegression LOG_PATH name n k N seed_instance seed_model trans comb seed_challenge seed_challenge seed_chl_distance seed_distance for transformation in transformations for combiner in combiners experiment_1 get_exp exp1 k transformation combiner experiment_2 get_exp exp2 k transformation combiner check_experiments experiment_1 experiment_2\\n',\n",
       " 'logging def test_fix_convergence self logger n 8 k 2 N 255 seed_instance 12248414 seed_model 96906782 seed_challenge 11560478 seed_distance 2828 experiment ExperimentLogisticRegression LOG_PATH exp n k N seed_instance seed_model LTFArray transform_soelter_lightweight_secure LTFArray combiner_xor seed_challenge seed_challenge seed_chl_distance seed_distance experiment execute logger queue logger logger_name legacy_result 0xbae55e 0x5c6ae1e 0 8 2 255 transform_soelter_lightweight_secure combiner_xor 363 1 000000 0 00443419669755 0 00616546911566 0 0186346081194 0 0061619719475 0 00795284461334 0 00443539877583 0 00316047872599 0 00993214368373 0 0507595729459 0 415207373134 0 0517173737839 0 285900582842 0 467512016377 0 550102231366 0 000739711610042 0 467757977178 result_str logger read_result_log self assertFalse result_str The result log was empty experiment_result result_str split t del experiment_result 9 self assertTrue experiment_result legacy_result You changed the code significant\\n',\n",
       " 'logging def test_run_and_analyze self logger n 8 experiment ExperimentMajorityVoteFindVotes log_name logger logger_name n n k 2 challenge_count 2 8 seed_instance 3235822174 seed_instance_noise 3735928559 transformation LTFArray transform_id combiner LTFArray combiner_xor mu 0 sigma 1 sigma_noise_ratio NoisyLTFArray sigma_noise_from_random_weights n 1 0 5 seed_challenges 61440 desired_stability 0 95 overall_desired_stability 0 8 minimum_vote_count 1 iterations 2 bias None experiment execute logger queue logger logger_name self assertGreaterEqual experiment result_overall_stab experiment overall_desired_stability No vote_count was found\\n',\n",
       " 'logging def test_run_and_analyze_bias_list self logger n 8 experiment ExperimentMajorityVoteFindVotes log_name logger logger_name n n k 2 challenge_count 2 8 seed_instance 3235822174 seed_instance_noise 3735928559 transformation LTFArray transform_id combiner LTFArray combiner_xor mu 0 sigma 1 sigma_noise_ratio NoisyLTFArray sigma_noise_from_random_weights n 1 0 5 seed_challenges 61440 desired_stability 0 95 overall_desired_stability 0 8 minimum_vote_count 1 iterations 2 bias 0 001 0 002 experiment execute logger queue logger logger_name self assertGreaterEqual experiment result_overall_stab experiment overall_desired_stability No vote_count was found\\n',\n",
       " 'logging def test_run_and_analyze_bias_value self logger n 8 experiment ExperimentMajorityVoteFindVotes log_name logger logger_name n n k 2 challenge_count 2 8 seed_instance 3235822174 seed_instance_noise 3735928559 transformation LTFArray transform_id combiner LTFArray combiner_xor mu 0 sigma 1 sigma_noise_ratio NoisyLTFArray sigma_noise_from_random_weights n 1 0 5 seed_challenges 61440 desired_stability 0 95 overall_desired_stability 0 8 minimum_vote_count 1 iterations 2 bias 0 56 experiment execute logger queue logger logger_name self assertGreaterEqual experiment result_overall_stab experiment overall_desired_stability No vote_count was found\\n',\n",
       " 'logging def test_run_and_analyze_tests self logger def create_experiment N test_function ins_gen_function param_ins_gen A shortcut function to create property experiments measurements 10 challenge_seed 3557 return ExperimentPropertyTest log_name logger logger_name test_function test_function challenge_count N measurements measurements challenge_seed challenge_seed ins_gen_function ins_gen_function param_ins_gen param_ins_gen tests PropertyTest reliability_statistic PropertyTest uniqueness_statistic N 255 array_parameter n 16 k 1 instance_count 10 transformation NoisyLTFArray transform_id combiner NoisyLTFArray combiner_xor bias None mu 0 sigma 1 weight_random_seed 62702 sigma_noise 0 5 noise_random_seed 61007 for test_function in tests exp_rel create_experiment N test_function ExperimentPropertyTest create_noisy_ltf_arrays array_parameter exp_rel execute logger queue logger logger_name with open exp_rel log_name log r as log_file self assertNotEqual log_file read\\n',\n",
       " 'def random_input n random_instance RandomState return random_instance choice 1 1 n astype RESULT_TYPE\\n',\n",
       " 'def all_inputs n return array list itertools product 1 1 repeat n astype RESULT_TYPE\\n',\n",
       " 'def random_inputs n num random_instance RandomState res zeros num n dtype RESULT_TYPE for i in range num res i random_input n random_instance random_instance return res\\n',\n",
       " 'def sample_inputs n num random_instance RandomState return random_inputs n num random_instance if num 2 n else all_inputs n\\n',\n",
       " 'def append_last arr item dimension list shape arr assert len dimension 1 arr must have at least one dimension dimension 1 1 item_arr full dimension item axis len dimension 1 return append arr item_arr axis axis\\n',\n",
       " 'def approx_dist instance1 instance2 num random_instance RandomState assert instance1 n instance2 n inputs random_inputs instance1 n num random_instance random_instance return num count_nonzero instance1 eval inputs instance2 eval inputs num\\n',\n",
       " 'def approx_fourier_coefficient s training_set assert_result_type s assert_result_type training_set challenges return mean training_set responses chi_vectorized s training_set challenges\\n',\n",
       " 'def chi_vectorized s inputs assert_result_type s assert_result_type inputs assert len s len inputs 0 result inputs s 0 if result size 0 return ones len inputs dtype RESULT_TYPE return prod result axis 1 dtype RESULT_TYPE\\n',\n",
       " 'def compare_functions function1 function2 function1_code function1 __code__ function2_code function2 __code__ functions_equal function1_code co_code function2_code co_code functions_equal function1_code co_name function2_code co_name return functions_equal and function1_code co_filename function2_code co_filename\\n',\n",
       " 'def transform_challenge_01_to_11 challenge assert_result_type challenge res copy challenge res res 1 1 res res 0 1 return res\\n',\n",
       " 'def transform_challenge_11_to_01 challenge assert_result_type challenge res copy challenge res res 1 0 res res 1 1 return res\\n',\n",
       " 'def poly_mult_div challenge irreducible_polynomial k import polymath as pm assert_result_type challenge assert_result_type irreducible_polynomial c_original challenge res None for i in range k challenge pm polymul challenge c_original challenge pm polymodpad challenge irreducible_polynomial if i 0 res array challenge dtype RESULT_TYPE else res vstack res challenge res res astype RESULT_TYPE assert_result_type res return res\\n',\n",
       " 'def approx_stabilities instance num reps random_instance RandomState challenges sample_inputs instance n num random_instance responses zeros reps num for i in range reps responses i instance eval challenges return 0 5 0 5 np_abs np_sum responses axis 0 reps\\n',\n",
       " 'def assert_result_type arr assert arr dtype dtype RESULT_TYPE Must be an array of 0 Got array of 1 format RESULT_TYPE arr dtype\\n',\n",
       " 'def __init__ self instance N random_instance RandomState self instance instance self challenges sample_inputs instance n N random_instance random_instance self responses instance eval self challenges self N N\\n',\n",
       " 'def logging function logger TestLogger log_name function __name__ def _logging args kwargs This function starts and shutdowns the multiprocessing logger param args extra positional generic arguments param kwargs extra keyword arguments generic argument return return value of function logger start_multiprocessing_logger kwargs logger logger try result function args kwargs except Exception as exception raise exception finally logger shutdown_multiprocessing_logger return result return wraps function _logging\\n',\n",
       " 'def remove_test_logs log_dir_path LOG_PATH paths list glob glob log_dir_path log for path in paths os remove path\\n',\n",
       " 'def get_functions_with_prefix prefix obj return getattr obj func for func in dir obj if func startswith prefix\\n',\n",
       " 'def mute function def _mute args kwargs This method absorbs stderr and stdout streams param args extra positional generic arguments param kwargs extra keyword arguments generic argument return return value of function output_sink StringIO try from contextlib import redirect_stdout redirect_stderr redirect_out redirect_stdout output_sink redirect_err redirect_stderr output_sink except ImportError redirect_out RedirectStdout output_sink redirect_err RedirectStderr output_sink try with redirect_err redirect_out result function args kwargs except Exception as exception raise exception return result return wraps function _mute\\n',\n",
       " 'def start_multiprocessing_logger self assert self listener is None Logger is running assert self queue is None Queue should be None You may use the class in a wrong way self queue multiprocessing Queue 1 self listener multiprocessing Process target log_listener args self queue setup_logger self logger_name self listener start\\n',\n",
       " 'def shutdown_multiprocessing_logger self if self listener exitcode is None self queue put_nowait None self listener join\\n',\n",
       " 'def read_result_log self self shutdown_multiprocessing_logger result_log open self logger_name log r result result_log read result_log close return result\\n',\n",
       " 'def test_lr_experiments self lr16_4_1 ExperimentLogisticRegression LOG_PATH test_lr_experiments1 8 2 2 8 48879 48879 LTFArray transform_id LTFArray combiner_xor lr16_4_2 ExperimentLogisticRegression LOG_PATH test_lr_experiments2 8 2 2 8 48879 48879 LTFArray transform_id LTFArray combiner_xor lr16_4_3 ExperimentLogisticRegression LOG_PATH test_lr_experiments3 8 2 2 8 48879 48879 LTFArray transform_id LTFArray combiner_xor lr16_4_4 ExperimentLogisticRegression LOG_PATH test_lr_experiments4 8 2 2 8 48879 48879 LTFArray transform_id LTFArray combiner_xor lr16_4_5 ExperimentLogisticRegression LOG_PATH test_lr_experiments5 8 2 2 8 48879 48879 LTFArray transform_id LTFArray combiner_xor experiments lr16_4_1 lr16_4_2 lr16_4_3 lr16_4_4 lr16_4_5 experimenter Experimenter LOG_PATH test_lr_experiments experiments experimenter run\\n',\n",
       " 'def test_mv_experiments self experiments for i in range 5 n 8 logger_name LOG_PATH test_mv_exp 0 format i experiment ExperimentMajorityVoteFindVotes log_name logger_name n n k 2 challenge_count 2 8 seed_instance 3235822174 seed_instance_noise 3735928559 transformation LTFArray transform_id combiner LTFArray combiner_xor mu 0 sigma 1 sigma_noise_ratio NoisyLTFArray sigma_noise_from_random_weights n 1 0 5 seed_challenges 61440 i desired_stability 0 95 overall_desired_stability 0 8 minimum_vote_count 1 iterations 2 bias None experiments append experiment experimenter Experimenter LOG_PATH test_mv_experiments experiments experimenter run\\n',\n",
       " 'def test_multiprocessing_logs self experiments n 28 for i in range n log_name LOG_PATH test_multiprocessing_logs 0 format i lr16_4_1 ExperimentLogisticRegression log_name 8 2 2 8 48879 48879 LTFArray transform_id LTFArray combiner_xor experiments append lr16_4_1 for i in range n log_name LOG_PATH test_multiprocessing_logs 0 format i experiment ExperimentMajorityVoteFindVotes log_name log_name n 8 k 2 challenge_count 2 8 seed_instance 3235822174 seed_instance_noise 3735928559 transformation LTFArray transform_id combiner LTFArray combiner_xor mu 0 sigma 1 sigma_noise_ratio NoisyLTFArray sigma_noise_from_random_weights n 1 0 5 seed_challenges 61440 i desired_stability 0 95 overall_desired_stability 0 8 minimum_vote_count 1 iterations 2 bias None experiments append experiment experimenter_log_name LOG_PATH test_multiprocessing_logs experimenter Experimenter experimenter_log_name experiments experimenter run def line_count file_object param file_object return number of lines count 0 while file_object readline count count 1 return count paths list glob glob LOG_PATH log for log_path in paths exp_log_file open log_path r self assertGreater line_count exp_log_file 0 The experiment log is empty exp_log_file close log_file open experimenter_log_name log r self assertEqual line_count log_file n 2 Unexpected number of results log_file close\\n',\n",
       " 'def test_file_handle self class ExperimentDummy Experiment This is an empty experiment class which can be used to run a huge amount of experiments with an experimenter def run self pass def analyze self pass experiments n 1024 for i in range n log_name LOG_PATH fail 0 format i experiments append ExperimentDummy log_name experimenter Experimenter LOG_PATH test_file_handle experiments experimenter run\\n',\n",
       " 'mute def test_id self sim_learn main sim_learn 8 2 id xor 20 1 2 1234 1234 LOG_PATH test_id\\n',\n",
       " 'mute def test_atf self sim_learn main sim_learn 8 2 atf xor 20 1 2 1234 1234 LOG_PATH test_atf\\n',\n",
       " 'mute def test_mm self sim_learn main sim_learn 8 2 mm xor 20 1 2 1234 1234 LOG_PATH test_mm\\n',\n",
       " 'mute def test_lightweight_secure self sim_learn main sim_learn 8 2 lightweight_secure xor 20 1 2 1234 1234 LOG_PATH test_lightweight_secure\\n',\n",
       " 'mute def test_lightweight_secure_original self sim_learn main sim_learn 8 2 lightweight_secure_original xor 20 1 2 1234 1234 LOG_PATH test_lightweight_secure_original\\n',\n",
       " 'mute def test_1_n_bent self sim_learn main sim_learn 8 2 1_n_bent xor 20 1 2 1234 1234 LOG_PATH test_1_n_bent\\n',\n",
       " 'mute def test_1_1_bent self sim_learn main sim_learn 8 2 1_1_bent xor 20 1 2 1234 1234 LOG_PATH test_1_1_bent\\n',\n",
       " 'mute def test_ip_mod2_id self sim_learn main sim_learn 8 2 id ip_mod2 20 1 2 1234 1234 LOG_PATH test_ip_mod2_id\\n',\n",
       " 'mute def test_ip_mod2_atf self sim_learn main sim_learn 8 2 atf ip_mod2 20 1 2 1234 1234 LOG_PATH test_ip_mod2_atf\\n',\n",
       " 'mute def test_ip_mod2_mm self sim_learn main sim_learn 8 2 mm ip_mod2 20 1 2 1234 1234 LOG_PATH test_ip_mod2_mm\\n',\n",
       " 'mute def test_ip_mod2_lightweight_secure self sim_learn main sim_learn 8 2 lightweight_secure ip_mod2 20 1 2 1234 1234 LOG_PATH test_ip_mod2_lightweight_secure\\n',\n",
       " 'mute def test_ip_mod2_1_n_bent self sim_learn main sim_learn 8 2 1_n_bent ip_mod2 20 1 2 1234 1234 LOG_PATH test_ip_mod2_1_n_bent\\n',\n",
       " 'mute def test_ip_mod2_1_1_bent self sim_learn main sim_learn 8 2 1_1_bent ip_mod2 20 1 2 1234 1234 LOG_PATH test_ip_mod2_1_1_bent\\n',\n",
       " 'mute def test_permutation_atf self sim_learn main sim_learn 8 2 permutation_atf xor 10 1 2 1234 1234 LOG_PATH test_permutation_atf\\n',\n",
       " 'mute def test_log_name self instance_count 2 log_name LOG_PATH test_log_name sim_learn main sim_learn 8 2 id xor 10 1 str instance_count 1234 1234 log_name def line_count file_object param file_object return number of lines count 0 while file_object readline count count 1 return count log_file open log_name log r self assertEqual line_count log_file instance_count Unexpected number of results log_file close\\n',\n",
       " 'mute def test_number_of_results self instance_count 7 restarts 13 expected_number_of_result instance_count restarts log_name LOG_PATH test_number_of_results sim_learn main sim_learn 8 2 id xor 10 str restarts str instance_count 1234 1234 log_name def line_count file_object param file_object return number of lines count 0 while file_object readline count count 1 return count log_file open log_name log r self assertEqual line_count log_file expected_number_of_result Unexpected number of results log_file close\\n',\n",
       " 'def test_learn_xor self instance_prng RandomState seed TestLowDegree seed_instance instance LTFArray weight_array LTFArray normal_weights TestLowDegree n TestLowDegree k random_instance instance_prng transform LTFArray transform_id combiner LTFArray combiner_xor low_degree_learner LowDegreeAlgorithm TrainingSet instance instance N TestLowDegree N degree TestLowDegree degree low_degree_learner learn\\n',\n",
       " 'def __init__ self log_name n k N seed_instance seed_model transformation combiner seed_challenge 370001 seed_chl_distance 45141 super __init__ log_name s 0x x_0x x_0_ i_ i_ i_ s_ s log_name seed_model seed_instance n k N transformation __name__ combiner __name__ self n n self k k self N N self seed_instance seed_instance self instance_prng RandomState seed self seed_instance self seed_model seed_model self model_prng RandomState seed self seed_model self combiner combiner self transformation transformation self seed_challenge seed_challenge self challenge_prng RandomState self seed_challenge self seed_chl_distance seed_chl_distance self distance_prng RandomState self seed_chl_distance self instance None self learner None self model None\\n',\n",
       " 'def run self self instance LTFArray weight_array LTFArray normal_weights self n self k random_instance self instance_prng transform self transformation combiner self combiner self learner LogisticRegression tools TrainingSet instance self instance N self N random_instance self challenge_prng self n self k transformation self transformation combiner self combiner weights_prng self model_prng logger self progress_logger self model self learner learn\\n',\n",
       " 'def analyze self assert self model is not None self result_logger info 0x x t0x x t i t i t i t i t s t s t i t f t f t s self seed_instance self seed_model 0 self n self k self N self transformation __name__ self combiner __name__ self learner iteration_count self measured_time 1 0 tools approx_dist self instance self model min 10000 2 self n random_instance self distance_prng join map str self model weight_array flatten norm self model weight_array flatten\\n',\n",
       " 'def _build_connect_string self opts o UserKnownHostsFile dev null o StrictHostKeyChecking no o PasswordAuthentication no opts p d self _port if python in self config python self config python else python python return ssh s s s python s opts self _user self _host python\\n',\n",
       " 'def __getattr__ self name def call args kwargs result self _execute name args kwargs return result return call\\n',\n",
       " 'def test_jwt_create inmanta_config jot protocol encode_token api payload protocol decode_token jot assert api in payload urn inmanta ct jot1 protocol encode_token agent idempotent True jot3 protocol encode_token agent time sleep 1 jot2 protocol encode_token agent idempotent True jot4 protocol encode_token agent assert jot1 jot2 assert jot3 jot4 assert jot1 jot3 assert jot2 jot3\\n',\n",
       " 'def is_open self version int bool return version in self counterforVersion\\n',\n",
       " 'def open_version self version int if version in self counterforVersion self counterforVersion version 1 else LOGGER debug Cache open version d version self counterforVersion version 1 self keysforVersion version set\\n',\n",
       " 'def close_version self version int if version not in self counterforVersion raise Exception Closed version that does not exist self counterforVersion version 1 if self counterforVersion version 0 return LOGGER debug Cache close version d version for x in self keysforVersion version try item self cache x item delete del self cache x except KeyError pass del self counterforVersion version del self keysforVersion version\\n',\n",
       " 'def cache_value self key value resource None version 0 timeout 5000 call_on_delete None key key if resource is not None key append str resource id resource_str if version 0 key append str version key __ join key self _cache CacheItem key Scope timeout version value call_on_delete\\n',\n",
       " 'def find self key resource None version 0 key key if resource is not None key append resource id resource_str if version 0 key append str version key __ join key return self _get key value\\n',\n",
       " 'def get_or_else self key function for_version True timeout 5000 ignore set cache_none True call_on_delete None kwargs acceptable set resource if for_version acceptable add version args k v for k v in kwargs items if k in acceptable and k not in ignore others sorted k for k in kwargs keys if k not in acceptable and k not in ignore for k in others key s s k repr kwargs k key try return self find key args except KeyError with self addLock if key in self addLocks lock self addLocks key else lock Lock self addLocks key lock with lock try value self find key args except KeyError value function kwargs if cache_none or value is not None self cache_value key value timeout timeout call_on_delete call_on_delete args with self addLock del self addLocks key return value\\n',\n",
       " 'def t_ID t t type reserved get t value ID if t value 0 isupper t type CID lexer t lexer end lexer lexpos lexer linestart 1 s e lexer lexmatch span start end e s t value LocatableString t value Range lexer inmfile lexer lineno start lexer lineno end lexer lexpos lexer namespace return t\\n',\n",
       " 'def t_SEP t return t\\n',\n",
       " 'def t_CMP_OP t return t\\n',\n",
       " 'def t_PEQ t return t\\n',\n",
       " 'def t_begin_mls t t lexer begin mls t type MLS lexer t lexer end lexer lexpos lexer linestart 1 s e lexer lexmatch span start end e s t value LocatableString Range lexer inmfile lexer lineno start lexer lineno end lexer lexpos lexer namespace return t\\n',\n",
       " 'def t_FLOAT t t value float t value return t\\n',\n",
       " 'def t_INT t t value int t value return t\\n',\n",
       " 'def t_STRING_EMPTY t t type STRING t value return t\\n',\n",
       " 'def t_STRING t t value bytes t value 1 1 utf 8 decode unicode_escape lexer t lexer end lexer lexpos lexer linestart 1 s e lexer lexmatch span start end e s t value LocatableString t value Range lexer inmfile lexer lineno start lexer lineno end lexer lexpos lexer namespace return t\\n',\n",
       " 'def to_dict self return file self file lnr self lnr\\n',\n",
       " 'def to_dict self return type self type multi self multi nullable self nullable comment self comment location self location to_dict\\n',\n",
       " 'def to_dict self return value self value\\n',\n",
       " 'def to_dict self return reference self reference\\n',\n",
       " 'def to_dict self return type self type multi self multi 0 self multi 1 reverse self reverse comment self comment location self location to_dict source_annotations x to_dict for x in self source_annotations target_annotations x to_dict for x in self target_annotations\\n',\n",
       " 'def to_dict self return parents self parents attributes n a to_dict for n a in self attributes items relations n r to_dict for n r in self relations items location self location to_dict\\n',\n",
       " 'def compiler_config parser parser add_argument e dest environment help The environment to compile this model for parser add_argument X extended errors dest errors help Show stack traces for compile errors action store_true default False parser add_argument server_address dest server help The address of the server hosting the environment parser add_argument server_port dest port help The port of the server hosting the environment parser add_argument username dest user help The username of the server parser add_argument password dest password help The password of the server parser add_argument ssl help Enable SSL action store_true default False parser add_argument ssl ca cert dest ca_cert help Certificate authority for SSL parser add_argument f dest main_file help Main file default main cf\\n',\n",
       " 'def export_parser_config parser parser add_argument g dest depgraph help Dump the dependency graph action store_true parser add_argument j dest json help Do not submit to the server but only store the json that would have been submitted in the supplied file parser add_argument e dest environment help The environment to compile this model for parser add_argument d dest deploy help Trigger a deploy for the exported version action store_true default False parser add_argument m dest model help Also export the complete model action store_true default False parser add_argument server_address dest server help The address of the server to submit the model to parser add_argument server_port dest port help The port of the server to submit the model to parser add_argument token dest token help The token to auth to the server parser add_argument ssl help Enable SSL action store_true default False parser add_argument ssl ca cert dest ca_cert help Certificate authority for SSL parser add_argument X extended errors dest errors help Show stack traces for compile errors action store_true default False parser add_argument f dest main_file help Main file default main cf parser add_argument metadata dest metadata help JSON metadata why this compile happened If a non json string is passed it is used as the message attribute in the metadata default None\\n',\n",
       " 'def app normalformatter logging Formatter fmt levelname 8s message s formatter colorlog ColoredFormatter log_color s levelname 8s reset s blue s message s datefmt None reset True log_colors DEBUG cyan INFO green WARNING yellow ERROR red CRITICAL red stream logging StreamHandler stream setLevel logging INFO if hasattr sys stdout isatty and sys stdout isatty stream setFormatter formatter else stream setFormatter normalformatter logging root handlers logging root addHandler stream logging root setLevel 0 Config load_config parser cmd_parser options other parser parse_known_args options other other if options timed if hasattr sys stdout isatty and sys stdout isatty formatter colorlog ColoredFormatter asctime s log_color s levelname 8s reset s blue s message s datefmt None reset True log_colors DEBUG cyan INFO green WARNING yellow ERROR red CRITICAL red else formatter logging Formatter fmt asctime s levelname 8s message s stream setFormatter formatter level options verbose if level len log_levels level 3 stream setLevel log_levels level if options log_file level options log_file_level if level len log_levels level 3 formatter logging Formatter fmt asctime s levelname 8s name 10s message s file_handler logging FileHandler filename options log_file mode w file_handler setFormatter formatter file_handler setLevel log_levels level logging root addHandler file_handler Config load_config options config_file if not hasattr options func parser print_usage return options func options\\n',\n",
       " 'def copy_location self statement Locatable None statement location self location\\n',\n",
       " 'def requires self List str raise Exception Not Implemented str type self\\n',\n",
       " 'def emit self resolver Resolver queue QueueScheduler None raise Exception Not Implemented str type self\\n',\n",
       " 'def requires_emit self resolver Resolver queue QueueScheduler Dict object ResultVariable raise Exception Not Implemented str type self\\n',\n",
       " 'def execute self requires Dict object ResultVariable resolver Resolver queue QueueScheduler object raise Exception Not Implemented str type self\\n',\n",
       " 'def parse_agent_uri uri str str dict parts urllib parse urlparse uri config scheme local if parts query items urllib parse parse_qs parts query for key values in items items config key values 0 if parts scheme scheme parts scheme if parts netloc match re search P user s P host s P port d parts netloc if match is None raise ValueError config update match groupdict if parts path and parts scheme and parts netloc if parts path localhost scheme local else scheme ssh config update host parts path port None user None return scheme config\\n',\n",
       " 'def _get_io_class scheme local IOBase if scheme local return local LocalIO elif scheme ssh return remote SshIO\\n',\n",
       " 'def get_io cache AgentCache uri str version int if cache is None io _get_io_instance uri else io cache get_or_else uri lambda version _get_io_instance uri call_on_delete lambda x x close version version return io\\n',\n",
       " 'def _type self return self _get_instance type\\n',\n",
       " 'def is_unknown self if isinstance self _get_instance Unknown return True return False\\n',\n",
       " 'def default_fact_renew return int server_fact_expire get 3\\n',\n",
       " 'def validate_fact_renew value out int value if not out server_fact_expire get LOGGER warn can not set fact_renew to d must be smaller than fact expire d using d instead out server_fact_expire get default_fact_renew out default_fact_renew return out\\n',\n",
       " 'def default_hangtime return str int agent_timeout get 3 4\\n',\n",
       " 'def test_unknown_in_id_requires snippetcompiler caplog snippetcompiler setup_for_snippet import exp import tests a exp Test name tests unknown agent aa b exp Test name b agent aa requires a c exp Test name c agent aa requires b config Config set unknown_handler default prune resource _version json_value snippetcompiler do_export assert len json_value 2 assert_count 0 for resource_id resource in json_value items if resource_id attribute_value test_value_b assert len resource requires 0 assert_count 1 elif resource_id attribute_value test_value_c assert len resource requires 1 assert_count 1 warning x for x in caplog records if x msg The resource s had requirements before flattening but not after flattening Initial set was s Perhaps provides relation is not wired through correctly assert len warning 0 assert assert_count 2\\n',\n",
       " 'def test_unknown_in_attribute_requires snippetcompiler caplog snippetcompiler setup_for_snippet import exp import tests a exp Test name a agent aa field1 tests unknown b exp Test name b agent aa requires a c exp Test name c agent aa requires b config Config set unknown_handler default prune resource _version json_value status model snippetcompiler do_export include_status True assert len json_value 3 assert len x for x in status values if x const ResourceState available 2 assert len x for x in status values if x const ResourceState undefined 1 warning x for x in caplog records if x msg The resource s had requirements before flattening but not after flattening Initial set was s Perhaps provides relation is not wired through correctly assert len warning 0\\n',\n",
       " 'def create_function expression def function args kwargs A function that evaluates the expression if len args 1 raise NotImplementedError return expression execute_direct self args 0 return function\\n',\n",
       " 'classmethod def validate cls value raise NotImplementedError\\n',\n",
       " 'classmethod def cast cls value raise NotImplementedError\\n',\n",
       " 'def type_string self raise NotImplemented\\n',\n",
       " 'def __str__ self raise NotImplemented\\n',\n",
       " 'def get_double_defined_exception self other NamedType DuplicateException raise NotImplementedError\\n',\n",
       " 'def cast self value return self basetype cast value\\n',\n",
       " 'def validate self value if isinstance value NoneValue return True return self basetype validate value\\n',\n",
       " 'classmethod def validate cls value if isinstance value AnyType return True if not isinstance value numbers Number raise RuntimeException None Invalid value s expected Number value return True\\n',\n",
       " 'classmethod def cast cls value if value is None return value try fl_value float value try int_value int value except ValueError int_value 0 if fl_value int_value return int_value return fl_value except ValueError raise CastException\\n',\n",
       " 'classmethod def validate cls value if isinstance value AnyType return True if isinstance value bool return True else raise RuntimeException None Invalid value s expected Bool value\\n',\n",
       " 'classmethod def cast cls value if value true or value True or value 1 or value 1 or value is True return True if value false or value False or value 0 or value 0 or value is False return False raise CastException\\n',\n",
       " 'classmethod def cast cls value return str value\\n',\n",
       " 'classmethod def validate cls value if isinstance value AnyType return True if not isinstance value str raise RuntimeException None Invalid value s expected String value return True\\n',\n",
       " 'def cast self value return list self basetype cast x for x in value\\n',\n",
       " 'def validate self value if isinstance value AnyType return True if value is None return True if not isinstance value list raise RuntimeException None Invalid value s expected list value for x in value self basetype validate x return True\\n',\n",
       " 'classmethod def cast cls value return list value\\n',\n",
       " 'classmethod def validate cls value if value is None return True if isinstance value AnyType return True if not isinstance value list raise RuntimeException None Invalid value s expected list value return True\\n',\n",
       " 'classmethod def cast cls value return dict value\\n',\n",
       " 'classmethod def validate cls value if isinstance value AnyType return True if value is None return True if not isinstance value dict raise RuntimeException None Invalid value s expected dict value return True\\n',\n",
       " 'def set_constraint self expression self expression expression self _constraint create_function expression\\n',\n",
       " 'def get_constaint self return self _constraint\\n',\n",
       " 'def cast self value self __base_type cast value\\n',\n",
       " 'def validate self value if isinstance value AnyType return True self basetype validate value if not self _constraint value raise RuntimeException None Invalid value s constraint does not match value return True\\n',\n",
       " 'pytest mark gen_test timeout 10 def test_fork server io_loop i 0 while i 5 i 1 sub_process process Subprocess true yield sub_process wait_for_exit raise_error False sub_process uninitialize\\n',\n",
       " 'def setup_project self self _client protocol SyncClient client project_name cfg_prj get if project_name is None LOGGER error The name of the project should be configured for an all in one deploy return False environment_name cfg_env get if environment_name is None LOGGER error The name of the environment in the project should be configured for an all in one deploy return False tries 0 while tries MAX_TRIES try self _client list_projects break except Exception tries 1 projects self _client list_projects if projects code 200 LOGGER error Unable to retrieve project listing from the server return False project_id None for project in projects result projects if project_name project name project_id project id break if project_id is None project_id self _create_project project_name if not project_id return False environments self _client list_environments if environments code 200 LOGGER error Unable to retrieve environments from server return False for env in environments result environments if project_id env project and environment_name env name self _environment_id env id break if self _environment_id is None self _environment_id self _create_environment project_id environment_name if not self _environment_id return False return True\\n',\n",
       " 'def setup self no_agent_log True project module Project get self _data_path os path join project project_path data deploy LOGGER debug Storing state data in s self _data_path self _ensure_dir os path join project project_path data self _ensure_dir self _data_path if not self setup_mongodb return False if not self setup_server no_agent_log return False if not self setup_project LOGGER error Failed to setup project return False return True\\n',\n",
       " 'def export self inmanta_path sys executable m inmanta app cmd inmanta_path vvv export e str self _environment_id server_address localhost server_port str self _server_port sub_process subprocess Popen cmd stdout subprocess PIPE stderr subprocess PIPE log_out log_err sub_process communicate if sub_process returncode 0 print An error occurred while compiling the model if len log_out 0 print log_out decode if len log_err 0 print log_err decode return False LOGGER info Export of model complete return True\\n',\n",
       " 'def execute self requires typing Dict object ResultVariable resolver Resolver queue QueueScheduler object qlist List for i in range len self items value self items i qlist append value execute requires resolver queue return qlist\\n',\n",
       " 'def execute self requires typing Dict object ResultVariable resolver Resolver queue QueueScheduler object qlist Dict for i in range len self items key value self items i qlist key value execute requires resolver queue return qlist\\n',\n",
       " 'def __repr__ self str return s s self index_type self query\\n',\n",
       " 'def __repr__ self str return s s s self rootobject self relation self querypart\\n',\n",
       " 'def init_env self python_exec sys executable python_name os path basename sys executable python_bin os path join self env_path bin python_name if not os path exists python_bin venv_call python_exec m virtualenv try subprocess check_output venv_call version except subprocess CalledProcessError raise Exception Virtualenv not installed for python s python_exec proc subprocess Popen venv_call p python_exec self env_path env os environ copy stdout subprocess PIPE stderr subprocess PIPE out err proc communicate if proc returncode 0 LOGGER debug Created a new virtualenv at s self env_path else LOGGER error Unable to create new virtualenv at s s s self env_path out decode err decode return False self virtual_python python_bin self virtual_pip os path join self env_path bin pip return True\\n',\n",
       " 'def use_virtual_env self if not self init_env raise Exception Unable to init virtual environment activate_file os path join self env_path bin activate_this py if os path exists activate_file with open activate_file as f code compile f read activate_file exec exec code __file__ activate_file else raise Exception Unable to activate virtual environment because s does not exist activate_file pkg_resources working_set pkg_resources WorkingSet _build_master\\n',\n",
       " 'def _parse_line self req_line str tuple at VirtualEnv _at_fragment_re search req_line if at is not None d at groupdict return d name d req egg d name egg VirtualEnv _egg_fragment_re search req_line if egg is not None d egg groupdict return d name req_line return None req_line\\n',\n",
       " 'def _install self requirements_list None requirements_file self _gen_requirements_file requirements_list try fdnum path tempfile mkstemp fd os fdopen fdnum w fd write requirements_file fd close cmd self virtual_pip install r path output b try output subprocess check_output cmd stderr subprocess STDOUT except Exception LOGGER debug s s cmd output decode LOGGER debug requirements s requirements_file raise else LOGGER debug s s cmd output decode finally if os path exists path os remove path pkg_resources working_set pkg_resources WorkingSet _build_master\\n',\n",
       " 'def _read_current_requirements_hash self path os path join self env_path requirements sha1sum if not os path exists path return with open path r as fd return fd read strip\\n',\n",
       " 'def _set_current_requirements_hash self new_hash path os path join self env_path requirements sha1sum with open path w as fd fd write new_hash\\n',\n",
       " 'def install_from_list self requirements_list list detailed_cache False cache True None requirements_list sorted requirements_list if detailed_cache requirements_list sorted list set requirements_list self __cache_done if len requirements_list 0 return sha1sum hashlib sha1 sha1sum update n join requirements_list encode new_req_hash sha1sum hexdigest current_hash self _read_current_requirements_hash if new_req_hash current_hash and cache return self _install requirements_list self _set_current_requirements_hash new_req_hash for x in requirements_list self __cache_done add x\\n',\n",
       " 'pytest mark gen_test timeout 60 def test_param client environment fake_uuid uuid uuid4 result yield client list_params tid fake_uuid assert result code 404 result yield client list_params tid environment assert result code 200\\n',\n",
       " 'def execute self requires Dict object ResultVariable instance Resolver queue QueueScheduler object expr self implements constraint if not expr execute requires instance queue return None myqueue queue for_tracker ImplementsTracker self instance implementations self implements implementations for impl in implementations if instance add_implementation impl xc ExecutionContext impl statements instance for_namespace impl statements namespace xc emit myqueue return None\\n',\n",
       " 'def execute self requires Dict object object resolver Resolver queue QueueScheduler object var self base execute requires resolver queue if isinstance var Unknown return None if not isinstance var list raise TypingException self A for loop can only be applied to lists and relations helper requires self for loop_var in var helper receive_result loop_var self location return None\\n',\n",
       " 'def execute self requires Dict object ResultVariable resolver Resolver queue QueueScheduler type_class self type get_entity attributes k v execute requires resolver queue for k v in self _direct_attributes items for k v in self type get_defaults items if k not in attributes attributes k v execute requires resolver queue for k v in type_class get_default_values items if k not in attributes attributes k v execute requires resolver queue instances for index in type_class get_indices params for attr in index params append attr attributes attr obj type_class lookup_index params self if obj is not None if obj get_type get_entity type_class raise DuplicateException self object Type found in index is not an exact match instances append obj if len instances 0 first instances 0 for i in instances 1 if i first raise Exception Inconsistent indexes detected object_instance first for k v in attributes items object_instance set_attribute k v self location else object_instance type_class get_instance attributes resolver queue self location self copy_location object_instance for attributename valueexpression in self _indirect_attributes items var object_instance get_attribute attributename reqs valueexpression requires_emit_gradual resolver queue var SetAttributeHelper queue resolver var reqs valueexpression self object_instance attributename if self implemented raise Exception don t know this feature else for stmt in type_class get_sub_constructor stmt emit object_instance queue if self register raise Exception don t know this feature object_instance trackers append queue get_tracker return object_instance\\n',\n",
       " 'def add_attribute self lname LocatableString value object name str lname if name not in self __attributes self __attributes name value self anchors append AttributeReferenceAnchor lname get_location lname namespace self class_type name self anchors extend value get_anchors else raise RuntimeException self The attribute s in the constructor call of s is already set name self class_type\\n',\n",
       " 'def get_attributes self Dict str ExpressionStatement return self __attributes\\n',\n",
       " 'def __repr__ self str return Construct s self class_type\\n',\n",
       " 'def do_compile refs project Project get compiler Compiler os path join project project_path project main_file refs refs LOGGER debug Starting compile statements blocks compiler compile sched scheduler Scheduler success sched run compiler statements blocks LOGGER debug Compile done if not success sys stderr write Unable to execute all statements n return sched get_types compiler get_ns\\n',\n",
       " 'def anchormap refs project Project get compiler Compiler os path join project project_path project main_file refs refs LOGGER debug Starting compile statements blocks compiler compile sched scheduler Scheduler return sched anchormap compiler statements blocks\\n',\n",
       " 'def is_loaded self return self __root_ns is not None\\n',\n",
       " 'def get_ns self if not self is_loaded self load return self __root_ns\\n',\n",
       " 'def read self path with open path r as file_d return file_d read\\n',\n",
       " 'def _load_plugins self plugin_dir namespace if not os path exists os path join plugin_dir __init__ py raise Exception The plugin directory s should be a valid python package with a __init__ py file plugin_dir mod_name join namespace to_path imp load_package mod_name plugin_dir for py_file in glob glob os path join plugin_dir py if not py_file endswith __init__ py sub_mod mod_name os path basename py_file split 0 new_ns Namespace sub_mod split 1 new_ns parent namespace self graph add_namespace new_ns namespace imp load_source sub_mod py_file\\n',\n",
       " 'def compile self project Project get self __root_ns project get_root_namespace project load statements blocks project get_complete_ast for name cls in PluginMeta get_functions items mod_ns cls __module__ split if mod_ns 0 inmanta_plugins raise Exception All plugin modules should be loaded in the impera_plugins package not in s cls __module__ mod_ns mod_ns 1 ns self __root_ns for part in mod_ns if ns is None break ns ns get_child part if ns is None raise Exception Unable to find namespace for plugin module s cls __module__ cls namespace ns name name split 1 statement PluginStatement ns name cls statements append statement ns self __root_ns get_child_or_create std nullrange Range internal 1 0 0 0 entity DefineEntity ns LocatableString Entity nullrange 0 ns The entity all other entities inherit from str_std_entity LocatableString std Entity nullrange 0 ns requires_rel DefineRelation str_std_entity LocatableString requires nullrange 0 ns 0 None False str_std_entity LocatableString provides nullrange 0 ns 0 None False requires_rel namespace self __root_ns get_ns_from_string std statements append entity statements append requires_rel return statements blocks\\n',\n",
       " 'def get_type self Type return self __type\\n',\n",
       " 'def get_name self str return self __name\\n',\n",
       " 'def __hash__ self int return hash self __name\\n',\n",
       " 'def get_entity self Entity return self __entity\\n',\n",
       " 'def validate self value object None if not isinstance value Unknown self type validate value\\n',\n",
       " 'def set_multiplicity self values Tuple int int None self low values 0 self high values 1\\n',\n",
       " 'def test_set_wrong_relation_type snippetcompiler snippetcompiler setup_for_error entity Credentials end Credentials file 1 std File implement Credentials using std none creds Credentials file creds Could not set attribute file on instance __config__ Credentials instantiated at dir main cf 9 caused by Invalid class type for __config__ Credentials instantiated at dir main cf 9 should be std File reported in Construct Credentials dir main cf 9 34 reported in Construct Credentials dir main cf 9 snippetcompiler setup_for_error entity Credentials end Credentials file 1 std File implement Credentials using std none creds Credentials creds file creds Could not set attribute file on instance __config__ Credentials instantiated at dir main cf 9 caused by Invalid class type for __config__ Credentials instantiated at dir main cf 9 should be std File reported in creds file creds dir main cf 10 22 reported in creds file creds dir main cf 10\\n',\n",
       " 'def test_snippets here os getcwd fixture CompilerFixture fail False for i in glob glob here snip print 20 print i try fixture run_file i except Exception as e print e fail True print 20 for i in os listdir here try x os path join here i print 20 print x if os path isdir x fixture run_project x except Exception as e print e fail True print 20 return fail\\n',\n",
       " 'def ensure_module name if name not in sys modules parts name split mod imp new_module parts 1 sys modules name mod\\n',\n",
       " 'def resume self requires Dict object ResultVariable resolver Resolver queue_scheduler QueueScheduler target ResultVariable None obj self instance execute requires resolver queue_scheduler if isinstance obj list raise RuntimeException self can not get a attribute s s is a list self attribute obj if not isinstance obj Instance raise RuntimeException self can not get a attribute s s not an entity self attribute obj attr obj get_attribute self attribute self attr attr if attr is_ready self target set_value attr get_value self location else if self resultcollector is not None attr listener self resultcollector self location ExecutionUnit queue_scheduler resolver self target x attr self\\n',\n",
       " 'def resume self requires Dict object ResultVariable resolver Resolver queue_scheduler QueueScheduler None try obj self instance execute k v get_value for k v in requires items resolver queue_scheduler if isinstance obj list raise RuntimeException self can not get a attribute s s is a list self attribute obj attr obj get_attribute self attribute self attr attr if attr is_ready attr get_value self target set_value True self location else requires x attr RawUnit queue_scheduler resolver requires self except RuntimeException self target set_value False self location\\n',\n",
       " 'classmethod def register_operator cls operator_string operator_class cls __operator operator_string operator_class\\n',\n",
       " 'classmethod def get_operator_class cls oper if oper in cls __operator return cls __operator oper return None\\n',\n",
       " 'abstractmethod def _op self args\\n',\n",
       " 'def __repr__ self arg_list for arg in self _arguments arg_list append str arg return s s self __class__ __name__ join arg_list\\n',\n",
       " 'def to_function self return create_function self\\n',\n",
       " 'def _op self args return self _bin_op args\\n',\n",
       " 'abstractmethod def _bin_op self arg1 arg2\\n',\n",
       " 'def _bin_op self arg1 arg2 raise NotImplementedError\\n',\n",
       " 'def _op self args return self _un_op args\\n',\n",
       " 'abstractmethod def _un_op self arg\\n',\n",
       " 'def _un_op self arg return not arg\\n',\n",
       " 'def __repr__ self return s s s self __class__ __name__ self _arguments 0 self _arguments 1 value\\n',\n",
       " 'classmethod def add cls name function help_msg parser_config require_project False if name in cls __command_functions raise Exception Command s already registered name cls __command_functions name function function help help_msg parser_config parser_config require_project require_project\\n',\n",
       " 'classmethod def reset cls cls __command_functions\\n',\n",
       " 'classmethod def commands cls return cls __command_functions\\n',\n",
       " 'def make_random_file size 0 randomvalue str random randint 0 10000 if size 0 while len randomvalue size randomvalue randomvalue content Hello world s n randomvalue encode hash hash_file content body base64 b64encode content decode ascii return hash content body\\n',\n",
       " 'pytest mark gen_test def test_gzip_encoding server hash content body make_random_file size 1024 port config Config get server_rest_transport port url http localhost s api v1 file s port hash zipped body protocol gzipped_json content body assert zipped request HTTPRequest url url method PUT headers Accept Encoding gzip Content Encoding gzip body body decompress_response True client AsyncHTTPClient response yield client fetch request assert response code 200 request HTTPRequest url url method GET headers Accept Encoding gzip decompress_response True client AsyncHTTPClient response yield client fetch request assert response code 200 assert response headers X Consumed Content Encoding gzip\\n',\n",
       " 'pytest mark gen_test timeout 60 def test_form client environment form_id cwdemo forms ClearwaterSize form_data attributes bono default 1 options min 1 max 100 widget slider help help type number ralf default 1 options min 1 max 100 widget slider help help type number options title VNF replication help help record_count 1 type cwdemo forms ClearwaterSize result yield client put_form tid environment id form_id form form_data assert result code 200 result yield client get_form environment form_id assert result code 200 result yield client list_forms environment assert result code 200 assert len result result forms 1 assert result result forms 0 form_type form_id\\n',\n",
       " 'pytest mark gen_test timeout 60 def test_update_form client environment form_id cwdemo forms ClearwaterSize form_data attributes bono default 1 options min 1 max 100 widget slider help help type number ralf default 1 options min 1 max 100 widget slider help help type number options title VNF replication help help record_count 1 type cwdemo forms ClearwaterSize result yield client put_form tid environment id form_id form form_data assert result code 200 result yield client get_form environment form_id assert result code 200 assert len result result form field_options 2 form_data attributes sprout default 1 options min 1 max 100 widget slider help help type number result yield client put_form tid environment id form_id form form_data assert result code 200 result yield client get_form environment form_id assert result code 200 assert len result result form field_options 3\\n',\n",
       " 'pytest mark gen_test timeout 60 def test_records client environment form_id FormType result yield client put_form tid environment id form_id form attributes field1 default 1 options min 1 max 100 type number field2 default options type string options type form_id assert result code 200 result yield client create_record tid environment form_type form_id form field1 10 field2 value assert result code 200 record_id result result record id result yield client update_record tid environment id record_id form field1 20 field2 value2 assert result code 200 result yield client get_record tid environment id record_id assert result code 200 result yield client list_records tid environment form_type form_id assert result code 200 assert len result result records 1 yield client create_record tid environment form_type form_id form field1 10 field2 value result yield client list_records tid environment form_type form_id include_record True assert result code 200 assert len result result records 2 assert field1 in result result records 0 fields result yield client delete_record tid environment id record_id assert result code 200\\n',\n",
       " 'def hash_file content sha1sum hashlib new sha1 sha1sum update content return sha1sum hexdigest\\n',\n",
       " 'def plugin function typing Callable None commands typing List str None emits_statements bool False def curry_name name None commands None emits_statements False Function to curry the name of the function def call fnc Create class to register the function and return the function itself def wrapper self args Python will bind the function as method into the class return fnc args nonlocal name commands emits_statements if name is None name fnc __name__ dictionary dictionary __module__ fnc __module__ dictionary __function_name__ name dictionary opts bin commands emits_statements emits_statements dictionary call wrapper dictionary __function__ fnc bases Plugin PluginMeta __new__ PluginMeta name bases dictionary return fnc return call if function is None return curry_name commands commands emits_statements emits_statements elif isinstance function str return curry_name function commands commands emits_statements emits_statements elif function is not None fnc curry_name commands commands emits_statements emits_statements return fnc function\\n',\n",
       " 'def emit_expression self stmt self owner copy_location stmt stmt normalize self resolver reqs stmt requires_emit self resolver self queue ExecutionUnit self queue self resolver self result reqs stmt provides False\\n',\n",
       " 'def get_type self name str try return self queue get_types name except KeyError raise TypeNotFoundException name self owner namespace\\n',\n",
       " 'def get_data_dir self data_dir os path join data self owner function namespace get_full_name if not os path exists data_dir os makedirs data_dir exist_ok True return data_dir\\n',\n",
       " 'def run_sync self function typing Callable timeout int 5 from tornado ioloop import IOLoop TimeoutError try return IOLoop current run_sync function timeout except TimeoutError raise ConnectionRefusedError\\n',\n",
       " 'classmethod def add_function cls plugin_class name plugin_class __function_name__ ns_parts str plugin_class __module__ split ns_parts append name if ns_parts 0 inmanta_plugins raise Exception All plugin modules should be loaded in the inmanta_plugins package name join ns_parts 1 cls __functions name plugin_class\\n',\n",
       " 'classmethod def get_functions cls return cls __functions\\n',\n",
       " 'def _load_signature self function arg_spec inspect getfullargspec function if arg_spec defaults is not None default_start len arg_spec args len arg_spec defaults else default_start None arguments for i in range len arg_spec args arg arg_spec args i if arg not in arg_spec annotations raise Exception All arguments of plugin s should be annotated function __name__ spec_type arg_spec annotations arg if spec_type Context self _context i elif default_start is not None and default_start i default_value arg_spec defaults default_start i arguments append arg spec_type default_value else arguments append arg spec_type if return in arg_spec annotations self _return arg_spec annotations return return arguments\\n',\n",
       " 'def add_argument self arg_type arg_type_name arg_name optional False self arguments append arg_type arg_type_name arg_name optional\\n',\n",
       " 'def get_signature self arg_list for arg in self arguments if len arg 3 arg_list append s s s arg 0 arg 1 str arg 2 elif len arg 2 arg_list append s s arg 0 arg 1 else arg_list append arg 0 args join arg_list if self _return is None return s s self __class__ __function_name__ args return s s s self __class__ __function_name__ args self _return\\n',\n",
       " 'def to_type self arg_type resolver if arg_type is None return None if not isinstance arg_type str raise CompilerException bad annotation in plugin s s expected str but got s s self ns self __class__ __function_name__ type arg_type arg_type if arg_type any return None if arg_type list return list if arg_type expression return None if arg_type endswith basetypename arg_type 0 2 basetype resolver get_type basetypename return TypedList basetype return resolver get_type arg_type\\n',\n",
       " 'def _is_instance self value arg_type if arg_type is None return True if hasattr arg_type validate return arg_type validate value return isinstance value arg_type\\n',\n",
       " 'def check_args self args max_arg len self arguments required len x for x in self arguments if len x 2 if len args required or len args max_arg raise Exception Incorrect number of arguments for s Expected at least d got d self get_signature required len args for i in range len args if isinstance args i Unknown return False if self arguments i 0 is not None and not self _is_instance args i self argtypes i raise Exception Invalid type for argument d of s it should be s and s given i 1 self __class__ __function_name__ self arguments i 1 args i __class__ __name__ return True\\n',\n",
       " 'def emit_statement self return self new_statement\\n',\n",
       " 'def get_variable self name scope return DynamicProxy return_value self _scope get_variable name scope value\\n',\n",
       " 'def check_requirements self if bin in self opts and self opts bin is not None for _bin in self opts bin p subprocess Popen bash c type p s _bin stdout subprocess PIPE result p communicate if len result 0 0 raise Exception s requires s to be available in PATH self __function_name__ _bin\\n',\n",
       " 'def __call__ self args self check_requirements new_args for arg in args if isinstance arg Context new_args append arg else new_args append DynamicProxy return_value arg value self call new_args value DynamicProxy unwrap value if self returntype is not None and not isinstance value Unknown valid False exception None try valid value is None or self _is_instance value self returntype except RuntimeException as e raise e except Exception as exp exception exp if not valid msg if exception is not None msg n tException details str exception raise Exception Plugin s should return value of type s s was returned s self __class__ __function_name__ self returntype value msg return value\\n',\n",
       " 'pytest fixture scope session def mongo_client mongo_db port int mongo_db port return pymongo MongoClient port port\\n',\n",
       " 'def get_free_tcp_port tcp socket socket socket AF_INET socket SOCK_STREAM tcp bind 0 _addr port tcp getsockname tcp close return str port\\n',\n",
       " 'pytest fixture scope function def environment client server io_loop def create_project return client create_project env test result io_loop run_sync create_project assert result code 200 project_id result result project id def create_env return client create_environment project_id project_id name dev result io_loop run_sync create_env env_id result result environment id yield env_id\\n',\n",
       " 'pytest fixture scope function def environment_multi client_multi server_multi io_loop def create_project return client_multi create_project env test result io_loop run_sync create_project assert result code 200 project_id result result project id def create_env return client_multi create_environment project_id project_id name dev result io_loop run_sync create_env env_id result result environment id yield env_id\\n',\n",
       " 'def load_modules self mod_dir os path join self __code_dir MODULE_DIR if os path exists os path join self __code_dir VERSION_FILE fd open os path join self __code_dir VERSION_FILE r self __current_version int fd read fd close pkg_resources working_set pkg_resources WorkingSet _build_master for py in glob glob os path join mod_dir py if mod_dir in py mod_name py len mod_dir 1 3 else mod_name py 3 source_code with open py r as fd source_code fd read encode utf 8 sha1sum hashlib new sha1 sha1sum update source_code hv sha1sum hexdigest self _load_module mod_name py hv\\n',\n",
       " 'def __check_dir self if not os path exists self __code_dir os makedirs self __code_dir exist_ok True if not os path exists os path join self __code_dir MODULE_DIR os makedirs os path join self __code_dir MODULE_DIR exist_ok True\\n',\n",
       " 'def _load_module self mod_name python_file hv try mod imp load_source mod_name python_file self __modules mod_name hv mod LOGGER info Loaded module s mod_name except ImportError LOGGER exception Unable to load module s mod_name\\n',\n",
       " 'def deploy_version self key mod persist False LOGGER info Deploying code key s key name mod 1 source_code mod 2 if name not in self __modules or key self __modules name 0 source_file os path join self __code_dir MODULE_DIR name py fd open source_file w fd write source_code fd close self _load_module name source_file key if persist with open os path join self __code_dir PERSIST_FILE w as fd json dump mod fd pkg_resources working_set pkg_resources WorkingSet _build_master\\n',\n",
       " 'def get_module_payload self with open os path join self __code_dir PERSIST_FILE w as fd return fd read\\n',\n",
       " 'def search_artist_ids self filepath artists self create_collection filepath item_type artists print artists found format len artists artist_ids set ignored set for artist in tqdm artists while True try results self spotify search q artist artist type artist limit 3 aid str results artists items 0 id artist_ids add aid logging info Added artist ID for format artist except IndexError ignored add artist logging debug Ignored format artist except ConnectionError continue except SpotifyException as e logging debug e __str__ self refresh_token return list artist_ids\\n',\n",
       " 'def search_song_ids self filepath tracks self create_collection filepath item_type tracks print tracks found format len tracks track_ids set ignored set with open cache 0 tracks txt format os environ USERNAME w as track_cache for track in tqdm tracks try results self spotify search q track type track limit 3 tid str results tracks items 0 id track_ids add tid track_cache write tid n logging info Added track ID for format track except IndexError ignored add track logging debug Ignored format track except ConnectionError continue except SpotifyException as e self refresh_token logging debug e __str__ return list track_ids\\n',\n",
       " 'def find_clang set_library_path True verbose 0 SUPPORTED_VERSIONS 6 0 5 0 4 0 3 9 3 8 3 7 3 6 3 5 basepath os sep join clang __file__ split os sep 4 if verbose 1 print Found Python bindings of libclang in s basepath for clang_version in SUPPORTED_VERSIONS search_paths os path join usr lib llvm s clang_version lib os path join usr local lib llvm s clang_version lib basepath for lib_path in search_paths if verbose 2 print Searching for libclang s in s clang_version lib_path if not os path exists lib_path if verbose 2 print Directory does not exist continue lib_filename _find_lib lib_path clang_version if lib_filename is None if verbose 2 print Library not found continue if verbose 1 print Found libclang at s lib_filename clang_incdir _find_include_directory lib_path clang_version if verbose 1 print Found clang include directory at s clang_incdir if set_library_path cindex Config set_library_path lib_path return clang_version clang_incdir raise ImportError Could not find a valid installation of libclang dev Only versions s are supported at the moment SUPPORTED_VERSIONS\\n',\n",
       " 'def full_paths filenames if isinstance filenames str filenames filenames if PREFIX return filenames else attach_prefix lambda filename filename if filename startswith else os path join PREFIX filename full_paths map attach_prefix filenames for path in full_paths assert os path exists path return full_paths\\n',\n",
       " 'def assert_warns_message warning_class message func args kw clean_warning_registry with warnings catch_warnings record True as w warnings simplefilter always if hasattr np VisibleDeprecationWarning warnings simplefilter ignore np VisibleDeprecationWarning result func args kw if not len w 0 raise AssertionError No warning raised when calling s func __name__ found issubclass warning category warning_class for warning in w if not any found raise AssertionError No warning raised for s with class s func __name__ warning_class message_found False for index in i for i x in enumerate found if x msg w index message msg str msg args 0 if hasattr msg args else msg if callable message check_in_message message else check_in_message lambda msg message in msg if check_in_message msg message_found True break if not message_found raise AssertionError Did not receive the message you expected s for s got s message func __name__ msg return result\\n',\n",
       " 'def clean_warning_registry warnings resetwarnings reg __warningregistry__ for mod in list sys modules values if hasattr mod reg getattr mod reg clear\\n',\n",
       " 'abstractmethod def _specialize self general specs\\n',\n",
       " 'def lines args return os linesep join args\\n',\n",
       " 'def indent_block block level lines block split os linesep indented_lines _indent_line line level level for line in lines indented_block os linesep join indented_lines return indented_block\\n',\n",
       " 'def convert_to_docstring comment if comment is None return comment _strip_comment_markers comment return _separate_brief_comment comment\\n',\n",
       " 'def from_camel_case name new_name str name i 0 while i len new_name if new_name i isupper and i 0 new_name new_name i _ new_name i i 1 i 1 return new_name lower\\n',\n",
       " 'def make_header header return lines 78 header ljust 79 78\\n',\n",
       " 'def file_ending filename return filename split 1\\n',\n",
       " 'contextmanager def hidden_stream fileno if fileno not in 1 2 raise ValueError Expected fileno 1 or 2 stream_name stdout stderr fileno 1 getattr sys stream_name flush oldstream_fno os dup fileno devnull os open os devnull os O_WRONLY newstream os dup fileno os dup2 devnull fileno os close devnull setattr sys stream_name os fdopen newstream w try yield finally os dup2 oldstream_fno fileno\\n',\n",
       " 'def remove_files filenames for f in filenames if os path exists f os remove f\\n',\n",
       " 'def replace_keyword_argnames argname if keyword iskeyword argname argname _ argname return argname\\n',\n",
       " 'def load_config custom_config if custom_config is None return Config if not os path exists custom_config raise ValueError Configuration file s does not exist custom_config parts custom_config split os sep path os sep join parts 1 filename parts 1 module _derive_module_name_from filename sys path insert 0 path imported_module __import__ module sys path pop 0 return imported_module config\\n',\n",
       " 'def make_cython_wrapper filenames sources modulename None target config Config incdirs compiler_flags O3 verbose 0 if isinstance filenames str filenames filenames if len filenames 1 and modulename is None modulename _derive_module_name_from filenames 0 if modulename is None raise ValueError Please give a module name when there are multiple C files that you want to wrap for incdir in incdirs if not os path exists incdir raise ValueError Include directory s does not exist incdir for filename in filenames if file_ending filename not in config cpp_header_endings raise ValueError s does not seem to be a header file which is required filename if not os path exists filename raise ValueError File s does not exist filename includes type_info asts _parse_files filenames config incdirs verbose postprocess_asts asts results dict _make_extension modulename asts includes type_info config _make_declarations asts includes config _make_setup sources modulename target incdirs compiler_flags config if verbose 2 for filename in sorted results keys print make_header Exporting file s filename print results filename return results\\n',\n",
       " 'def write_files files target for filename content in files items outputfile os path join target filename with open outputfile w as f f write content\\n',\n",
       " 'def run_setup setuppy_name setup py hide_errors False cmd python s build_ext inplace setuppy_name with hidden_stdout if hide_errors with hidden_stderr os system cmd else os system cmd\\n',\n",
       " 'def postprocess_asts asts classes _build_classdict asts leaf_names _find_leaves classes values for leaf_name in leaf_names _copy_methods_from_base classes classes leaf_name _remove_overloaded_methods classes values _remove_overloaded_functions asts\\n',\n",
       " 'def _build_classdict asts classes ClassDict for ast in asts for n in ast nodes if isinstance n Clazz classes insert n return classes\\n',\n",
       " 'def _find_leaves classes leaf_names set for clazz in classes leaf_names add clazz fullname if clazz base is not None if clazz base in leaf_names leaf_names remove clazz base return leaf_names\\n',\n",
       " 'def _copy_methods_from_base classes clazz if clazz is None return if clazz base is not None base_methods _copy_methods_from_base classes classes get clazz base unique_methods set n name for n in clazz nodes if isinstance n Method base_methods m for m in base_methods if m name not in unique_methods clazz nodes extend base_methods return node for node in clazz nodes if isinstance node Method\\n',\n",
       " 'def _remove_overloaded_methods classes for clazz in classes methods n for n in clazz nodes if isinstance n Method method_names removed_methods for m in methods if m name in method_names warnings warn Method s s is already defined Only one method will be exposed clazz name m name removed_methods append m else method_names append m name clazz nodes n for n in clazz nodes if n not in removed_methods\\n',\n",
       " 'def _remove_overloaded_functions asts functions n for ast in asts for n in ast nodes if isinstance n Function function_names removed_functions for f in functions if f name in function_names warnings warn Function s is already defined Only one method will be exposed f name removed_functions append f else function_names append f name for ast in asts ast nodes n for n in ast nodes if n not in removed_functions\\n',\n",
       " 'def render template kwargs template_file resource_filename pywrap os path join template_data template template if not os path exists template_file raise IOError No template for s found template template jinja2 Template open template_file r read return template render kwargs\\n',\n",
       " 'def num2char sth if isinstance sth int return charList sth else return join map lambda x charList x sth\\n',\n",
       " 'def char2num sth if len sth 1 return charNum sth else return tuple map lambda x charNum x sth\\n',\n",
       " 'def num2pinyin sth if isinstance sth int return pinyinList sth else return tuple map lambda x pinyinList x sth\\n',\n",
       " 'def pinyin2num sth if isinstance sth str return pinyinNum sth else return tuple map lambda x pinyinNum x sth\\n',\n",
       " 'def main if path exists tmp return True\\n',\n",
       " 'def test_transforms_typecheck self with self assertRaises TypeError list 0 for x in range 2 for x in range 10 convert_arr_pixel_to_metric list self control out None with self assertRaises TypeError convert_arr_pixel_to_metric np empty 10 3 self control out None with self assertRaises TypeError convert_arr_metric_to_pixel np empty 2 1 self control out None with self assertRaises TypeError convert_arr_metric_to_pixel np zeros 11 2 self control out np zeros 12 2\\n',\n",
       " 'def test_transforms_regress self input np full 3 2 100 0 output np zeros 3 2 correct_output_pixel_to_metric 8181 0 6657 92 8181 0 6657 92 8181 0 6657 92 correct_output_metric_to_pixel 646 60066007 505 81188119 646 60066007 505 81188119 646 60066007 505 81188119 convert_arr_pixel_to_metric input self control out output np testing assert_array_almost_equal output correct_output_pixel_to_metric decimal 7 output np zeros 3 2 convert_arr_metric_to_pixel input self control out output np testing assert_array_almost_equal output correct_output_metric_to_pixel decimal 7 output convert_arr_pixel_to_metric input self control out None np testing assert_array_almost_equal output correct_output_pixel_to_metric decimal 7 output np zeros 3 2 output convert_arr_metric_to_pixel input self control out None np testing assert_array_almost_equal output correct_output_metric_to_pixel decimal 7\\n',\n",
       " 'def test_transforms self cpar ControlParams 1 cpar set_image_size 1280 1000 cpar set_pixel_size 0 1 0 1 metric_pos np array 1 0 1 0 10 0 15 0 20 0 30 0 pixel_pos np array 650 0 490 0 540 0 350 0 840 0 800 0 np testing assert_array_almost_equal pixel_pos convert_arr_metric_to_pixel metric_pos cpar np testing assert_array_almost_equal metric_pos convert_arr_pixel_to_metric pixel_pos cpar\\n',\n",
       " 'def test_brown_affine self cal Calibration cal set_pos np r_ 0 0 0 0 40 0 cal set_angles np r_ 0 0 0 0 0 0 cal set_primary_point np r_ 0 0 0 0 10 0 cal set_glass_vec np r_ 0 0 0 0 20 0 cal set_radial_distortion np zeros 3 cal set_decentering np zeros 2 cal set_affine_trans np r_ 1 0 ref_pos np array 0 1 0 1 1 0 1 0 10 0 10 0 distorted distort_arr_brown_affine ref_pos cal np testing assert_array_almost_equal distorted ref_pos cal set_radial_distortion np r_ 0 001 0 0 0 0 distorted distort_arr_brown_affine ref_pos cal self failUnless np all abs distorted abs ref_pos\\n',\n",
       " 'def test_full_correction self cal Calibration cal set_pos np r_ 0 0 0 0 40 0 cal set_angles np r_ 0 0 0 0 0 0 cal set_primary_point np r_ 0 0 0 0 10 0 cal set_glass_vec np r_ 0 0 0 0 20 0 cal set_radial_distortion np zeros 3 cal set_decentering np zeros 2 cal set_affine_trans np r_ 1 0 ref_pos np array 0 1 0 1 1 0 1 0 5 0 5 0 cal set_radial_distortion np r_ 0 001 0 0 0 0 distorted distort_arr_brown_affine ref_pos cal corrected distorted_to_flat distorted cal np testing assert_array_almost_equal ref_pos corrected decimal 6\\n',\n",
       " 'def test_match_detection_to_ref self xyz_input np array 10 10 10 200 200 200 600 800 100 20 10 2000 30 30 30 dtype float coords_count len xyz_input xy_img_pts_metric image_coordinates xyz_input self calibration self control get_multimedia_params xy_img_pts_pixel convert_arr_metric_to_pixel xy_img_pts_metric control self control target_array TargetArray coords_count for i in range coords_count target_array i set_pnr i target_array i set_pos xy_img_pts_pixel i 0 xy_img_pts_pixel i 1 indices range coords_count shuffled_indices range coords_count while indices shuffled_indices random shuffle shuffled_indices rand_targ_array TargetArray coords_count for i in range coords_count rand_targ_array shuffled_indices i set_pos target_array i pos rand_targ_array shuffled_indices i set_pnr target_array i pnr matched_target_array match_detection_to_ref cal self calibration ref_pts xyz_input img_pts rand_targ_array cparam self control for i in range coords_count if matched_target_array i pos target_array i pos or matched_target_array i pnr target_array i pnr self fail with self assertRaises ValueError match_detection_to_ref cal self calibration ref_pts xyz_input img_pts TargetArray coords_count 1 cparam self control\\n',\n",
       " 'def test_external_calibration self ref_pts np array 40 0 25 0 8 0 40 0 15 0 0 0 40 0 15 0 0 0 40 0 0 0 8 0 targets convert_arr_metric_to_pixel image_coordinates ref_pts self cal self control get_multimedia_params self control targets 1 0 1 self assertTrue external_calibration self cal ref_pts targets self control np testing assert_array_almost_equal self cal get_angles self orig_cal get_angles decimal 4 np testing assert_array_almost_equal self cal get_pos self orig_cal get_pos decimal 3\\n',\n",
       " 'def test_Calibration_instantiation self self output_ori_file_name self output_directory output_ori self output_add_file_name self output_directory output_add self cal from_file self input_ori_file_name self input_add_file_name self cal write self output_ori_file_name self output_add_file_name self assertTrue filecmp cmp self input_ori_file_name self output_ori_file_name 0 self assertTrue filecmp cmp self input_add_file_name self output_add_file_name 0\\n',\n",
       " 'def test_set_pos self new_np numpy array 111 1111 222 2222 333 3333 self cal set_pos new_np numpy testing assert_array_equal new_np self cal get_pos self assertRaises ValueError self cal set_pos numpy array 1 2 3 4 self assertRaises ValueError self cal set_pos numpy array 1 2\\n',\n",
       " 'def test_set_angles self dmatrix_before self cal get_rotation_matrix angles_np numpy array 0 1111 0 2222 0 3333 self cal set_angles angles_np dmatrix_after self cal get_rotation_matrix numpy testing assert_array_equal self cal get_angles angles_np self assertFalse numpy array_equal dmatrix_before dmatrix_after self assertRaises ValueError self cal set_angles numpy array 1 2 3 4 self assertRaises ValueError self cal set_angles numpy array 1 2\\n',\n",
       " 'def test_set_primary self new_pp numpy array 111 1111 222 2222 333 3333 self cal set_primary_point new_pp numpy testing assert_array_equal new_pp self cal get_primary_point self assertRaises ValueError self cal set_primary_point numpy ones 4 self assertRaises ValueError self cal set_primary_point numpy ones 2\\n',\n",
       " 'def test_set_radial self new_rd numpy array 111 1111 222 2222 333 3333 self cal set_radial_distortion new_rd numpy testing assert_array_equal new_rd self cal get_radial_distortion self assertRaises ValueError self cal set_radial_distortion numpy ones 4 self assertRaises ValueError self cal set_radial_distortion numpy ones 2\\n',\n",
       " 'def test_set_decentering self new_de numpy array 111 1111 222 2222 self cal set_decentering new_de numpy testing assert_array_equal new_de self cal get_decentering self assertRaises ValueError self cal set_decentering numpy ones 3 self assertRaises ValueError self cal set_decentering numpy ones 1\\n',\n",
       " 'def test_set_glass self new_gv numpy array 1 0 2 0 3 0 self cal set_glass_vec new_gv numpy testing assert_array_equal new_gv self cal get_glass_vec self assertRaises ValueError self cal set_glass_vec numpy ones 2 self assertRaises ValueError self cal set_glass_vec numpy ones 1\\n',\n",
       " 'def test_read_targets self targs read_targets liboptv tests testing_fodder sample_ 42 self failUnlessEqual len targs 2 self failUnlessEqual targ tnr for targ in targs 1 0 self failUnlessEqual targ pos 0 for targ in targs 1127 0 796 0 self failUnlessEqual targ pos 1 for targ in targs 796 0 809 0\\n',\n",
       " 'def test_sort_y self targs read_targets testing_fodder frame cam1 333 revs read_targets testing_fodder frame cam1_reversed 333 revs sort_y for targ rev in zip targs revs self failUnless targ pos rev pos\\n',\n",
       " 'def test_write_targets self targs read_targets liboptv tests testing_fodder sample_ 42 targs write testing_fodder round_trip 1 tback read_targets testing_fodder round_trip 1 self failUnlessEqual len targs len tback self failUnlessEqual targ tnr for targ in targs targ tnr for targ in tback self failUnlessEqual targ pos 0 for targ in targs targ pos 0 for targ in tback self failUnlessEqual targ pos 1 for targ in targs targ pos 1 for targ in tback\\n',\n",
       " 'def test_read_frame self targ_files testing_fodder frame cam d c for c in xrange 1 5 frm Frame 4 corres_file_base testing_fodder frame rt_is linkage_file_base testing_fodder frame ptv_is target_file_base targ_files frame_num 333 pos frm positions self failUnlessEqual pos shape 10 3 targs frm target_positions_for_camera 3 self failUnlessEqual targs shape 10 2 targs_correct np array 426 0 199 0 429 0 60 0 431 0 327 0 509 0 315 0 345 0 222 0 465 0 139 0 487 0 403 0 241 0 178 0 607 0 209 0 563 0 238 0 np testing assert_array_equal targs targs_correct\\n',\n",
       " 'def test_forward self shutil copytree testing_fodder track res_orig testing_fodder track res self tracker restart last_step 1 while self tracker step_forward self failUnless self tracker current_step last_step with open testing_fodder track res linkage d last_step as f lines f readlines if last_step 3 self failUnless lines 0 1 n else self failUnless lines 0 2 n last_step 1 self tracker finalize\\n',\n",
       " 'def test_full_forward self shutil copytree testing_fodder track res_orig testing_fodder track res self tracker full_forward\\n',\n",
       " 'def test_full_backward self shutil copytree testing_fodder track res_orig testing_fodder track res self tracker full_forward self tracker full_backward\\n',\n",
       " 'def parse_elb self global_params region lb elb elb name lb pop LoadBalancerName vpc_id lb VPCId if VPCId in lb and lb VPCId else ec2_classic manage_dictionary self vpcs vpc_id VPCConfig self vpc_resource_types get_keys lb elb DNSName CreatedTime AvailabilityZones Subnets Scheme elb security_groups for sg in lb SecurityGroups elb security_groups append GroupId sg manage_dictionary elb listeners policy_names for l in lb ListenerDescriptions listener l Listener manage_dictionary listener policies for policy_name in l PolicyNames policy_id self get_non_aws_id policy_name listener policies append policy_id if policy_id not in self elb_policies policy_names append policy_name elb listeners l Listener LoadBalancerPort listener if len policy_names policies api_clients region describe_load_balancer_policies LoadBalancerName elb name PolicyNames policy_names PolicyDescriptions for policy in policies policy name policy pop PolicyName policy_id self get_non_aws_id policy name self elb_policies policy_id policy manage_dictionary elb instances for i in lb Instances elb instances append i InstanceId elb attributes api_clients region describe_load_balancer_attributes LoadBalancerName elb name LoadBalancerAttributes self vpcs vpc_id elbs self get_non_aws_id elb name elb\\n',\n",
       " 'def parse_identitie self global_params region identity_name identity name identity_name policies policy_names api_clients region list_identity_policies Identity identity_name PolicyNames if len policy_names policies api_clients region get_identity_policies Identity identity_name PolicyNames policy_names Policies for policy_name in policies identity policies policy_name json loads policies policy_name dkim api_clients region get_identity_dkim_attributes Identities identity_name DkimAttributes identity_name identity DkimEnabled dkim DkimEnabled identity DkimVerificationStatus dkim DkimVerificationStatus self identities self get_non_aws_id identity_name identity\\n',\n",
       " 'def get_non_aws_id self name m sha1 m update name encode utf 8 return m hexdigest\\n',\n",
       " 'def fetch_all self credentials regions partition_name aws targets None global status formatted_string if not targets targets type self targets printInfo Fetching s config format_service_name self service formatted_string None api_service self service lower if self service in s3 api_clients for region in build_region_list self service regions partition_name api_clients region connect_service s3 credentials region silent True api_client api_clients list api_clients keys 0 elif self service route53domains api_client connect_service self service credentials us east 1 silent True else api_client connect_service self service credentials silent True params api_client api_client if self service in s3 params api_clients api_clients q self _init_threading self __fetch_target params self thread_config parse params api_client api_client q q if self service in s3 params api_clients api_clients qt self _init_threading self __fetch_service params self thread_config list self fetchstatuslogger FetchStatusLogger targets for target in targets qt put target qt join q join if self service iam self fetchstatuslogger show True\\n',\n",
       " 'def load self rule_type quiet False if self filename and os path exists self filename try with open self filename rt as f ruleset json load f self about ruleset about if about in ruleset else self rules for filename in ruleset rules self rules filename for rule in ruleset rules filename self handle_rule_versions filename rule_type rule except Exception as e printException e printError Error ruleset file s contains malformed JSON self filename self rules self about else self rules if not quiet printError Error the file s does not exist self filename\\n',\n",
       " 'def handle_rule_versions self filename rule_type rule if versions in rule versions rule pop versions for version_key_suffix in versions version versions version_key_suffix version key_suffix version_key_suffix tmp_rule dict rule version self rules filename append Rule filename rule_type tmp_rule else self rules filename append Rule filename rule_type rule\\n',\n",
       " 'def prepare_rules self attributes ip_ranges params for filename in self rule_definitions if filename in self rules for rule in self rules filename rule set_definition self rule_definitions attributes ip_ranges params else self rules filename new_rule Rule filename self rule_type enabled False level danger new_rule set_definition self rule_definitions attributes ip_ranges params self rules filename append new_rule\\n',\n",
       " 'def load_rule_definitions self ruleset_generator False rule_dirs self rule_definitions for rule_filename in self rules for rule in self rules rule_filename if not rule enabled and not ruleset_generator continue self rule_definitions os path basename rule_filename RuleDefinition rule_filename rule_dirs rule_dirs if ruleset_generator rule_dirs append os path join os path dirname os path realpath __file__ data findings rule_filenames for rule_dir in rule_dirs rule_filenames f for f in os listdir rule_dir if os path isfile os path join rule_dir f for rule_filename in rule_filenames if rule_filename not in self rule_definitions self rule_definitions os path basename rule_filename RuleDefinition rule_filename\\n',\n",
       " 'def search_ruleset self environment_name no_prompt False ruleset_found False if environment_name default ruleset_file_name ruleset s json environment_name ruleset_file_path os path join os getcwd ruleset_file_name if os path exists ruleset_file_path if no_prompt or prompt_4_yes_no A ruleset whose name matches your environment name was found in s Would you like to use it instead of the default one ruleset_file_name ruleset_found True self filename ruleset_file_path if not ruleset_found self filename os path join os path dirname os path realpath __file__ data rulesets default json\\n',\n",
       " 'def find_file self filename filetype rulesets if filename and not os path isfile filename if not filename startswith findings and not filename startswith filters filename s s filetype filename if not os path isfile filename filename os path join self rules_data_path filename if not os path isfile filename and not filename endswith json filename self find_file s json filename filetype return filename\\n',\n",
       " 'def prompt_4_yes_no question while True sys stdout write question y n try choice raw_input lower except choice input lower if choice yes or choice y return True elif choice no or choice n return False else printError s is not a valid answer Enter yes y or no n choice\\n',\n",
       " 'def prompt_4_overwrite filename force_write if not os path exists filename or force_write return True return prompt_4_yes_no File already exists Do you want to overwrite it format filename\\n',\n",
       " 'def parse_buckets self bucket params bucket name bucket pop Name api_client params api_clients get_s3_list_region list params api_clients keys 0 bucket CreationDate str bucket CreationDate bucket region get_s3_bucket_location api_client bucket name if bucket region EU bucket region eu west 1 if bucket region not in params api_clients printInfo Skipping bucket s region s outside of scope bucket name bucket region self buckets_count 1 return api_client params api_clients bucket region get_s3_bucket_logging api_client bucket name bucket get_s3_bucket_versioning api_client bucket name bucket get_s3_bucket_webhosting api_client bucket name bucket bucket grantees get_s3_acls api_client bucket name bucket get_s3_bucket_policy api_client bucket name bucket bucket id self get_non_aws_id bucket name self buckets bucket id bucket\\n',\n",
       " 'def get_attribute_at config target_path key default_value None for target in target_path config config target return config key if key in config else default_value\\n',\n",
       " 'def get_object_at dictionary path attribute_name None o dictionary for p in path o o p if attribute_name return o attribute_name else return o\\n',\n",
       " 'def get_value_at all_info current_path key to_string False keys key split if keys 1 id target_obj current_path len keys 1 else if key this target_path current_path elif in key target_path for i key in enumerate keys if key id target_path append current_path i else target_path append key if len keys len current_path target_path target_path keys len target_path else target_path copy deepcopy current_path target_path append key target_obj all_info for p in target_path try if type target_obj list and type target_obj 0 dict target_obj target_obj int p elif type target_obj list target_obj p elif p target_obj target_obj else try target_obj target_obj p except Exception as e printError Current path s str current_path printException e raise Exception except Exception as e printError Current path s str current_path printException e raise Exception if to_string return str target_obj else return target_obj\\n',\n",
       " 'def parse_cluster self global_params region cluster vpc_id cluster pop VpcId if VpcId in cluster else ec2_classic manage_dictionary self vpcs vpc_id VPCConfig self vpc_resource_types name cluster pop ClusterIdentifier cluster name name self vpcs vpc_id clusters name cluster\\n',\n",
       " 'def parse_parameter_group self global_params region parameter_group pg_name parameter_group pop ParameterGroupName pg_id self get_non_aws_id pg_name parameter_group name pg_name parameter_group parameters api_client api_clients region parameters handle_truncated_response api_client describe_cluster_parameters ParameterGroupName pg_name Parameters Parameters for parameter in parameters param param value parameter ParameterValue param source parameter Source parameter_group parameters parameter ParameterName param self parameter_groups pg_id parameter_group\\n',\n",
       " 'def parse_security_group self global_params region security_group name security_group pop ClusterSecurityGroupName security_group name name self security_groups name security_group\\n',\n",
       " 'def format_service_name service return formatted_service_name service if service in formatted_service_name else service upper\\n',\n",
       " 'def get_keys src dst keys for key in keys dst key src key if key in src else None\\n',\n",
       " 'def no_camel name s1 re sub A Z a z 1_ 2 name return re sub a z0 9 A Z 1_ 2 s1 lower\\n',\n",
       " 'def is_throttled e return True if hasattr e response and Error in e response and e response Error Code in Throttling RequestLimitExceeded ThrottlingException else False\\n',\n",
       " 'def parse_elastic_ip self global_params region eip self elastic_ips eip PublicIp eip\\n',\n",
       " 'def parse_instance self global_params region reservation for i in reservation Instances instance vpc_id i VpcId if VpcId in i and i VpcId else ec2_classic manage_dictionary self vpcs vpc_id VPCConfig self vpc_resource_types instance reservation_id reservation ReservationId instance id i InstanceId get_name i instance InstanceId get_keys i instance KeyName LaunchTime InstanceType State IamInstanceProfile SubnetId manage_dictionary instance network_interfaces for eni in i NetworkInterfaces nic get_keys eni nic Association Groups PrivateIpAddresses SubnetId Ipv6Addresses instance network_interfaces eni NetworkInterfaceId nic self vpcs vpc_id instances i InstanceId instance\\n',\n",
       " 'def parse_security_group self global_params region group vpc_id group VpcId if VpcId in group and group VpcId else ec2_classic manage_dictionary self vpcs vpc_id VPCConfig self vpc_resource_types security_group security_group name group GroupName security_group id group GroupId security_group description group Description security_group owner_id group OwnerId security_group rules ingress egress security_group rules ingress protocols security_group rules ingress count self __parse_security_group_rules group IpPermissions security_group rules egress protocols security_group rules egress count self __parse_security_group_rules group IpPermissionsEgress self vpcs vpc_id security_groups group GroupId security_group\\n',\n",
       " 'def __parse_security_group_rules self rules protocols rules_count 0 for rule in rules ip_protocol rule IpProtocol upper if ip_protocol 1 ip_protocol ALL protocols manage_dictionary protocols ip_protocol protocols ip_protocol manage_dictionary protocols ip_protocol ports port_value N A if FromPort in rule and ToPort in rule if ip_protocol ICMP port_value icmp_message_types_dict str rule FromPort elif rule FromPort rule ToPort port_value str rule FromPort else port_value s s rule FromPort rule ToPort manage_dictionary protocols ip_protocol ports port_value for grant in rule UserIdGroupPairs manage_dictionary protocols ip_protocol ports port_value security_groups protocols ip_protocol ports port_value security_groups append grant rules_count rules_count 1 for grant in rule IpRanges manage_dictionary protocols ip_protocol ports port_value cidrs protocols ip_protocol ports port_value cidrs append CIDR grant CidrIp rules_count rules_count 1 for grant in rule Ipv6Ranges manage_dictionary protocols ip_protocol ports port_value cidrs protocols ip_protocol ports port_value cidrs append CIDR grant CidrIpv6 rules_count rules_count 1 return protocols rules_count\\n',\n",
       " 'def parse_snapshot self global_params region snapshot snapshot id snapshot pop SnapshotId snapshot name get_name snapshot snapshot id self snapshots snapshot id snapshot snapshot createVolumPermission api_clients region describe_snapshot_attribute Attribute createVolumePermission SnapshotId snapshot id CreateVolumePermissions\\n',\n",
       " 'def parse_volume self global_params region volume volume id volume pop VolumeId volume name get_name volume volume id self volumes volume id volume\\n',\n",
       " 'def parse_alarm self global_params region alarm alarm arn alarm pop AlarmArn alarm name alarm pop AlarmName for k in AlarmConfigurationUpdatedTimestamp StateReason StateReasonData StateUpdatedTimestamp foo alarm pop k if k in alarm else None alarm_id self get_non_aws_id alarm arn self alarms alarm_id alarm\\n',\n",
       " 'def parse_file_system self global_params region file_system fs_id file_system pop FileSystemId file_system name file_system pop Name file_system tags handle_truncated_response api_clients region describe_tags FileSystemId fs_id Tags Tags mount_targets handle_truncated_response api_clients region describe_mount_targets FileSystemId fs_id MountTargets MountTargets file_system mount_targets for mt in mount_targets mt_id mt MountTargetId file_system mount_targets mt_id mt file_system mount_targets mt_id security_groups api_clients region describe_mount_target_security_groups MountTargetId mt_id SecurityGroups self file_systems fs_id file_system\\n',\n",
       " 'def init_region_config self region self regions region self region_config_class region_name region resource_types self resource_types\\n',\n",
       " 'def fetch_all self credentials regions partition_name aws targets None realtargets if not targets targets self targets for i target in enumerate targets first_region params self tweak_params target 3 credentials realtargets realtargets target 0 target 1 target 2 params target 4 targets first_region realtargets realtargets for i target in enumerate targets other_regions params self tweak_params target 3 credentials realtargets realtargets target 0 target 1 target 2 params target 4 targets other_regions realtargets printInfo Fetching s config format_service_name self service self fetchstatuslogger FetchStatusLogger targets first_region True api_service ec2 if self service lower vpc else self service lower regions build_region_list api_service regions partition_name self fetchstatuslogger counts regions discovered len regions q self _init_threading self _fetch_target self thread_config parse qr self _init_threading self _fetch_region api_service api_service credentials credentials q q targets self thread_config list for i region in enumerate regions qr put region targets first_region if i 0 else targets other_regions qr join q join self fetchstatuslogger show True\\n',\n",
       " 'def parse_stack self global_params region stack stack id stack pop StackId stack name stack pop StackName stack_policy api_clients region get_stack_policy StackName stack name if StackPolicyBody in stack_policy stack policy json loads stack_policy StackPolicyBody self stacks stack name stack\\n',\n",
       " 'def parse_queue self global_params region queue_url queue QueueUrl queue_url attributes api_clients region get_queue_attributes QueueUrl queue_url AttributeNames CreatedTimestamp Policy QueueArn Attributes queue arn attributes pop QueueArn for k in CreatedTimestamp queue k attributes k if k in attributes else None if Policy in attributes queue Policy json loads attributes Policy else queue Policy Statement queue name queue arn split 1 self queues queue name queue\\n',\n",
       " 'def recurse all_info current_info target_path current_path config add_suffix False results if len target_path 0 setattr config checked_items getattr config checked_items 1 if pass_conditions all_info current_path copy deepcopy config conditions if add_suffix and hasattr config id_suffix suffix fix_path_string all_info current_path config id_suffix current_path append suffix results append join current_path return results target_path copy deepcopy target_path dbg_target_path copy deepcopy target_path current_path copy deepcopy current_path attribute target_path pop 0 if type current_info dict if attribute in current_info split_path copy deepcopy current_path split_path append attribute results results recurse all_info current_info attribute target_path split_path config add_suffix elif attribute id for key in current_info split_target_path copy deepcopy target_path split_current_path copy deepcopy current_path split_current_path append key split_current_info current_info key results results recurse all_info split_current_info split_target_path split_current_path config add_suffix elif type current_info list for index split_current_info in enumerate current_info split_current_path copy deepcopy current_path split_current_path append str index results results recurse all_info split_current_info copy deepcopy target_path split_current_path config add_suffix else printError Error unhandled case typeof current_info s type current_info printError Path s current_path printError Object s str current_info printError Entry target path s str dbg_target_path raise Exception return results\\n',\n",
       " 'def pass_conditions all_info current_path conditions unknown_as_pass_condition False result False if len conditions 0 return True condition_operator conditions pop 0 for condition in conditions if condition 0 in condition_operators res pass_conditions all_info current_path condition unknown_as_pass_condition else path_to_value test_name test_values condition path_to_value fix_path_string all_info current_path path_to_value target_obj get_value_at all_info current_path path_to_value if type test_values list dynamic_value re_get_value_at match test_values if dynamic_value test_values get_value_at all_info current_path dynamic_value groups 0 True try res pass_condition target_obj test_name test_values except Exception as e res True if unknown_as_pass_condition else False printError Unable to process testcase s on value s interpreted as s test_name str target_obj res printException e True if condition_operator and and not res return False if condition_operator or and res return True if condition_operator or return False else return True\\n',\n",
       " 'def parse_subscription self params region subscription topic_arn subscription pop TopicArn topic_name topic_arn split 1 if topic_name in self topics topic self topics topic_name manage_dictionary topic subscriptions protocol protocol subscription pop Protocol manage_dictionary topic subscriptions protocol protocol topic subscriptions protocol protocol append subscription topic subscriptions_count 1\\n',\n",
       " 'def parse_topic self params region topic topic arn topic pop TopicArn topic name topic arn split 1 prefix partition service region account name topic arn split api_client api_clients region attributes api_client get_topic_attributes TopicArn topic arn Attributes for k in Owner DisplayName topic k attributes k if k in attributes else None for k in Policy DeliveryPolicy EffectiveDeliveryPolicy topic k json loads attributes k if k in attributes else None topic name topic arn split 1 manage_dictionary topic subscriptions manage_dictionary topic subscriptions_count 0 self topics topic name topic\\n',\n",
       " 'def parse_domains self domain params domain_id self get_non_aws_id domain DomainName domain name domain pop DomainName self domains domain_id domain\\n',\n",
       " 'def parse_hosted_zones self hosted_zone params hosted_zone_id hosted_zone pop Id hosted_zone name hosted_zone pop Name api_client params api_client record_sets handle_truncated_response api_client list_resource_record_sets HostedZoneId hosted_zone_id ResourceRecordSets hosted_zone update record_sets self hosted_zones hosted_zone_id hosted_zone\\n',\n",
       " 'def parse_connection self global_params region connection connection id connection pop connectionId connection name connection pop connectionName self connections connection id connection\\n',\n",
       " 'def parse_flow_log self global_params region fl get_name fl fl FlowLogId fl_id fl pop FlowLogId self flow_logs fl_id fl\\n',\n",
       " 'def parse_network_acl self global_params region network_acl vpc_id network_acl VpcId network_acl id network_acl pop NetworkAclId get_name network_acl network_acl id manage_dictionary network_acl rules network_acl rules ingress self __parse_network_acl_entries network_acl Entries False network_acl rules egress self __parse_network_acl_entries network_acl Entries True network_acl pop Entries manage_dictionary self vpcs vpc_id SingleVPCConfig self vpc_resource_types self vpcs vpc_id network_acls network_acl id network_acl\\n',\n",
       " 'def __parse_network_acl_entries self entries egress acl_dict for entry in entries if entry Egress egress acl for key in RuleAction RuleNumber acl key entry key acl CidrBlock entry CidrBlock if CidrBlock in entry else entry Ipv6CidrBlock acl protocol protocols_dict entry Protocol if PortRange in entry from_port entry PortRange From if entry PortRange From else 1 to_port entry PortRange To if entry PortRange To else 65535 acl port_range from_port if from_port to_port else str from_port str to_port else acl port_range 1 65535 acl_dict acl pop RuleNumber acl return acl_dict\\n',\n",
       " 'def parse_subnet self global_params region subnet vpc_id subnet VpcId manage_dictionary self vpcs vpc_id SingleVPCConfig self vpc_resource_types subnet_id subnet SubnetId get_name subnet subnet SubnetId subnet flow_logs manage_dictionary self vpcs vpc_id SingleVPCConfig self vpc_resource_types self vpcs vpc_id subnets subnet_id subnet\\n',\n",
       " 'def parse_vpc self global_params region_name vpc vpc_id vpc VpcId manage_dictionary self vpcs vpc_id SingleVPCConfig self vpc_resource_types self vpcs vpc_id name get_name vpc VpcId\\n',\n",
       " 'def format_listall_output format_file format_item_dir format rule option_prefix None template None skip_options False if format_file and os path isfile format_file if not template with open format_file rt as f template f read if not skip_options re_option re compile _OPTION_ _NOITPO_ optional_files re_option findall template for optional_file in optional_files if optional_file 1 startswith option_prefix with open os path join format_item_dir optional_file 1 strip rt as f template template replace optional_file 0 strip f read re_file re compile _FILE_ _ELIF_ while True requested_files re_file findall template available_files os listdir format_item_dir if format_item_dir else for requested_file in requested_files if requested_file 1 strip in available_files with open os path join format_item_dir requested_file 1 strip rt as f template template replace requested_file 0 strip f read re_line re compile _ITEM_ _METI_ re_key re compile _KEY_ re DOTALL re MULTILINE lines re_line findall template for i line in enumerate lines lines i line re_key findall line 1 requested_files re_file findall template if len requested_files 0 break elif format and format 0 csv keys rule keys line join _KEY_ s k for k in keys lines line line keys template line return lines template\\n',\n",
       " 'def load self file_name_valid False rule_type_valid False for rule_dir in self rule_dirs file_path os path join rule_dir self file_name if rule_dir else self file_name if os path isfile file_path self file_path file_path file_name_valid True break if not file_name_valid for rule_type in self rule_types if self file_name startswith rule_type self file_path os path join self rules_data_path self file_name rule_type_valid True file_name_valid True break if not rule_type_valid for rule_type in self rule_types self file_path os path join self rules_data_path rule_type self file_name if os path isfile self file_path file_name_valid True break elif os path isfile self file_path file_name_valid True if not file_name_valid printError Error could not find s self file_name else try with open self file_path rt as f self string_definition f read self load_from_string_definition except Exception as e printException e printError Failed to load rule defined in s file_path\\n',\n",
       " 'def parse_lb self global_params region lb lb arn lb pop LoadBalancerArn lb name lb pop LoadBalancerName vpc_id lb pop VpcId if VpcId in lb and lb VpcId else ec2_classic manage_dictionary self vpcs vpc_id VPCConfig self vpc_resource_types lb security_groups try for sg in lb SecurityGroups lb security_groups append GroupId sg lb pop SecurityGroups except Exception as e pass lb listeners listeners handle_truncated_response api_clients region describe_listeners LoadBalancerArn lb arn Listeners Listeners for listener in listeners listener pop ListenerArn listener pop LoadBalancerArn port listener pop Port lb listeners port listener lb attributes api_clients region describe_load_balancer_attributes LoadBalancerArn lb arn Attributes self vpcs vpc_id lbs self get_non_aws_id lb name lb\\n',\n",
       " 'def set_definition self rule_definitions attributes ip_ranges params string_definition rule_definitions self filename string_definition definition json loads string_definition definition conditions self conditions loaded_conditions for condition in definition conditions if condition 0 startswith _INCLUDE_ include re findall _INCLUDE_ condition 0 0 with open os path join os path dirname os path realpath __file__ data s include rt as f new_conditions f read for i value in enumerate condition 1 new_conditions re sub condition 1 i condition 2 i new_conditions new_conditions json loads new_conditions conditions loaded_conditions append new_conditions else loaded_conditions append condition definition conditions loaded_conditions string_definition json dumps definition parameters re findall _ARG_ a zA Z0 9 _ string_definition for param in parameters index int param 1 if len self args index string_definition string_definition replace param 0 elif type self args index list value s join s v for v in self args index string_definition string_definition replace s param 0 value else string_definition string_definition replace param 0 self args index stripdots re_strip_dots findall string_definition for value in stripdots string_definition string_definition replace value 0 value 1 replace definition json loads string_definition for condition in definition conditions if type condition list or len condition 1 or type condition 2 list continue for testcase in testcases result testcase regex match condition 2 if result and testcase name ip_ranges_from_file or testcase name ip_ranges_from_local_file filename result groups 0 conditions result groups 1 if len result groups 1 else if filename ip_ranges_from_args prefixes for filename in ip_ranges prefixes read_ip_ranges filename local_file True ip_only True conditions conditions condition 2 prefixes break else local_file True if testcase name ip_ranges_from_local_file else False condition 2 read_ip_ranges filename local_file local_file ip_only True conditions conditions break break elif result condition 2 params testcase name break if len attributes 0 attributes attr for attr in definition for attr in attributes if attr in definition setattr self attr definition attr if hasattr self path self service format_service_name self path split 0 if not hasattr self key setattr self key self filename setattr self key self key replace json if self key_suffix setattr self key s s self key self key_suffix\\n',\n",
       " 'def parse_instance self global_params region dbi vpc_id dbi DBSubnetGroup VpcId if DBSubnetGroup in dbi and VpcId in dbi DBSubnetGroup and dbi DBSubnetGroup VpcId else ec2_classic instance instance name dbi pop DBInstanceIdentifier for key in InstanceCreateTime Engine DBInstanceStatus AutoMinorVersionUpgrade DBInstanceClass MultiAZ Endpoint BackupRetentionPeriod PubliclyAccessible StorageEncrypted VpcSecurityGroups DBSecurityGroups DBParameterGroups EnhancedMonitoringResourceArn StorageEncrypted instance key dbi key if key in dbi else None if DBClusterIdentifier in dbi api_client api_clients region cluster api_client describe_db_clusters DBClusterIdentifier dbi DBClusterIdentifier DBClusters 0 instance MultiAZ cluster MultiAZ manage_dictionary self vpcs vpc_id VPCConfig self vpc_resource_types self vpcs vpc_id instances instance name instance\\n',\n",
       " 'def parse_snapshot self global_params region dbs vpc_id dbs VpcId if VpcId in dbs else ec2_classic snapshot_id dbs pop DBSnapshotIdentifier snapshot arn dbs pop DBSnapshotArn id snapshot_id name snapshot_id vpc_id vpc_id attributes DBInstanceIdentifier SnapshotCreateTime Encrypted OptionGroupName for attribute in attributes snapshot attribute dbs attribute if attribute in dbs else None api_client api_clients region attributes api_client describe_db_snapshot_attributes DBSnapshotIdentifier snapshot_id DBSnapshotAttributesResult snapshot attributes attributes DBSnapshotAttributes if DBSnapshotAttributes in attributes else manage_dictionary self vpcs vpc_id VPCConfig self vpc_resource_types self vpcs vpc_id snapshots snapshot_id snapshot\\n',\n",
       " 'def parse_security_group self global_params region security_group security_group arn security_group pop DBSecurityGroupArn security_group name security_group pop DBSecurityGroupName self security_groups security_group name security_group\\n',\n",
       " 'def parse_trail self global_params region trail trail_config trail_config name trail pop Name trail_id self get_non_aws_id trail_config name trail_details None api_client api_clients region if IsMultiRegionTrail in trail and trail IsMultiRegionTrail and trail HomeRegion region for key in HomeRegion TrailARN trail_config key trail key trail_config scout2_link services cloudtrail regions s trails s trail HomeRegion trail_id else for key in trail trail_config key trail key trail_config bucket_id self get_non_aws_id trail_config pop S3BucketName for key in IsMultiRegionTrail LogFileValidationEnabled if key not in trail_config trail_config key False trail_details api_client get_trail_status Name trail TrailARN for key in IsLogging LatestDeliveryTime LatestDeliveryError StartLoggingTime StopLoggingTime LatestNotificationTime LatestNotificationError LatestCloudWatchLogsDeliveryError LatestCloudWatchLogsDeliveryTime trail_config key trail_details key if key in trail_details else None if trail_details trail_config wildcard_data_logging self data_logging_status trail_config name trail_details api_client self trails trail_id trail_config\\n',\n",
       " 'def parse_cluster self global_params region cluster cluster_name cluster pop CacheClusterId cluster name cluster_name if CacheSubnetGroupName in cluster subnet_group api_clients region describe_cache_subnet_groups CacheSubnetGroupName cluster CacheSubnetGroupName CacheSubnetGroups 0 vpc_id subnet_group VpcId else vpc_id ec2_classic subnet_group None manage_dictionary self vpcs vpc_id VPCConfig self vpc_resource_types self vpcs vpc_id clusters cluster_name cluster if subnet_group self vpcs vpc_id subnet_groups subnet_group CacheSubnetGroupName subnet_group\\n',\n",
       " 'def parse_security_group self global_params region security_group security_group name security_group pop CacheSecurityGroupName self security_groups security_group name security_group\\n',\n",
       " 'def fetch self credentials regions skipped_regions partition_name aws self services fetch credentials self service_list regions partition_name\\n',\n",
       " 'def postprocessing aws_config current_time ruleset update_metadata aws_config update_last_run aws_config current_time ruleset\\n',\n",
       " 'def fetch self credentials services regions partition_name for service in vars self try if services and service not in services continue service_config getattr self service if fetch_all in dir service_config method_args method_args credentials credentials if service iam method_args regions regions method_args partition_name partition_name service_config fetch_all method_args if hasattr service_config finalize service_config finalize except Exception as e printError Error could not fetch s configuration service printException e\\n',\n",
       " 'def __open_file self config_filename force_write quiet False if not quiet printInfo Saving config if prompt_4_overwrite config_filename force_write try config_dirname os path dirname config_filename if not os path isdir config_dirname os makedirs config_dirname return open config_filename wt except Exception as e printException e else return None\\n',\n",
       " 'def parse_cluster self global_params region cluster cluster_id cluster Id cluster api_clients region describe_cluster ClusterId cluster_id Cluster cluster id cluster pop Id cluster name cluster pop Name vpc_id TODO manage_dictionary self vpcs vpc_id VPCConfig self vpc_resource_types self vpcs vpc_id clusters cluster_id cluster\\n',\n",
       " 'def test_5 self log error This is a case having an error that is not AssertionError unittest distinguishes failure and error by if the case raises an AssertionError or not sleep 1 raise TypeError\\n',\n",
       " 'def test_6 self log info Here is logging of test_6 sleep 1 self assertEqual 1 1\\n',\n",
       " 'unittest skip Reason of skipping test_7 def test_7 self sleep 3 self assertEqual 1 1\\n',\n",
       " 'unittest expectedFailure def test_8 self log error This is unexpected to be passed sleep 1 self assertEqual 1 1\\n',\n",
       " 'def test_9 self log error This is a failure case which raises AssertionError sleep 2 self assertEqual 1 2\\n',\n",
       " 'unittest expectedFailure def test_10 self log info This is expected to be failed sleep 1 log debug Here is test_10 DEBUG log raise ValueError\\n',\n",
       " 'def exc_info_to_string exc_info exctype value tb exc_info while tb and _is_relevant_tb_level tb tb tb tb_next if exctype is AssertionError length _count_relevant_tb_levels tb msg_lines format_exception exctype value tb length else msg_lines format_exception exctype value tb return join msg_lines rstrip\\n',\n",
       " 'def test_1 self log info This is an example of data_driven decorator self repeat_part Auth interval 0 5\\n',\n",
       " 'unishark data_driven user_id 1 2 3 4 5 passwd a b c d def test_2 self param log info Another example of data_driven decorator sleep 1 log info user_id d passwd s param user_id param passwd\\n',\n",
       " 'unittest skip Here is the reason of skipping test_3 def test_3 self log info Here is logging of test_3 sleep 2 self assertEqual 1 2\\n',\n",
       " 'def test_4 self log info Here is logging of test_4 sleep 2 log info Try escape div self assertEqual 1 1\\n',\n",
       " 'def test_successes self self assertEqual 1 1 logger info n t 汉 _ n\\n',\n",
       " 'def test_11 self log info Here is logging of test_11 sleep 2 self assertEqual 1 1\\n',\n",
       " 'def test_12 self log info Here is logging of test_12 sleep 2 self assertEqual 1 1\\n',\n",
       " 'def test_13 self log info Here is logging of test_13 sleep 2 self assertEqual 1 1\\n',\n",
       " 'def test_14 self log info Here is logging of test_14 sleep 2 self assertEqual 1 1\\n',\n",
       " 'def test_15 self log info Here is logging of test_15 sleep 2 self assertEqual 1 1\\n',\n",
       " 'def test_16 self log info Here is logging of test_16 sleep 1 self assertEqual 1 1\\n',\n",
       " 'unishark data_driven left list range 9 unishark data_driven right list range 9 def test_17 self param l param left r param right log info d x d d l r l r\\n',\n",
       " 'def test_11 self log info Here is logging of test_11 self assertEqual 1 1\\n',\n",
       " 'def test_12 self log info Here is logging of test_12 self assertEqual 1 1\\n',\n",
       " 'def test_13 self log info Here is logging of test_13 self assertEqual 1 1\\n',\n",
       " 'def test_14 self log info Here is logging of test_14 self assertEqual 1 1\\n',\n",
       " 'def test_15 self log info Here is logging of test_15 self assertEqual 1 1\\n',\n",
       " 'def test_16 self log info Here is logging of test_16 self assertEqual 1 1\\n',\n",
       " 'unishark data_driven left list range 9 unishark data_driven right list range 9 def test_17 self param l param left r param right log info str l x str r str l r\\n',\n",
       " 'def test_1 self log info This is an example of data_driven decorator self repeat_part\\n',\n",
       " 'unishark data_driven user_id 1 2 3 4 passwd a b c d def test_2 self param log info Another example of data_driven decorator log info user_id d passwd s param user_id param passwd\\n',\n",
       " 'unittest skip Here is the reason of skipping test_3 def test_3 self log info Here is logging of test_3 self assertEqual 1 2\\n',\n",
       " 'def test_4 self log info Here is logging of test_4 log info Try escape div self assertEqual 1 1\\n',\n",
       " 'def _exc_info_to_string self error test exctype value tb error while tb and self _is_relevant_tb_level tb tb tb tb_next if exctype is test failureException length self _count_relevant_tb_levels tb msg_lines traceback format_exception exctype value tb length else msg_lines traceback format_exception exctype value tb return join msg_lines\\n',\n",
       " 'def test_successes self self assertEqual 1 1 print A stdout log\\n',\n",
       " 'unishark data_driven left list range 2 unishark data_driven right list range 2 def test_1 self param l param left r param right logger info d x d d l r l r print thread d Bob test dx d threading current_thread ident l r time sleep 1\\n',\n",
       " 'def test_5 self log error This is a case having an error that is not AssertionError unittest distinguishes failure and error by if the case raises an AssertionError or not raise TypeError\\n',\n",
       " 'def test_6 self log info Here is logging of test_6 self assertEqual 1 1\\n',\n",
       " 'unittest skip Reason of skipping test_7 def test_7 self self assertEqual 1 1\\n',\n",
       " 'def test_8 self log debug There is an error raise TypeError\\n',\n",
       " 'def test_9 self log error This is a failure case which raises AssertionError self assertEqual 1 2\\n',\n",
       " 'def test_10 self log info Here is test_10 INFO log log debug Here is test_10 DEBUG log self assertEqual 1 1\\n',\n",
       " 'def test_flush p progressbar ProgressBar poll_interval 0 001 for i in range 10 print pre updates p updates p update i print need update p _needs_update if i 5 time sleep 0 1 print post updates p updates\\n',\n",
       " 'def test_left_justify p progressbar ProgressBar widgets progressbar BouncingBar marker progressbar RotatingMarker max_value 100 term_width 20 left_justify True assert p term_width is not None for i in range 100 p update i\\n',\n",
       " 'def test_right_justify p progressbar ProgressBar widgets progressbar BouncingBar marker progressbar RotatingMarker max_value 100 term_width 20 left_justify False assert p term_width is not None for i in range 100 p update i\\n',\n",
       " 'def test_auto_width monkeypatch def ioctl args return x00ë x00 x00 x00 x00 x00 def fake_signal signal func pass try import fcntl monkeypatch setattr fcntl ioctl ioctl monkeypatch setattr signal signal fake_signal p progressbar ProgressBar widgets progressbar BouncingBar marker progressbar RotatingMarker max_value 100 left_justify True term_width None assert p term_width is not None for i in range 100 p update i except ImportError pass\\n',\n",
       " 'def test_fill_right p progressbar ProgressBar widgets progressbar BouncingBar fill_left False max_value 100 term_width 20 assert p term_width is not None for i in range 100 p update i\\n',\n",
       " 'def test_fill_left p progressbar ProgressBar widgets progressbar BouncingBar fill_left True max_value 100 term_width 20 assert p term_width is not None for i in range 100 p update i\\n',\n",
       " 'def test_no_fill monkeypatch bar progressbar BouncingBar bar INTERVAL timedelta seconds 1 p progressbar ProgressBar widgets bar max_value progressbar UnknownLength term_width 20 assert p term_width is not None for i in range 30 p update i force True p start_time p start_time timedelta seconds i\\n',\n",
       " 'def test_list_example testdir v testdir makepyfile import time import freezegun import progressbar with freezegun freeze_time as fake_time bar progressbar ProgressBar term_width 65 bar _MINIMUM_UPDATE_INTERVAL 1e 9 for i in bar list range 9 fake_time tick 1 result testdir runpython v result stderr lines l rstrip for l in result stderr lines if l strip pprint pprint result stderr lines width 70 result stderr fnmatch_lines N A 0 of 9 Elapsed Time 00 00 ETA 11 1 of 9 Elapsed Time 00 01 ETA 00 08 22 2 of 9 Elapsed Time 00 02 ETA 00 07 33 3 of 9 Elapsed Time 00 03 ETA 00 06 44 4 of 9 Elapsed Time 00 04 ETA 00 05 55 5 of 9 Elapsed Time 00 05 ETA 00 04 66 6 of 9 Elapsed Time 00 06 ETA 00 03 77 7 of 9 Elapsed Time 00 07 ETA 00 02 88 8 of 9 Elapsed Time 00 08 ETA 00 01 100 9 of 9 Elapsed Time 00 09 Time 00 09\\n',\n",
       " 'def test_generator_example testdir v testdir makepyfile import time import freezegun import progressbar with freezegun freeze_time as fake_time bar progressbar ProgressBar term_width 60 bar _MINIMUM_UPDATE_INTERVAL 1e 9 for i in bar iter range 9 fake_time tick 1 result testdir runpython v result stderr lines l for l in result stderr lines if l strip pprint pprint result stderr lines width 70 lines for i in range 9 lines append s s s i d Elapsed Time d 00 i 02d dict i i result stderr re_match_lines lines\\n',\n",
       " 'def test_rapid_updates testdir v testdir makepyfile import time import freezegun import progressbar with freezegun freeze_time as fake_time bar progressbar ProgressBar term_width 60 bar _MINIMUM_UPDATE_INTERVAL 1e 9 for i in bar range 10 if i 5 fake_time tick 1 else fake_time tick 2 result testdir runpython v result stderr lines l for l in result stderr lines if l strip pprint pprint result stderr lines width 70 result stderr fnmatch_lines N A 0 of 10 Elapsed Time 00 00 ETA 10 1 of 10 Elapsed Time 00 01 ETA 00 09 20 2 of 10 Elapsed Time 00 02 ETA 00 08 30 3 of 10 Elapsed Time 00 03 ETA 00 07 40 4 of 10 Elapsed Time 00 04 ETA 00 06 50 5 of 10 Elapsed Time 00 05 ETA 00 05 60 6 of 10 Elapsed Time 00 07 ETA 00 06 70 7 of 10 Elapsed Time 00 09 ETA 00 06 80 8 of 10 Elapsed Time 00 11 ETA 00 04 90 9 of 10 Elapsed Time 00 13 ETA 00 02 100 10 of 10 Elapsed Time 00 15 Time 00 15\\n',\n",
       " 'def test_timer widgets progressbar Timer p progressbar ProgressBar max_value 2 widgets widgets poll_interval 0 0001 p start p update p update 1 p _needs_update time sleep 0 001 p update 1 p finish\\n',\n",
       " 'def test_eta widgets progressbar ETA p progressbar ProgressBar min_value 0 max_value 2 widgets widgets poll_interval 0 0001 p start time sleep 0 001 p update 0 time sleep 0 001 p update 1 time sleep 0 001 p update 1 time sleep 0 001 p update 2 time sleep 0 001 p finish time sleep 0 001 p update 2\\n',\n",
       " 'def test_adaptive_eta widgets progressbar AdaptiveETA widgets 0 INTERVAL datetime timedelta microseconds 1 p progressbar ProgressBar max_value 2 samples 2 widgets widgets poll_interval 0 0001 p start for i in range 20 p update 1 time sleep 0 001 p finish\\n',\n",
       " 'def test_adaptive_transfer_speed widgets progressbar AdaptiveTransferSpeed p progressbar ProgressBar max_value 2 widgets widgets poll_interval 0 0001 p start p update 1 time sleep 0 001 p update 1 p finish\\n',\n",
       " 'def test_non_changing_eta widgets progressbar AdaptiveETA progressbar ETA progressbar AdaptiveTransferSpeed p progressbar ProgressBar max_value 2 widgets widgets poll_interval 0 0001 p start p update 1 time sleep 0 001 p update 1 p finish\\n',\n",
       " 'def test_eta_not_available def gen for x in range 200 yield x widgets progressbar AdaptiveETA progressbar ETA bar progressbar ProgressBar widgets widgets for i in bar gen pass\\n',\n",
       " 'def test_list p progressbar ProgressBar for i in p range 10 time sleep 0 001\\n',\n",
       " 'def test_iterator_with_max_value p progressbar ProgressBar max_value 10 for i in p i for i in range 10 time sleep 0 001\\n',\n",
       " 'def test_iterator_without_max_value_error p progressbar ProgressBar for i in p i for i in range 10 time sleep 0 001 assert p max_value is progressbar UnknownLength\\n',\n",
       " 'def test_iterator_without_max_value p progressbar ProgressBar widgets progressbar AnimatedMarker progressbar FormatLabel value d progressbar BouncingBar progressbar BouncingBar marker progressbar RotatingMarker for i in p i for i in range 10 time sleep 0 001\\n',\n",
       " 'def test_iterator_with_incorrect_max_value p progressbar ProgressBar max_value 10 with pytest raises ValueError for i in p i for i in range 20 time sleep 0 001\\n',\n",
       " 'def test_no_max_value p progressbar ProgressBar p start for i in range 5 time sleep 1 p update i\\n',\n",
       " 'def test_correct_max_value p progressbar ProgressBar max_value 10 for i in range 5 time sleep 1 p update i\\n',\n",
       " 'def test_minus_max_value p progressbar ProgressBar min_value 2 max_value 1 with pytest raises ValueError p update 1\\n',\n",
       " 'def test_zero_max_value p progressbar ProgressBar max_value 0 p update 0 with pytest raises ValueError p update 1\\n',\n",
       " 'def test_one_max_value p progressbar ProgressBar max_value 1 p update 0 p update 0 p update 1 with pytest raises ValueError p update 2\\n',\n",
       " 'def test_changing_max_value p progressbar ProgressBar max_value 10 range 20 max_value 20 for i in p time sleep 1\\n',\n",
       " 'def test_backwards p progressbar ProgressBar max_value 1 p update 1 p update 0\\n',\n",
       " 'def test_incorrect_max_value p progressbar ProgressBar max_value 5 for i in range 5 time sleep 1 p update i with pytest raises ValueError for i in range 5 10 time sleep 1 p update i\\n',\n",
       " 'def example fn functools wraps fn def wrapped try sys stdout write Running s n fn __name__ fn sys stdout write n except KeyboardInterrupt sys stdout write nSkipping example n n time sleep 0 2 examples append wrapped return wrapped\\n',\n",
       " 'def cache self self author self date self body self files self files_status\\n',\n",
       " 'def show self args cmd git git dir self _git_dir show self sha1 args p subprocess Popen cmd stdout subprocess PIPE return p stdout read\\n',\n",
       " 'staticmethod def _glob_content env content glob_regex import fnmatch re docs None for entry in content if glob_regex re_comp re compile entry else re_comp re compile fnmatch translate entry if docs is None docs sorted env found_docs reverse True for entry_test in docs if re_comp match entry_test yield entry_test\\n',\n",
       " 'def __init__ self token self token token self _api facebook GraphAPI self token\\n',\n",
       " 'def get_user_name self _id if not isinstance _id str raise ValueError id must be a str user self _api get_object _id return user first_name if user else None\\n',\n",
       " 'def __init__ self api_token self token api_token\\n',\n",
       " 'def get_current_weather self city_name api_url http api openweathermap org data 2 5 weather q 0 APPID 1 format city_name self token info r get api_url json return info\\n',\n",
       " 'def __init__ self tokens self api twitter Api consumer_key tokens consumer_key consumer_secret tokens consumer_secret access_token_key tokens access_token_key access_token_secret tokens access_token_secret\\n',\n",
       " 'def get_stack_answer_by self kwargs if len kwargs 1 raise BotChuckyError The argument must be one for key in kwargs keys query kwargs get key self _default_parameters update key query if not isinstance query str raise TypeError f query must be a string encode_query parse urlencode self _default_parameters stack_url f https api stackexchange com 2 2 search advanced encode_query questions r get stack_url json links obj link for obj in questions items return links\\n',\n",
       " 'def __init__ self client_id self client_id client_id self _api soundcloud Client client_id self client_id\\n',\n",
       " 'def resolve_track self url try track self _api get resolve str url return success True track track id except Exception as error return success False detail f Error error message Code error response status_code\\n',\n",
       " 'def search self artist None self artist artist if self artist is not None try artists self _api get users q self artist tracks self _api get tracks q self artist return success True artists artists tracks tracks except Exception as error return success False detail f Error error message Code error response status_code\\n',\n",
       " 'def __init__ self key self key key self categories bsns business entnt entertainment gmg gaming gnrl general music music pltcs politics scntr science and nature sport sport tech technology self countries au Australia de Germany gb United Kingdom in India us United States of America self languages en English de German fr French\\n',\n",
       " 'def get_categories self return self categories\\n',\n",
       " 'def get_countries self return self countries\\n',\n",
       " 'def get_languages self return self languages\\n',\n",
       " 'def get_key self return self key\\n',\n",
       " 'def get_article self source count order None from constants import NEWS_URL order f sortBy order if order is not None else url f NEWS_URL articles source source order apiKey self key data r get url json if data status error raise ValueError data message count min count len data articles return data articles count\\n',\n",
       " 'def get_sources self count category None language None country None from constants import NEWS_URL if category is not None and category not in bsns entnt gmg gnrl music pltcs scntr sport tech raise BotChuckyError f Invalid category choose from self categories elif category is not None category f category self categories category else category if language is not None and language not in en de fr raise BotChuckyError f Invalid language choose from self languages elif language is not None language f language language else language if country is not None and country not in au de gb in us raise BotChuckyError f Invalid country choose from self countries elif country is not None country f country country else country url f NEWS_URL sources category language country data r get url json if not len data sources raise ValueError Query doesn t match count min count len data sources return data sources count\\n',\n",
       " 'def update_headers self self session headers update User Agent Mozilla 5 0 X11 Linux x86_64 AppleWebKit 537 36 KHTML like Gecko Chrome 57 0 2987 110 Safari 537 36\\n',\n",
       " 'def get_meaning self word str page self session get self _url format word parsed_page bs page text html parser divs parsed_page find_all div class def set 3 data for div in divs extracted_string div get_text strip split n 1 extracted_string extracted_string strip data append extracted_string return data\\n',\n",
       " 'def get_text self text str return split_text text\\n',\n",
       " 'property def config_keys self return self config keys\\n',\n",
       " 'def check_and_run self text str func None for key in self config_keys if key not in text return Sorry could you repeat please if key in text func self config get key if isinstance func Callable return func else for topic in self config get key if topic in text func self config key topic return func if topic not in text return I m Chucky bot check your config\\n',\n",
       " 'def __init__ self token open_weather_token None tw_consumer_key None tw_consumer_secret None tw_access_token_key None tw_access_token_secret None soundcloud_id None news_api_key None self token token self open_weather_token open_weather_token self params access_token self token self headers Content Type application json self fb FacebookData self token self weather WeatherData open_weather_token self twitter_tokens consumer_key tw_consumer_key consumer_secret tw_consumer_secret access_token_key tw_access_token_key access_token_secret tw_access_token_secret self twitter TwitterData self twitter_tokens self soundcloud_id soundcloud_id self soundcloud SoundCloudData self soundcloud_id self stack StackExchangeData self news NewsData news_api_key self dictionary DictionaryData\\n',\n",
       " 'def send_message self id_ str text data recipient id id_ message text text message r post API_URL params self params headers self headers json data if message status_code is not 200 return message text\\n',\n",
       " 'def send_attachment self id_ str attachment data recipient id id_ message attachment type image payload url attachment message r post API_URL params self params headers self headers json data if message status_code is not 200 return message text\\n',\n",
       " 'def send_weather_message self id_ str city_name str if self open_weather_token is None raise BotChuckyTokenError Open Weather weather_info self weather get_current_weather city_name if weather_info cod 401 error weather_info message raise BotChuckyInvalidToken error if weather_info cod 404 msg f Sorry I cant find information about weather in city_name return self send_message id_ msg description weather_info weather 0 description code weather_info weather 0 icon icon f http openweathermap org img w code png msg f Current weather in city_name is description n self send_message id_ msg self send_attachment id_ icon\\n',\n",
       " 'def send_tweet self status str if not all self twitter_tokens values raise BotChuckyTokenError Twitter reply self twitter send_tweet status if reply success return f I have placed your tweet with status status return f Twitter Error reply detail\\n',\n",
       " 'def send_soundcloud_message self id_ str artist str if not self soundcloud_id raise BotChuckyTokenError SoundCloud result self soundcloud search artist if result success tracks_from_artist list result tracks title msg f SoundCloud found result artists nTrack Listing tracks_from_artist return self send_message id_ msg msg f SoundCloud Error result detail return self send_message id_ msg\\n',\n",
       " 'def send_stack_questions self id_ kwargs msg I can t find questions for you try again answers self stack get_stack_answer_by kwargs if answers if len answers 2 msg f I found questions for you links below Question 1 answers 0 Question 2 answers 1 return self send_message id_ msg if len answers 1 msg f I found question for you link below n n Question answers 0 return self send_message id_ msg else return self send_message id_ msg\\n',\n",
       " 'def send_article self id_ str source count order None if self news get_key is None raise BotChuckyTokenError Articles are available only with the newsapi org key count min count 10 data self news get_article source count order for article in data message f nBy article author n message f Title article title n message f Desc article description n message f Read more article url self send_message id_ message\\n',\n",
       " ...]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "holdout_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'set the value of the forum input for this choreo required string forum short short'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq2seq_inf.predict(\"def set_Forum self value super ListUsersInputSet self _set_input Forum value\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Required Group, str or dict. Received: <class 'pathlib.PosixPath'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-2c14851b7268>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mseq2seq_Model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOUTPUT_PATH\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m'code_summary_seq2seq_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/work/venv/lib/python3.7/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m   1088\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1090\u001b[0;31m         \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1091\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/venv/lib/python3.7/site-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mopened_new_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/venv/lib/python3.7/site-packages/keras/utils/io_utils.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, mode)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m             raise TypeError('Required Group, str or dict. '\n\u001b[0;32m--> 197\u001b[0;31m                             'Received: {}.'.format(type(path)))\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_only\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Required Group, str or dict. Received: <class 'pathlib.PosixPath'>."
     ]
    }
   ],
   "source": [
    "seq2seq_Model.save(OUTPUT_PATH/'code_summary_seq2seq_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq_Model = load_model('./py_func_sum_v9_.epoch16-val2.55276.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 3276 =================\n",
      "\n",
      "Original Input:\n",
      " def is_valid email\n",
      " \n",
      "\n",
      "Original Output:\n",
      " check if an email address if valid .\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " check if the email is valid\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 125187 =================\n",
      "\n",
      "Original Input:\n",
      " def _phrasedmlToSEDML self phrasedmlStr warnings warn Use inline_omex instead DeprecationWarning sedmlstr phrasedml convertString phrasedmlStr if sedmlstr is None raise Exception phrasedml getLastError return sedmlstr\n",
      " \n",
      "\n",
      "Original Output:\n",
      " convert phrasedml string to sedml .\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " return a string with the given c code\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 126515 =================\n",
      "\n",
      "Original Input:\n",
      " property def process self if hasattr self _process return self _process else self _process self _get_process return self _process\n",
      " \n",
      "\n",
      "Original Output:\n",
      " \"store the actual process in _ process . if it does n't exist yet , create it .\"\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " get the process s process\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 153598 =================\n",
      "\n",
      "Original Input:\n",
      " def test_eigh_build self level rlevel rvals 68 60568999 89 57756725 106 67185574 cov array 77 70273908 3 51489954 15 64602427 3 51489954 88 97013878 1 07431931 15 64602427 1 07431931 98 18223512 vals vecs linalg eigh cov assert_array_almost_equal vals rvals\n",
      " \n",
      "\n",
      "Original Output:\n",
      " ticket 662 .\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " ticket number\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 139428 =================\n",
      "\n",
      "Original Input:\n",
      " def get_partial_workspaces primary_detector secondary_detector reduction_mode_vs_output_bundles is_data_type primary reduction_mode_vs_output_bundles primary_detector primary_for_data_type next setting for setting in primary if is_data_type setting None primary_count None if primary_for_data_type is None else primary_for_data_type output_workspace_count primary_norm None if primary_for_data_type is None else primary_for_data_type output_workspace_norm secondary reduction_mode_vs_output_bundles secondary_detector secondary_for_data_type next setting for setting in secondary if is_data_type setting None secondary_count None if secondary_for_data_type is None else secondary_for_data_type output_workspace_count secondary_norm None if secondary_for_data_type is None else secondary_for_data_type output_workspace_norm return primary_count primary_norm secondary_count secondary_norm\n",
      " \n",
      "\n",
      "Original Output:\n",
      " get the partial workspaces for the primary and secondary detectors .\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " get a list of workspaces that have been created for a given indexer\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 140365 =================\n",
      "\n",
      "Original Input:\n",
      " def _reset self self env reset noops np random randint 1 self noop_max 1 for _ in range noops obs _ _ _ self env step 0 return obs\n",
      " \n",
      "\n",
      "Original Output:\n",
      " \"do no - op action for a number of steps in [ 1 , noop_max ] .\"\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " reset the environment to the initial state\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 36010 =================\n",
      "\n",
      "Original Input:\n",
      " def get self args kwargs self object self get_object return super BaseUpdateView self get args kwargs\n",
      " \n",
      "\n",
      "Original Output:\n",
      " handler for get requests .\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " get the object s details\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 51141 =================\n",
      "\n",
      "Original Input:\n",
      " def RunChecks self results affected_files self input_api change AffectedFiles file_filter self file_filter include_deletes False affected_js_files filter lambda f f LocalPath endswith js affected_files for f in affected_js_files error_lines for i line in enumerate f NewContents start 1 error_lines filter None self ChromeSendCheck i line self ConstCheck i line self GetElementByIdCheck i line self EndJsDocCommentCheck i line self ExtraDotInGenericCheck i line self InheritDocCheck i line self WrapperTypeCheck i line self VarNameCheck i line lint_errors self ClosureLint self input_api os_path join self input_api change RepositoryRoot f LocalPath for error in lint_errors highlight self _GetErrorHighlight error token start_index error token length error_msg line d E 04d s n s n s error token line_number error code error message error token line rstrip highlight error_lines append error_msg if error_lines error_lines Found JavaScript style violations in s f LocalPath error_lines results append self _MakeErrorOrWarning n join error_lines f AbsoluteLocalPath if results results append self output_api PresubmitNotifyResult See the JavaScript style guide at http www chromium org developers web development style guide TOC JavaScript return results\n",
      " \n",
      "\n",
      "Original Output:\n",
      " check for violations of the chromium javascript style guide . see http://chromium.org/developers/web-development-style-guide#toc-javascript\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " checks for violations of the chromium javascript files\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 145806 =================\n",
      "\n",
      "Original Input:\n",
      " def parseheader self reader reader nextline while self startswithheader reader self parseparameter reader return\n",
      " \n",
      "\n",
      "Original Output:\n",
      " parse table headers\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " read a reader from the reader tree\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 19016 =================\n",
      "\n",
      "Original Input:\n",
      " classmethod def _set_all_srcs cls rule_details details_map all_srcs for rule_symbol in rule_details su ALL_DEPS_KEY other_rule_details details_map rule_symbol assert su PYTHON_THRIFT_LIB_TYPE other_rule_details su TYPE_KEY all_srcs append other_rule_details su SRCS_KEY 0 rule_details su ALL_SRCS_KEY all_srcs\n",
      " \n",
      "\n",
      "Original Output:\n",
      " set all sources list for downstream consumption .\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " set all sources\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 71633 =================\n",
      "\n",
      "Original Input:\n",
      " def set_RefreshToken self value super RemoveChildInputSet self _set_input RefreshToken value\n",
      " \n",
      "\n",
      "Original Output:\n",
      " \"set the value of the refreshtoken input for this choreo . ( ( conditional , string ) an oauth refresh token used to generate a new access token when the original token is expired . required unless providing a valid accesstoken . )\"\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " set the value of the refreshtoken input for this choreo conditional string an oauth token\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 96790 =================\n",
      "\n",
      "Original Input:\n",
      " def aggregateData datain aggFac print Aggregate data by a factor of d aggFac ni_hi datain shape 0 nj_hi datain shape 1 ni_lo max 1 ni_hi aggFac nj_lo max 1 nj_hi aggFac shape_lo ni_lo nj_lo dataout np zeros shape_lo dtype datain dtype order F for j_lo in range nj_lo for i_lo in range ni_lo i_hi_beg i_lo int aggFac j_hi_beg j_lo int aggFac i_hi_end min i_lo 1 int aggFac 1 ni_hi 1 j_hi_end min j_lo 1 int aggFac 1 nj_hi 1 dataout i_lo j_lo pylab mean datain i_hi_beg i_hi_end 1 j_hi_beg j_hi_end 1 return dataout\n",
      " \n",
      "\n",
      "Original Output:\n",
      " perform aggregation by a factor of aggfac\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " generate the data for the datain\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 36458 =================\n",
      "\n",
      "Original Input:\n",
      " def applyCut self cut return Catalog self config data self data cut\n",
      " \n",
      "\n",
      "Original Output:\n",
      " return a new catalog which is a subset of objects selected using the input cut array .\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " return a cut for the cut\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 123124 =================\n",
      "\n",
      "Original Input:\n",
      " def __init__ self year if year 1 raise ValueError Year 0 is before creation format year self year year self leap HebrewDate _is_leap year\n",
      " \n",
      "\n",
      "Original Output:\n",
      " the initializer for a year object .\n",
      "\n",
      "\n",
      "****** Predicted Output ******:\n",
      " year is a year of the year\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 38349 =================\n",
      "\n",
      "Original Input:\n",
      " def to_unit self unit self x unit self x self y unit self y return self\n",
      " \n",
      "\n",
      "Original Output:\n",
      " translate coordinates to be of a certain unit type .\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "****** Predicted Output ******:\n",
      " convert to a unit vector\n"
     ]
    }
   ],
   "source": [
    "seq2seq_inf = Seq2Seq_Inference(encoder_preprocessor=enc_pp,\n",
    "                                 decoder_preprocessor=dec_pp,\n",
    "                                 seq2seq_model=seq2seq_Model)\n",
    "\n",
    "demo_testdf = pd.DataFrame({'code':holdout_code, 'comment':holdout_comment, 'ref':''})\n",
    "seq2seq_inf.demo_model_predictions(n=15, df=demo_testdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'set the value of the forum input for this choreo required string forum short short'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq2seq_inf.predict(\"def set_Forum self value super ListUsersInputSet self _set_input Forum value\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
